[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AWM useRguide",
    "section": "",
    "text": "Welcome\nThe useRguide for the Attention & Working Memory Lab at Georgia Tech documents the steps and tools for how we process and analyze data in our lab.\nThe workflow and data processing steps presented in this guide are specific to the type of data we tend to work with in our lab. We primarily collect behavioral data on a large set of cognitive tasks to test individual differences in cognitive ability. However, you will find that most of the principles presented here apply to working with other types of data.\nFor a more complete training in R, see the R for the Psychology Student Workshop.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installs and Updates",
    "section": "",
    "text": "Install Software",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "installation.html#install-r",
    "href": "installation.html#install-r",
    "title": "Installs and Updates",
    "section": "Install R",
    "text": "Install R\nFirst you need to download the latest version of R from their website https://www.r-project.org\n\nSelect CRAN on the left, just under Download\nSelect the first option under 0-Cloud\nSelect the download option depending on your computer\nSelect the base installation (for Windows) or the Latest Release (for Mac)\nOpen and Run the installation file",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "installation.html#install-rstudio",
    "href": "installation.html#install-rstudio",
    "title": "Installs and Updates",
    "section": "Install RStudio",
    "text": "Install RStudio\nThe easiest way to interact with R is through the RStudio environment. To do this you need to download RStudio\n\nSelect the Free version of RStudio Desktop\n\nSelect the download option depending on your computer",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "installation.html#update-r",
    "href": "installation.html#update-r",
    "title": "Installs and Updates",
    "section": "Update R",
    "text": "Update R\nIf you already have R installed, but want to update it to the most current version follow these steps.\nWarning: When updating R (not RStudio), it may remove all packages you have installed\nFirst check what version of R you have installed.\n\nOpen RStudio\nIn the console window you will see the R version you are running (e.g., R version 4.1.0)\nIf you have an R version older than 4.0.0 than you need to update R.\nRun the following lines of code in your console window. This is an easy way to re-install all your currently installed packages. This step will save a list of packages to re-install later.\n\n\n# Save current packages and their versions to object called ip\n\nip &lt;- installed.packages()\nip\n\n# Save the object as an .rds file\n\nsaveRDS(ip, \"CurrentPackages.rds\")\n\n\nExit out of all R or RStudio windows\nDownload and install the latest version of R (see the section on installing R above)\nOpen RStudio\nCheck if your previously installed packages are installed using the Packages tab in the bottom right window\nIf you need to re-install your previous packages, then run the following lines of code\n\n\n# After updating R, load the file and reinstall packages\n\nip &lt;- readRDS(\"CurrentPackages.rds\")\n\ninstall.packages(ip[,1])",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "installation.html#update-rstudio",
    "href": "installation.html#update-rstudio",
    "title": "Installs and Updates",
    "section": "Update RStudio",
    "text": "Update RStudio\nGo to Help -&gt; Check for Updates",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "installation.html#customizing-rstudio",
    "href": "installation.html#customizing-rstudio",
    "title": "Installs and Updates",
    "section": "Customizing RStudio",
    "text": "Customizing RStudio\nComing soon…",
    "crumbs": [
      "Installs and Updates"
    ]
  },
  {
    "objectID": "data-proc-overview.html",
    "href": "data-proc-overview.html",
    "title": "1  Overview",
    "section": "",
    "text": "How We Do Research\nWe make use of large data collection efforts where we recruit 300-500 people to participate in multiple 2-3 hour sessions and perform up to 40+ cognitive tasks. A single large data collection effort will contain multiple research projects and publications.\nFor instance, in one of our recent data collection efforts (2020-2022) we recruited 461 participants for 5 2.5-hour sessions in which they completed 42 cognitive tasks. Multiple publications have resulted from this data collection effort with several more in the works:\nBurgoyne, Seeburger, and Engle (2024) . Modality Matters: Three Auditory Conflict Tasks to Measure Individual Differences in Attention Control\nBurgoyne et al. (2023) . Nature and Measurement of Attention Control\nDraheim, Tsukahara, and Engle (2023) . Replication and Extension of the Toolbox Approach to Measuring Attention Control.\nOSF BOAT: See a description and full list of publications and projects to result from this data collection effort on OSF.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "data-proc-overview.html#solution-to-challenges",
    "href": "data-proc-overview.html#solution-to-challenges",
    "title": "1  Overview",
    "section": "Solution to Challenges",
    "text": "Solution to Challenges\nSimply using R is not enough\n\nDocument steps and aim for full reproducibility\nKeep projects, data files, and scripts well orgainzed",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "data-proc-overview.html#what-does-reproducibility-mean",
    "href": "data-proc-overview.html#what-does-reproducibility-mean",
    "title": "1  Overview",
    "section": "What does reproducibility mean?",
    "text": "What does reproducibility mean?\nReproducibility means that all data processing and analysis steps can be fully reproduced using only the original raw data files and the execution of the R scripts. There are different levels of reproducibility (I made these up):\n\nPartially reproducible - only some data processing and analysis steps can be reproduced, which may be due to a lack of original raw data files, the “just get it done” approach, or the use of proprietary and non-reproducible software.\nMinimally reproducible (acceptable) - all data processing and analysis steps can be reproduced on any research team members computer without any modifications needed.\nModerately reproducible (desired) - meets the minimal level plus other people not involved in the research project can reproduce the steps with minimal modifications.\nHighly reproducible (good luck!) - fully reproducible without major modifications needed by people not involved in the research project 5 - 10+ years from now.\n\nA minimal level of reproducibility is still acceptable, as achieving more requires significant time and effort. We should strive for a moderate amount of reproducibility but achieving it requires more than just writing code. Your code must be organized, easy to understand, and include notes and documentation. Even if you or someone else attempts to reproduce your steps in the future, they can modify the code to make it work. The highest level of reproducibility is difficult to achieve due to software and code updates. Your code may only work with the current version of R or other packages. There are solutions to this problem of software and code updates, but who knows if those will work in the future!\n\nSimply using R for data analysis does not guarantee that your workflow is reproducible. In fact, there are many non-reproducible ways to use R. To ensure at least a moderate level of reproducibility, consider the following criteria (this is not an exhaustive list):\n\nYour statistical analysis (the final step) can be fully reproduced from the raw data files and your R scripts\nYour code can be reproduced on other computers without any modifications\nYour data and R scripts are organized and documented in a way that makes them easily understandable\n\nThis last criterion is extremely important, but is often overlooked. If others cannot understand your workflow, then it is not moderately reproducible. Therefore, it is important to take the time and think about the organization of your project, files, data, and scripts.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "data-proc-overview.html#compile-raw-data",
    "href": "data-proc-overview.html#compile-raw-data",
    "title": "1  Overview",
    "section": "Compile Raw Data",
    "text": "Compile Raw Data\nData is collected locally on running room computers in the lab, each with individual subject files for every session and task. The data is organized by session/task on each of these computers but we need to organize the folders just by /task and to also compile the data from all the computers in one location.\nWe use a copy_to_drive.R script to compile the raw data from all the computers onto a Network Drive.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can find a template for the copy_to_drive.R script file on the Network Drive",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "data-proc-overview.html#sharepoint---data-collection",
    "href": "data-proc-overview.html#sharepoint---data-collection",
    "title": "1  Overview",
    "section": "SharePoint - Data Collection",
    "text": "SharePoint - Data Collection\nWe use the Network Drive to transfer data files to the lab’s SharePoint, where the data will be permanently stored. A SharePoint/Data Collection/[Study Name] directory is where ALL the raw data files for that data collection effort are stored.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "data-proc-overview.html#sharepoint---data-analysis",
    "href": "data-proc-overview.html#sharepoint---data-analysis",
    "title": "1  Overview",
    "section": "SharePoint - Data Analysis",
    "text": "SharePoint - Data Analysis\nRecall that a single data collection effort will contain multiple research projects, all with their own combination of tasks and decisions about processing, scoring, and cleaning the data. To maintain reproducibility across these research projects, we need to create a separate Data Analysis repository for a single data collection effort.\nOnce you create a data analysis repository in SharePoint/Data Analysis, you can copy over data files for the tasks specific to that research project. In SharePoint/Data Analysis/[Project Name] is where the data is processed, scored, cleaned, and analyzed for that research project.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "compile-raw-data.html",
    "href": "compile-raw-data.html",
    "title": "2  Compile Raw Data",
    "section": "",
    "text": "There are three stages of processing and analyzing data in our lab\n\nData Preparation: Convert messy raw data files into tidy raw data files\nData Scoring: Calculate aggregate scores, clean data, remove outliers, and more\nData Analysis: Perform statistical analysis and visualize data\n\nAll the steps in our data processing workflow can be fully reproduced with only the original raw data files and R scripts.\nBut first, we need to compile, organize, and store the raw data files.\n\nCompile Raw Data\nData is collected locally on running room computers in the lab, each with individual subject files for every session and task. The data is organized by session/task on each of these computers but we need to organize the folders just by /task and to also compile the data from all the computers in one location.\nWe use a copy_to_drive.R script to compile the raw data from all the computers onto a Network Drive.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can find a template for the copy_to_drive.R script file on the Network Drive\n\n\n\n\nSharePoint - Data Collection\nWe use the Network Drive to transfer data files to the lab’s SharePoint, where the data will be permanently stored. A SharePoint/Data Collection/[Study Name] directory is where ALL the raw data files for that data collection effort are stored.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Compile Raw Data</span>"
    ]
  },
  {
    "objectID": "setup-analysis.html",
    "href": "setup-analysis.html",
    "title": "\n3  Setup Project\n",
    "section": "",
    "text": "Setup Folders\n\nCreate a folder for your project that contains the three following folders\n\n📁 analyses\n📁 data\n📁 R\n\nInside of the data folder create a raw/messy folder and a scored folder\n\n📁 data\n   📁 raw\n      📁 messy\n   📁 scored\n\nCreate an RStudio Project in the project’s root directory\n\n\nFile -&gt; New Project… -&gt; Existing Directory\n\nCopy Data Files\nOnce you create a data analysis repository in SharePoint/Data Analysis, you can copy over data files for the tasks specific to that research project.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnly copy over the messy raw data files, otherwise your project will not be fully reproducible\n\n\nDownload R Script Templates\nThe englelab package contains R script templates you can download\nIn the console window, type:\n\nenglelab::get_template(raw_script = TRUE, score_script = TRUE, \n                       merge_script = TRUE, analysis_script = TRUE, \n                       wmc_scripts = TRUE, ac_scripts = TRUE, \n                       main_script = TRUE)\n\nCopy From Other Projects\nIf R scripts already exist in other projects for processing data from tasks you are using in your current project, then you can also just copy and paste those over to your project.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup Project</span>"
    ]
  },
  {
    "objectID": "data-prep.html",
    "href": "data-prep.html",
    "title": "\n4  Data Preparation\n",
    "section": "",
    "text": "Overview of Template\nYou can download an R script template to convert messy raw data files into tidy raw data files:\nenglelab::get_template(raw_script = TRUE)",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#setup",
    "href": "data-prep.html#setup",
    "title": "\n4  Data Preparation\n",
    "section": "Setup",
    "text": "Setup\n\n# ---- Setup -------------------------------------------------------------------\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr) # delete if not importing a batch of files\n\n# directories\nimport_dir &lt;- \"data/raw/messy\"\noutput_dir &lt;- \"data/raw\"\n\n# file names\ntask &lt;- \"taskname\"\nimport_file &lt;- paste(task, \".txt\", sep = \"\")\noutput_file &lt;- paste(task, \"raw.csv\", sep = \"_\")\n# ------------------------------------------------------------------------------\n\nThe Setup section is to:\n\nLoad any packages used in the script\nSet the directories of where to import and output data files to\nSet the file names of the data files to be imported and outputted\n\nI like to include the directory and file names at the top of the script that way it is easy to see what is being imported/outputted and from where right at the top of the script rather than having to search through the script for this information.\nWe can then use the import_dir , output_dir, import_file, and output_file variables in the script when we import and output a data file.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#import",
    "href": "data-prep.html#import",
    "title": "\n4  Data Preparation\n",
    "section": "Import",
    "text": "Import\nTwo different import options are included in the script:\n\n\nImport a single file: You can use the standard import functions from readr to do this\n\nImport multiple files and merge them: To do this, you need to use purrr::map_df() with a readr import function\n\n\n# ---- Import Data -------------------------------------------------------------\n# to import a single file\ndata_import &lt;- read_delim(here(import_dir, import_file), delim = \"\\t\",\n                          escape_double = FALSE, trim_ws = TRUE)\n\n# alternatively to import a batch of files...\n# change the arguments in purrr::map_df() depending on type of data files\n# this example is for files created from eprime and needs encoding = \"UCS-2LE\"\nfiles &lt;- list.files(here(import_dir, task), pattern = \".txt\", full.names = TRUE)\ndata_import &lt;- files |&gt;\n  map_df(~ read_delim(.x, locale = locale(encoding = \"UCS-2LE\"), delim = \"\\t\",\n                      escape_double = FALSE, trim_ws = TRUE, na = \"NULL\"))\n# ------------------------------------------------------------------------------",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#tidy-data",
    "href": "data-prep.html#tidy-data",
    "title": "\n4  Data Preparation\n",
    "section": "Tidy Data",
    "text": "Tidy Data\nThis is the meat of the script, where the action happens. It will also be different for every task, obviously. I will cover common steps in more detail below.\n\n# ---- Tidy Data ---------------------------------------------------------------\ndata_raw &lt;- data_import |&gt;\n  rename() |&gt;\n  filter() |&gt;\n  mutate() |&gt;\n  select()\n# ------------------------------------------------------------------------------",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#save-data",
    "href": "data-prep.html#save-data",
    "title": "\n4  Data Preparation\n",
    "section": "Save Data",
    "text": "Save Data\nNo need to change anything here. Isn’t that nice?\n\n# ---- Save Data ---------------------------------------------------------------\nwrite_csv(data_raw, here(output_dir, output_file))\n# ------------------------------------------------------------------------------\n\nrm(list = ls())",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#combine-information-across-columns",
    "href": "data-prep.html#combine-information-across-columns",
    "title": "\n4  Data Preparation\n",
    "section": "Combine information across columns",
    "text": "Combine information across columns\nYou can probably guess that the data contained in slide1.resp and slide2.resp contains the same information about what response was made. But one is for practice trials and the other for real trials. This should just be combined in one column. Same thing with slide1.rt and slide2.rt.\nThere is a convenient function to do this, dplyr::coalesce()\n\n\nCode\nData Frame\n\n\n\n\ndata_raw &lt;- data_import |&gt;\n  rename(TrialProc = `Proc Trial`) |&gt;\n  filter(TrialProc == \"prc\" | TrialProc == \"tsk\") |&gt;\n  mutate(Response = coalesce(slide1.resp, slide2.resp),\n         RT = coalesce(slide1.rt, slide2.rt))\n\nView the Data Frames tab to see the resulting data frames\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#values-in-columns-dont-make-sense",
    "href": "data-prep.html#values-in-columns-dont-make-sense",
    "title": "\n4  Data Preparation\n",
    "section": "Values in columns don’t make sense",
    "text": "Values in columns don’t make sense\nSome common examples\n\nThe trial number continues across practice practice and real trials\nCondition has values of 0 and 1. What do 0 and 1 mean???\nTrialProc has values that can be more clear\nValues in columns Response and Ans do not make sense. What do 1, 2, and 3 mean???\n\n\n\nCode\nData Frame\n\n\n\n\ndata_raw &lt;- data_import |&gt;\n  rename(TrialProc = `Proc Trial`) |&gt;\n  filter(TrialProc == \"prc\" | TrialProc == \"tsk\") |&gt;\n  mutate(Response = coalesce(slide1.resp, slide2.resp),\n         RT = coalesce(slide1.rt, slide2.rt),\n         Condition = case_when(COND == 0 ~ \"congruent\",\n                               COND == 1 ~ \"incongruent\"),\n         TrialProc = case_when(TrialProc == \"prc\" ~ \"practice\",\n                               TrialProc == \"tsk\" ~ \"real\"),\n         Response = case_when(Response == 1 ~ \"red\",\n                              Response == 2 ~ \"green\",\n                              Response == 3 ~ \"blue\"),\n         Correct_Response = case_when(Ans == 1 ~ \"red\",\n                                      Ans == 2 ~ \"green\",\n                                      Ans == 3 ~ \"blue\")) |&gt;\nmutate(.by = c(ID, TrialProc),\n         Trial = row_number())\n\nView the Data Frame tab to see the resulting data frame\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-prep.html#add-additional-columns",
    "href": "data-prep.html#add-additional-columns",
    "title": "\n4  Data Preparation\n",
    "section": "Add additional columns",
    "text": "Add additional columns\nYou may have noticed that there is no column corresponding to accuracy. We have Response and Correct_Response information so we can create a new column for Accuracy\n\n\nCode\nData Frame\n\n\n\n\ndata_raw &lt;- data_import |&gt;\n  rename(TrialProc = `Proc Trial`) |&gt;\n  filter(TrialProc == \"prc\" | TrialProc == \"tsk\") |&gt;\n  mutate(Response = coalesce(slide1.resp, slide2.resp),\n         RT = coalesce(slide1.rt, slide2.rt),\n         Condition = case_when(COND == 0 ~ \"congruent\",\n                               COND == 1 ~ \"incongruent\"),\n         TrialProc = case_when(TrialProc == \"prc\" ~ \"practice\",\n                               TrialProc == \"tsk\" ~ \"real\"),\n         Response = case_when(Response == 1 ~ \"red\",\n                              Response == 2 ~ \"green\",\n                              Response == 3 ~ \"blue\"),\n         Correct_Response = case_when(Ans == 1 ~ \"red\",\n                                      Ans == 2 ~ \"green\",\n                                      Ans == 3 ~ \"blue\"),\n         Accuracy = case_when(Response == Correct_Response ~ 1,\n                              Response != Correct_Response ~ 0)) |&gt;\nmutate(.by = c(ID, TrialProc),\n         Trial = row_number())\n\nView the Data Frame tab to see the resulting data frames\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "data-score.html",
    "href": "data-score.html",
    "title": "\n5  Data Scoring\n",
    "section": "",
    "text": "Overview of Template\nYou can download an R script template to score tidy raw data files:\nenglelab::get_template(score_script = TRUE)",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#setup",
    "href": "data-score.html#setup",
    "title": "\n5  Data Scoring\n",
    "section": "Setup",
    "text": "Setup\n\n# ---- Setup -------------------------------------------------------------------\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(englelab)    # for data cleaning functions\nlibrary(tidyr)       # for pivot_wider. delete if not using\nlibrary(psych)       # for cronbach's alpha. delete if not using\n\n# directories\nimport_dir &lt;- \"data/raw\"\noutput_dir &lt;- \"data/scored\"\n\n# file names\ntask &lt;- \"taskname\"\nimport_file &lt;- paste(task, \"raw.csv\", sep = \"_\")\noutput_file &lt;- paste(task, \"Scores.csv\", sep = \"_\")\n\n## data cleaning parameters\noutlier_cutoff &lt;- 3.5\n# ------------------------------------------------------------------------------\n\nThe Setup section is to:\n\nLoad any packages used in the script\nSet the directories of where to import and output data files to\nSet the file names of the data files to be imported and outputted\nSet data cleaning parameters (e.g., outlier criterion)",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#import",
    "href": "data-score.html#import",
    "title": "\n5  Data Scoring\n",
    "section": "Import",
    "text": "Import\nGiven that 1) you created a tidy raw data file in .csv format, and 2) you specified import_dir and import_file in the setup section, you most likely do not need to change anything here.\nOptionally, you might want to go ahead and filter out any rows that you absolutely do not want to include at any point in scoring the data (e.g., practice trials or certain subjects).\n\n# ---- Import Data -------------------------------------------------------------\ndata_import &lt;- read_csv(here(import_dir, import_file)) |&gt;\n  filter()\n# ------------------------------------------------------------------------------",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#score-data",
    "href": "data-score.html#score-data",
    "title": "\n5  Data Scoring\n",
    "section": "Score Data",
    "text": "Score Data\nThe Score Data section is where most of the work needs to be done. You should be using dplyr and possibly tidyr to do most of the work here, though you may need other packages and functions. You can delete whatever is in there, that is just a placeholder as an example of the type of functions you might use.\n\n# ---- Score Data --------------------------------------------------------------\ndata_scores &lt;- data_import |&gt;\n  summarise(.by = Subject)\n# ------------------------------------------------------------------------------",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#clean-data",
    "href": "data-score.html#clean-data",
    "title": "\n5  Data Scoring\n",
    "section": "Clean Data",
    "text": "Clean Data\nThe next section of the script template is for cleaning the data by removing problematic subjects and/or removing outliers.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#reliability",
    "href": "data-score.html#reliability",
    "title": "\n5  Data Scoring\n",
    "section": "Reliability",
    "text": "Reliability\nThere are two standard ways of calculating reliability: split-half and cronbach’s alpha. The script template provides some template code for calculating both of these.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#save-data",
    "href": "data-score.html#save-data",
    "title": "\n5  Data Scoring\n",
    "section": "Save Data",
    "text": "Save Data\nNo need to change anything here. Isn’t that nice?\n\n# ---- Save Data ---------------------------------------------------------------\nwrite_csv(data_raw, here(output_dir, output_file))\n# ------------------------------------------------------------------------------\n\nrm(list = ls())",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#between-subjects-anova",
    "href": "data-score.html#between-subjects-anova",
    "title": "\n5  Data Scoring\n",
    "section": "Between-Subjects ANOVA",
    "text": "Between-Subjects ANOVA\nCondition is not a between-subject variable in this sample data but if you have a between-subject variable or design, you will want to keep the column(s) for that variable(s) in long format.",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#within-subjects-anova",
    "href": "data-score.html#within-subjects-anova",
    "title": "\n5  Data Scoring\n",
    "section": "Within-Subjects ANOVA",
    "text": "Within-Subjects ANOVA\nCondition is a within-subject variable in this sample data. Whether you want this variable in long or wide format depends on whether you will use R or JASP to analyze the data.\n\nFor R, you can keep it in long format\nFor JASP, you will need to restructure it to wide format\n\n\n\nCode\nData Frame\n\n\n\n\n# set criterion in setup section of script\nrt_short_criterion &lt;- 200\nrt_long_criterion &lt;- 5000\n\ndata_scores &lt;- data_import |&gt;\n  filter(RT &gt;= rt_short_criterion, RT &lt;= rt_long_criterion) |&gt;\n  summarise(.by = c(ID, Condition),\n            Accuracy.mean = mean(Accuracy, na.rm = TRUE),\n            RT.mean = mean(RT, na.rm = TRUE),\n            RT.sd = sd(RT, na.rm = TRUE)) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = Condition,\n              values_from = c(Accuracy.mean, RT.mean, RT.sd),\n              names_glue = \"{Condition}_{.value}\")\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#correlation-and-regression",
    "href": "data-score.html#correlation-and-regression",
    "title": "\n5  Data Scoring\n",
    "section": "Correlation and Regression",
    "text": "Correlation and Regression\nFor correlation and regression you will want to restructure the data to wide format (same as the code above).\nIf you have multiple data files from different measures or tasks, that you eventually want to merge into a single data frame for analysis, it is a good idea to add the measure or task name to the column names. That way when you merge the data you know which column corresponds to which task.\n\n\nCode\nData Frame\n\n\n\n\n# set criterion in setup section of script\nrt_short_criterion &lt;- 200\nrt_long_criterion &lt;- 5000\n\ndata_scores &lt;- data_import |&gt;\n  filter(RT &gt;= rt_short_criterion, RT &lt;= rt_long_criterion) |&gt;\n  summarise(.by = c(ID, Condition),\n            Accuracy.mean = mean(Accuracy, na.rm = TRUE),\n            RT.mean = mean(RT, na.rm = TRUE),\n            RT.sd = sd(RT, na.rm = TRUE)) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = Condition,\n              values_from = c(Accuracy.mean, RT.mean, RT.sd),\n              names_glue = \"{task}_{Condition}_{.value}\")\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#remove-problematic-subjects",
    "href": "data-score.html#remove-problematic-subjects",
    "title": "\n5  Data Scoring\n",
    "section": "Remove Problematic Subjects",
    "text": "Remove Problematic Subjects\nDepending on the task, problematic subjects can be detected in different ways. For this example data we will simply remove subjects that had less than chance performance on congruent trials (were just guessing or did not understand the task).\nTo do so, we will use a custom function created for this purpose, remove_problematic() from our englelab package. ?englelab::remove_problematic\nThe main argument is remove =. This argument takes a logical statement (e.g., remove = Accuracy.mean &lt;= .5).\nThe other argument, that is more optional, is log_file =. This allows us to save a data file containing only the subjects that were removed. This is good if we later on want to report in publications how many subjects were removed.\n\n\nCode\nData Frame\n\n\n\n\n# set criterion in setup section of script\nacc_criterion &lt;- .34\n\ndata_cleaned &lt;- data_scores |&gt;\n  remove_problematic(\n    remove = \"Stroop_congruent_Accuracy.mean &lt;= acc_criterion\",\n    log_file = here(\"data/logs\", paste(task, \"_problematic.csv\", sep = \"\")))\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#remove-outliers",
    "href": "data-score.html#remove-outliers",
    "title": "\n5  Data Scoring\n",
    "section": "Remove Outliers",
    "text": "Remove Outliers\nRemove outliers based on their final task scores. A typical way we remove outliers is by setting their score to missing NA if it is 3.5 standard deviations above or below the mean.\nTo do so, we will use a custom function created for this purpose, replace_outliers from our englelab package.\nThere are several arguments that need to be defined. See ?englelab::replace_outliers for a description on this.\nThe fully piped |&gt; code for this entire section looks like:\n\n\nCode\nData Frame\n\n\n\n\n# set criterion in setup section of script\nacc_criterion &lt;- .34\noutlier_criterion &lt;- 3.5\n\ndata_cleaned &lt;- data_scores |&gt;\n  remove_problematic(\n    remove = \"Stroop_congruent_Accuracy.mean &lt;= acc_criterion\",\n    log_file = here(\"data/logs\", paste(task, \"_problematic.csv\", sep = \"\"))) |&gt;\n  replace_outliers(\n    variables = \"Stroop_incongruent_RT.mean\",\n    cutoff = outlier_criterion,\n    with = \"NA\",\n    pass = 1,\n    id = \"ID\",\n    log_file = here(\"data/logs\", paste(task, \"_outliers.csv\", sep = \"\"))) |&gt;\n  filter(!is.na(Stroop_congruent_RT.mean))\n\nProblematic subjects removed: 1\n\n\nOutliers detected (pass = 1): 0\n\n\nNotice that filter(!is.na()) is specified after replace_outliers. This removes any of the outliers from the dataframe.\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#split-half-reliability",
    "href": "data-score.html#split-half-reliability",
    "title": "\n5  Data Scoring\n",
    "section": "Split-half reliability",
    "text": "Split-half reliability\nFirst trials need to be split between even and odd trials (or whatever split-half one is using).\n\n\nCode\nData Frame\n\n\n\n\nsplithalf &lt;- reliability |&gt;\n  mutate(.by = ID,\n         Split = ifelse(Trial %% 2, \"odd\", \"even\"))\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns\n\n\n\n  \n\n\n\n\n\n\nThen scores can be re-calculated based on the even/odd splits. Everything that was done to score the data needs to be included, such as trial-level cleaning.\n\n\nCode\nData Frame\n\n\n\n\nsplithalf &lt;- reliability |&gt;\n  mutate(.by = ID,\n         Split = ifelse(Trial %% 2, \"odd\", \"even\")) |&gt;\n  filter(RT &gt;= rt_short_criterion, RT &lt;= rt_long_criterion) |&gt;\n  summarise(.by = c(ID, Condition, Split),\n            RT.mean = mean(RT, na.rm = TRUE)) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = c(Condition, Split),\n              values_from = RT.mean)\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns\n\n\n\n  \n\n\n\n\n\n\nFinally, spearman-brown corrected split-half reliability can be calculated and added to the data frame\n\n\nCode\nData Frame\n\n\n\n\nsplithalf &lt;- reliability |&gt;\n  mutate(.by = ID,\n         Split = ifelse(Trial %% 2, \"odd\", \"even\")) |&gt;\n  filter(RT &gt;= rt_short_criterion, RT &lt;= rt_long_criterion) |&gt;\n  summarise(.by = c(ID, Condition, Split),\n            RT.mean = mean(RT, na.rm = TRUE)) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = c(Condition, Split),\n              values_from = RT.mean) |&gt;\n  summarise(r = cor(incongruent_even, incongruent_odd)) |&gt;\n  mutate(r = (2 * r) / (1 + r))\n\ndata_cleaned$Stroop_incongruent_RT.mean_splithalf &lt;- splithalf$r\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score.html#cronbachs-alpha",
    "href": "data-score.html#cronbachs-alpha",
    "title": "\n5  Data Scoring\n",
    "section": "Cronbach’s alpha",
    "text": "Cronbach’s alpha\nCronbach’s alpha is easier to calculate because we do not need to re-calculate the scores.\nThe first step is to get the data frame setup such that trial number is spread across columns\n\n\nCode\nData Frame\n\n\n\n\ncronbachalpha &lt;- reliability |&gt;\n  filter(RT &gt;= rt_short_criterion, RT &lt;= rt_long_criterion) |&gt;\n  select(ID, Trial, Accuracy) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = Trial,\n              values_from = Accuracy)\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns\n\n\n\n  \n\n\n\n\n\n\nThen we can use psych::alpha() to calculate the average correlation between items and save it to the data frame\n\n\nCode\nData Frame\n\n\n\n\ncronbachalpha &lt;- reliability |&gt;\n  select(ID, Trial, Accuracy) |&gt;\n  pivot_wider(id_cols = ID,\n              names_from = Trial,\n              values_from = Accuracy) |&gt;\n  select(-ID) |&gt;\n  alpha()  # from the psych package\n\nSome items ( 1 ) were negatively correlated with the total scale and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\ndata_cleaned$Stroop_incongruent_RT.mean_cronbachalpha &lt;- cronbachalpha$total$std.alpha\n\n\n\nUse the small arrow ▸ at the end of the column names to see more columns",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Scoring</span>"
    ]
  },
  {
    "objectID": "data-score-merge.html",
    "href": "data-score-merge.html",
    "title": "\n6  Data Scoring - Merge\n",
    "section": "",
    "text": "At this stage of data processing you have created multiple data files containing the task scores, reliabilities, and other variables for each task. The next step is to create a single merged data file containing the primary task scores for each task that you will be performing data analysis on. We can also create data files with the reliabilities, administration times, and a log of the data cleaning steps.\n\nYou can download an R script template to score tidy raw data files:\n\nenglelab::get_template(score_script = TRUE)\n\nSetup\n\n# ---- Setup -------------------------------------------------------------------\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\n\n# directories\nimport_dir &lt;- \"data/scored\"\noutput_dir &lt;- \"data\"\n\n# file names\noutput_scores &lt;- \"TaskScores.csv\"\noutput_reliabilities &lt;- \"Reliabilities.csv\"\noutput_admintimes &lt;- \"AdminTimes.csv\"\noutput_datacleaning &lt;- \"DataCleaning_log.csv\"\n# ------------------------------------------------------------------------------\n\nImport\nTo import and merge multiple data frames we can use purrr::map() |&gt; purrr:reduce()\n\n# ---- Import Data -------------------------------------------------------------\nfiles &lt;- list.files(here(import_dir), pattern = \"Scores\", full.names = TRUE)\ndata_import &lt;- files |&gt;\n  map(read_csv) |&gt;\n  reduce(full_join, by = \"Subject\")\n# ------------------------------------------------------------------------------\n\nTask Scores\n\n# ---- Select Variables --------------------------------------------------------\ndata_scores &lt;- data_import |&gt;\n  select(Subject) |&gt;\n  filter()\n\n# list of final subjects\nsubjlist &lt;- select(data_scores, Subject)\n# ------------------------------------------------------------------------------\n\nAt this step, you may also want to filter out subjects that have missing data on specific tasks, or too much missing data across all the tasks.\nI advise creating a final subject list of all subjects that made it to this state of data processing.\nReliabilities\n\n# ---- Reliabilities -----------------------------------------------------------\ndata_reliabilities &lt;- data_import |&gt;\n  select(contains(\"splithalf\"), contains(\"cronbach_alpha\")) |&gt;\n  drop_na() |&gt;\n  distinct() |&gt;\n  pivot_longer(everything(),\n               names_to = c(\"Task\", \"metric\"),\n               names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\",\n               values_to = \"value\") |&gt;\n  pivot_wider(id_col = Task,\n              names_from = metric,\n              values_from = value)\n# ------------------------------------------------------------------------------\n\nThe code in pivot_longer(names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\") follows a specific naming scheme used for column names. Underscores (optional) can be used for task names and column descriptions, but a period (required) is ONLY used to separate the task name / description from the description of the task score type / what the value represents (e.g., RT = reaction time, splithalf = split-half reliability)\nTask_Name_MoreStuff.ScoreType\ne.g., StroopDL_Last4Rev.ResponseDeadline, VAorient_S.k, Antisaccade.ACC\nThis should also be used for reliabilities and admin times:\ne.g., StroopDL_Last4Rev.splithalf, StroopDL.AdminTime\nAdmin Times\n\n# ---- Admin Times -------------------------------------------------------------\ndata_merge &lt;- data_import |&gt;\n  select(contains(\"AdminTime\")) |&gt;\n  summarise_all(list(mean = mean, sd = sd), na.rm = TRUE) |&gt;\n  pivot_longer(everything(),\n               names_to = c(\"Task\", \"metric\"),\n               names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\",\n               values_to = \"value\") |&gt;\n  mutate(value = round(value, 3)) |&gt;\n  pivot_wider(id_col = Task,\n              names_from = metric,\n              names_prefix = \"AdminTime.\",\n              values_from = \"value\")\n# ------------------------------------------------------------------------------\n\nData Cleaning Log\nThere will be two sets of cleaning logs we need to import\n\nProblematic subjects\nOutliers\n\n\n# ---- Data Cleaning Log -------------------------------------------------------\n# problematic subjects\ndata_problematic &lt;- list.files(here(\"data/logs\"), \n                                 pattern = \"problematic\", \n                                 full.names = TRUE) %&gt;%\n  map(read_csv) |&gt;\n  map(function(x) {\n    reframe(x, tibble(Task = gsub(\"\\\\..*\", \"\", colnames(select(x, 2))), \n                       Problematic_Removed = nrow(x)))\n  }) |&gt;\n  bind_rows()\n\n# outliers\ndata_outliers &lt;- list.files(here(\"data/logs\"), \n                                 pattern = \"outliers\", \n                                 full.names = TRUE) %&gt;%\n  map(read_csv) |&gt;\n  map(function(x) {\n    reframe(x, tibble(Task = gsub(\"\\\\..*\", \"\", colnames(select(x, 2))),\n                      Outliers_Removed = nrow(x),\n                      Outliers_Passes = max(pull(x, Pass))))\n  }) |&gt;\n  bind_rows()\n\n# merge\ndata_log &lt;- merge(data_problematic, data_outliers, by = \"Task\", all = TRUE)\n# ------------------------------------------------------------------------------\n\nSave Data\n\n# ---- Save Data ---------------------------------------------------------------\nwrite_csv(data_scores, here(output_dir, output_scores))\nwrite_csv(data_reliabilities, here(output_dir, output_reliabilities))\nwrite_csv(data_reliabilities, here(output_dir, output_admintimes))\nwrite_csv(data_log, here(output_dir, output_datacleaning))\nwrite_csv(subjlist, here(output_dir, \"subjlist_final.csv\"))\n# ------------------------------------------------------------------------------\n\nrm(list = ls())",
    "crumbs": [
      "Data Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Scoring - Merge</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html",
    "href": "data-analysis-anova.html",
    "title": "7  ANOVA",
    "section": "",
    "text": "ggplot2 Theme\nIt can be nice to set a global ggplot2 theme that is applied to all ggplots. Here is some code on how to 1) create a custom theme e.g., theme_spacious and 2) how to set the global theme based on some combination of custom and template themes.\ntheme_spacious &lt;- function(font.size = 14, bold = TRUE){\n  key.size &lt;- trunc(font.size * .8)\n  if (bold == TRUE) {\n    face.type &lt;- \"bold\"\n  } else {\n    face.type &lt;- \"plain\"\n  }\n\n  theme(text = element_text(size = font.size),\n        axis.title.x = element_text(margin = margin(t = 15, r = 0,\n                                                    b = 0, l = 0),\n                                    face = face.type),\n        axis.title.y = element_text(margin = margin(t = 0, r = 15,\n                                                    b = 0, l = 0),\n                                    face = face.type),\n        legend.title = element_text(face = face.type),\n        legend.spacing = unit(20, \"pt\"),\n        legend.text = element_text(size = key.size),\n        plot.title = element_text(face = face.type, hjust = .5,\n                                  margin = margin(b = 10)),\n        plot.caption = element_text(hjust = 0, size = key.size,\n                                    margin = margin(t = 20)),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\",\n                                  face = face.type))\n}\n\noutput_theme &lt;- theme_linedraw() + \n  theme_spacious(font.size = 12) + \n  theme(panel.border = element_rect(color = \"gray\"),\n        axis.line.x = element_line(color = \"gray\"),\n        axis.line.y = element_line(color = \"gray\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\ntheme_set(output_theme)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#table-theme",
    "href": "data-analysis-anova.html#table-theme",
    "title": "7  ANOVA",
    "section": "Table Theme",
    "text": "Table Theme\nIt can also be nice to customize how tables from statistical analyses are displayed. Here is some code on how to define a table theme using the knitr and kableExtra packages. You will see that we will pass tables to table_theme() with some customization for the number of digits to round to, adding a table title, and footnotes.\n\ntable_theme &lt;- function(x, digits = 3, title = NULL, note = NULL) {\n  kable(x, digits = digits, caption = title, row.names = FALSE) |&gt;\n    kable_classic(position = \"left\") |&gt;\n    kable_styling(full_width = FALSE, position = \"left\") |&gt;\n    footnote(general = note)\n}",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#specify-factor-levels",
    "href": "data-analysis-anova.html#specify-factor-levels",
    "title": "7  ANOVA",
    "section": "Specify factor levels",
    "text": "Specify factor levels\nWhen dealing with categorical variables for statistical analyses in R, it is usually a good idea to define the order of the categories as this will by default determine which category is treated as the reference (comparison group).\nLet’s set factor levels for Memory Strategy and Presentation Rate\nRemember you can use colnames() to get the columns in a data frame and unique() to evaluate the unique values in a column.\n\nrecall_data &lt;- recall_data |&gt;\n  mutate(Memory_Strategy = factor(Memory_Strategy,\n                                    levels = c(\"Rote Repetition\", \n                                               \"Visual Imagery\")),\n         Presentation_Rate = factor(Presentation_Rate,\n                                    levels = c(1, 2, 4)))",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#anova",
    "href": "data-analysis-anova.html#anova",
    "title": "7  ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\nDepending on your factor design, you may need to perform different types of ANOVAs. We have a 2 x 3 mixed-factorial design and so will ultimately want to perform a Two-way ANOVA with a between-subject and a within-subject factors. However, for the sake of this exercise, let’s walk through the different types of ANOVAs.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#t-test",
    "href": "data-analysis-anova.html#t-test",
    "title": "7  ANOVA",
    "section": "t-test",
    "text": "t-test\nA t-test can be performed to test whether a difference between 2 means is statistically significant. There are three general types of t-tests.\n\nOne-sample t-test: Used to compare a sample mean to a population mean.\nTwo-sample t-test for independent samples: Used to compare means from two different groups of subjects (between-subject factor).\nTwo-sample t-test for dependent samples: Used to compare means from two conditions with the same subjects (within-subject factor).\n\nThe t.test() function can be used to compute any of these t-tests.\nt-test - independent samples\nWe can perform a two-sample t-test for independent samples to compare recall performance for the group of subjects assigned to the rote repetition condition vs. those assigned to the visual imagery condition.\n\nt_ms &lt;- t.test(recall_data$Recall_Performance ~\n                 recall_data$Memory_Strategy, \n               var.equal = TRUE)\n\nWe can then use model_parameters() , from parameters, to get the test statistics and cohens_d() , from modelbased to get the standardized effect size estimate.\n\nmodel_parameters(t_ms) |&gt; \n  select(-Method, -Alternative) |&gt;\n  table_theme()\n\n\n\n\nParameter\nGroup\nMean_Group1\nMean_Group2\nDifference\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\nrecall_data$Recall_Performance\nrecall_data$Memory_Strategy\n13.71\n25.23\n-11.52\n0.95\n-13.724\n-9.316\n-10.289\n268\n0\n\n\n\n\ncohens_d(t_ms) |&gt;\n  select(-CI) |&gt;\n  table_theme()\n\n\n\n\nCohens_d\nCI_low\nCI_high\n\n\n-1.252\n-1.512\n-0.99\n\n\n\n\n\nt-test - dependent samples\nWe can perform a two-sample t-test for dependent samples to compare the three presentation rate conditions because this variable was a within-subject factor.\nTo do so, we need to create three different data frames for each of the pairwise comparisons (there is probably a simpler way to do this but this is a good opportunity to demonstrate a dplyr function). We can use filter() from dplyr to do this.\n\ndata_pr_1v2 &lt;- filter(recall_data, \n                      Presentation_Rate == 1 | Presentation_Rate == 2)\n\ndata_pr_1v4 &lt;- filter(recall_data, \n                      Presentation_Rate == 1 | Presentation_Rate == 4)\n\ndata_pr_2v4 &lt;- filter(recall_data, \n                      Presentation_Rate == 2 | Presentation_Rate == 4)\n\nt_pr_1v2 &lt;- t.test(data_pr_1v2$Recall_Performance ~\n                     data_pr_1v2$Presentation_Rate, \n                   var.equal = TRUE, paired = TRUE)\n\nt_pr_1v4 &lt;- t.test(data_pr_1v4$Recall_Performance ~\n                     data_pr_1v4$Presentation_Rate, \n                   var.equal = TRUE, paired = TRUE)\n\nt_pr_2v4 &lt;- t.test(data_pr_2v4$Recall_Performance ~\n                     data_pr_2v4$Presentation_Rate, \n                   var.equal = TRUE, paired = TRUE)\n\n\nmodel_parameters(t_pr_1v2) |&gt; \n  select(-Method, -Alternative) |&gt;\n  table_theme()\n\n\n\n\nParameter\nGroup\nDifference\nt\ndf_error\np\nCI\nCI_low\nCI_high\n\n\ndata_pr_1v2$Recall_Performance\ndata_pr_1v2$Presentation_Rate\n-6.093\n-5.906\n89\n0\n0.95\n-8.143\n-4.043\n\n\n\n\ncohens_d(t_pr_1v2) |&gt;\n  select(-CI) |&gt;\n  table_theme()\n\n\n\n\nCohens_d\nCI_low\nCI_high\n\n\n-0.623\n-0.847\n-0.395\n\n\n\n\nmodel_parameters(t_pr_1v4) |&gt; \n  select(-Method, -Alternative) |&gt;\n  table_theme()\n\n\n\n\nParameter\nGroup\nDifference\nt\ndf_error\np\nCI\nCI_low\nCI_high\n\n\ndata_pr_1v4$Recall_Performance\ndata_pr_1v4$Presentation_Rate\n-11.496\n-10.278\n89\n0\n0.95\n-13.718\n-9.273\n\n\n\n\ncohens_d(t_pr_1v4) |&gt;\n  select(-CI) |&gt;\n  table_theme()\n\n\n\n\nCohens_d\nCI_low\nCI_high\n\n\n-1.083\n-1.342\n-0.821\n\n\n\n\nmodel_parameters(t_pr_2v4) |&gt; \n  select(-Method, -Alternative) |&gt;\n  table_theme()\n\n\n\n\nParameter\nGroup\nDifference\nt\ndf_error\np\nCI\nCI_low\nCI_high\n\n\ndata_pr_2v4$Recall_Performance\ndata_pr_2v4$Presentation_Rate\n-5.402\n-4.694\n89\n0\n0.95\n-7.689\n-3.116\n\n\n\n\ncohens_d(t_pr_2v4) |&gt;\n  select(-CI) |&gt;\n  table_theme()\n\n\n\n\nCohens_d\nCI_low\nCI_high\n\n\n-0.495\n-0.713\n-0.275",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#section",
    "href": "data-analysis-anova.html#section",
    "title": "7  ANOVA",
    "section": "",
    "text": "One-way between-subject ANOVA\nOne-way within-subject ANOVA (also called repeated measures ANOVA)\nTwo-way mixed-factor ANOVA\n\nIn R, there are some different ways to conduct an ANOVA. We will use the afex package to conduct ANOVAs with aov_car().",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#one-way-between-subject-anova",
    "href": "data-analysis-anova.html#one-way-between-subject-anova",
    "title": "7  ANOVA",
    "section": "One-Way Between-Subject ANOVA",
    "text": "One-Way Between-Subject ANOVA\nA one-way between-subject ANOVA is conducted when there is only one factor (between-subject) in the study design.\nModel\n\nanova_bs &lt;- aov_car(Recall_Performance ~ \n                      Memory_Strategy + Error(Subject),\n                    data = recall_data)\n\nWarning: More than one observation per design cell, aggregating data using `fun_aggregate = mean`.\nTo turn off this warning, pass `fun_aggregate = mean` explicitly.\n\n\nContrasts set to contr.sum for the following variables: Memory_Strategy\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor between-subject designs, when there are multiple rows per subject in the data (due to the presence of a within-subject factor), performance needs to be averaged across the rows per subject. The Error(Subject) specification in the formula tells aov_car() what column name is that contain subject ids.\n\n\nUsing parameters and modelbased\n\nYou can use model_parameters() to get an ANOVA table. You should specify type = 3 to get Type III sum of squares. You can also request to obtain omega-squared (or eta-squared) effect size estimate.\n\nmodel_parameters(anova_bs, type = 3, \n                 effectsize_type = \"omega\", ci = .95) |&gt;\n  select(-Method) |&gt;\n  table_theme()\n\n\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\nOmega2\nOmega2_CI_low\nOmega2_CI_high\n\n\n\n(Intercept)\n34115.983\n1\n34115.983\n1285.352\n0\nNA\nNA\nNA\n\n\nMemory_Strategy\n2985.984\n1\n2985.984\n112.500\n0\n0.553\n0.441\n1\n\n\nResiduals\n2335.709\n88\n26.542\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nYou can use estimate_contrasts(), from modelbased, to get post-hoc comparisons.\n\nestimate_contrasts(anova_bs, contrast = \"Memory_Strategy\",\n                   p_adjust = \"tukey\") |&gt;\n  table_theme()\n\n\n\n\nLevel1\nLevel2\nDifference\nCI_low\nCI_high\nSE\ndf\nt\np\n\n\nRote Repetition\nVisual Imagery\n-11.52\n-13.678\n-9.362\n1.086\n88\n-10.607\n0\n\n\n\n\n\nUsing modeloutput\n\nMy modeloutput package provides a way to display ANOVA tables in output format similar to other statistical software packages like JASP or SPSS. Add anova_tables(contrast = \"Memory_Strategy\") to get a table for post-hoc comparisons.\nanova_tables(anova_bs, contrast = \"Memory_Strategy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Recall_Performance\n\n\nTerm\nSS\ndf\nMS\nF\np\nη2\n\nω2\n\n\n\n\n\n(Intercept)\n34115.983\n 1.000\n34115.983\n1285.352\n&lt;0.001\n\n\n\n\nMemory_Strategy\n 2985.984\n 1.000\n 2985.984\n 112.500\n&lt;0.001\n0.561\n0.553\n\n\nResiduals\n 2335.709\n88.000\n   26.542\n\n\n\n\n\n\n\n\nModel: aov_car(Recall_Performance ~ Memory_Strategy)\n\n\ndf correction: none\n\n\nN = 90\n\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Memory_Strategy\n\n\nLevel 1\nLevel 2\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\nRote Repetition\nVisual Imagery\n−11.520\n−13.678 — −9.362\n1.086\n88 \n−10.607\n&lt;0.001\n−1.490\n\n\np-values are uncorrected.\n\n\n\n\nFigures - ggplot2\n\nThe most customizable way to plot model results is using the ggplot2 package.\n\nggplot(recall_data, aes(x = Memory_Strategy, y = Recall_Performance,\n                        color = Memory_Strategy)) +\n  geom_point(position = position_jitter(width = .1), alpha = .2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Memory Strategy\", y = \"Recall Performance\") +\n  scale_color_brewer(palette = \"Set1\") +\n  guides(color = \"none\")\n\n\n\n\n\n\n\nFigures - raincloud plot\nMy modeloutput function has a geom_flat_violin() function to create the cloud part of the raincloud plot. There are some other modifications that have to be made to other elements of the ggplot as well.\n\nggplot(recall_data, aes(x = Memory_Strategy, y = Recall_Performance,\n                 color = Memory_Strategy, fill = Memory_Strategy)) +\n  geom_flat_violin(position = position_nudge(x = .1, y = 0),\n                   adjust = 1.5, trim = FALSE, \n                   alpha = .5, colour = NA) +\n  geom_point(aes(as.numeric(Memory_Strategy) - .15), \n             position = position_jitter(width = .05), alpha = .2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Memory Strategy\", y = \"Recall Performance\") +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  guides(fill = \"none\", color = \"none\")\n\nWarning: Using the `size` aesthetic with geom_polygon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\nFigures - sjPlot\n\nThe main package in R to create and customize plots is ggplot2. However, there is definitely a bit of a learning curve to ggplot2. Instead, the sjPlot package offers convenient ways to plot the results of statistical analyses using plot_model().\n\nplot_model(anova_bs, type = \"pred\", show.data = TRUE, jitter = TRUE)\n\n$Memory_Strategy",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#one-way-within-subject-anova",
    "href": "data-analysis-anova.html#one-way-within-subject-anova",
    "title": "7  ANOVA",
    "section": "One-Way Within-Subject ANOVA",
    "text": "One-Way Within-Subject ANOVA\nA one-way within-subject ANOVA (or repeated measures ANOVA) is conducted when there is only one factor (within-subject) in the study design.\nStatistically, the main difference between a between-subject factor and a within-subject factor is what goes into the error term. Recall that within-subject factor designs are more powerful. One reason for this is that the Error or Residual term in the model becomes smaller because Subject gets entered into the model as a variable (we are modelling the effect of differences between subjects). We need to specify the structure of the residual term for within-subject designs, which usually just involves specifying the column that identifies the subject id column.\nIn aov_car() we can specify the error term as Error(Subject/Within-Subject Factor).\n\nanova_ws &lt;- aov_car(Recall_Performance ~ \n                      Presentation_Rate + \n                      Error(Subject/Presentation_Rate),\n                    data = recall_data)\n\nUsing parameters and modelbased\n\nYou can use model_parameters() to get an ANOVA table. You should specify type = 3 to get Type III sum of squares. You can also request to obtain omega-squared (or eta-squared) effect size estimate.\n\nmodel_parameters(anova_ws, type = 3, \n                 effectsize_type = \"omega\", ci = .95) |&gt;\n  select(-Method) |&gt;\n  table_theme()\n\nWarning in summary.Anova.mlm(model$Anova): HF eps &gt; 1 treated as 1\n\n\n\n\n\nParameter\nSum_Squares\nSum_Squares_Error\ndf\ndf_error\nMean_Square\nF\np\nOmega2_partial\nOmega2_CI_low\nOmega2_CI_high\n\n\nPresentation_Rate\n5953.815\n9718.278\n1.968\n175.154\n55.484\n54.525\n0\n0.184\n0.102\n1\n\n\n\n\n\nYou can use estimate_contrasts(), from modelbased, to get post-hoc comparisons.\n\nestimate_contrasts(anova_ws, contrast = \"Presentation_Rate\",\n                   p_adjust = \"bonferroni\") |&gt;\n  table_theme()\n\n\n\n\nLevel1\nLevel2\nDifference\nCI_low\nCI_high\nSE\ndf\nt\np\n\n\n\nX1\nX2\n-6.093\n-8.611\n-3.576\n1.032\n89\n-5.906\n0\n\n\nX1\nX4\n-11.496\n-14.225\n-8.766\n1.118\n89\n-10.278\n0\n\n\nX2\nX4\n-5.402\n-8.210\n-2.594\n1.151\n89\n-4.694\n0\n\n\n\n\n\n\nUsing modeloutput\n\nMy modeloutput package provides a way to display ANOVA tables in output format similar to other statistical software packages like JASP or SPSS. Add anova_tables(contrast = \"Presentation_Rate\") to get a table for post-hoc comparisons.\nanova_tables(anova_ws, contrast = \"Presentation_Rate\")\nWarning in summary.Anova.mlm(model$Anova): HF eps &gt; 1 treated as 1\n\n\n\n\n\n\nANOVA Table: Recall_Performance\n\n\nTerm\nSS\nSS Error\ndf\ndf Error\nMS\nMS Error\nF\np\nηp2\n\nωp2\n\n\n\n\nPresentation_Rate\n5953.815\n9718.278\n1.968\n175.154\n55.484\n1.018\n54.525\n&lt;0.001\n0.380\n0.184\n\n\n\nModel: aov_car(Recall_Performance ~ (Presentation_Rate) + Error(Subject/(Presentation_Rate)))\n\n\ndf correction: Greenhouse-Geisser\n\n\nN = 90\n\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Presentation_Rate\n\n\nLevel 1\nLevel 2\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\n\n1\n2\n −6.093\n −8.143 — −4.043\n1.032\n89 \n −5.906\n&lt;0.001\n−0.562\n\n\n1\n4\n−11.496\n−13.718 — −9.273\n1.118\n89 \n−10.278\n&lt;0.001\n−1.060\n\n\n2\n4\n −5.402\n −7.689 — −3.116\n1.151\n89 \n −4.694\n&lt;0.001\n−0.498\n\n\n\np-values are uncorrected.\n\n\n\n\nFigures - ggplot2\n\nThe most customizable way to plot model results is using the ggplot2 package.\n\nggplot(recall_data, aes(x = Presentation_Rate, y = Recall_Performance)) +\n  geom_point(position = position_jitter(width = .1), alpha = .2) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1, group = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Presentation Rate\", y = \"Recall Performance\")\n\n\n\n\n\n\n\nFigures - raincloud plot\nMy modeloutput function has a geom_flat_violin() function to create the cloud part of the raincloud plot. There are some other modifications that have to be made to other elements of the ggplot as well.\n\nggplot(recall_data, aes(x = Presentation_Rate, y = Recall_Performance)) +\n  geom_flat_violin(position = position_nudge(x = .1, y = 0),\n                   adjust = 1.5, trim = FALSE, \n                   alpha = .5, fill = \"gray\", color = NA) +\n  geom_point(aes(as.numeric(Presentation_Rate) - .15), \n             position = position_jitter(width = .05), alpha = .2) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1, group = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Presentation_Rate\", y = \"Recall Performance\")\n\n\n\n\n\n\n\nFigures - sjPlot\n\nThe main package in R to create and customize plots is ggplot2. However, there is definitely a bit of a learning curve to ggplot2. Instead, the sjPlot package offers convenient ways to plot the results of statistical analyses using plot_model().\n\nplot_model(anova_ws, type = \"pred\", show.data = TRUE, jitter = TRUE)\n\n$Presentation_Rate",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-anova.html#two-way-mixed-factors-anova",
    "href": "data-analysis-anova.html#two-way-mixed-factors-anova",
    "title": "7  ANOVA",
    "section": "Two-Way Mixed-Factors ANOVA",
    "text": "Two-Way Mixed-Factors ANOVA\nA Two-way ANOVA is conducted when you have two factors in your design. The factors can be both between-subject factors, both within-subject factors, or a mix of between-subject and within-subject factors. In the case of the data we are working with here we have a Two-way mixed-factors ANOVA; or a 2 x 3 mixed-factors ANOVA.\n\nanova_2way &lt;- aov_car(Recall_Performance ~ \n                      Presentation_Rate*Memory_Strategy + \n                      Error(Subject/Presentation_Rate),\n                    data = recall_data)\n\nContrasts set to contr.sum for the following variables: Memory_Strategy\n\n\nUsing parameters and modelbased\n\nYou can use model_parameters() to get an ANOVA table. You should specify type = 3 to get Type III sum of squares. You can also request to obtain omega-squared (or eta-squared) effect size estimate.\n\nmodel_parameters(anova_2way, type = 3, \n                 effectsize_type = \"omega\", ci = .95) |&gt;\n  select(-Method) |&gt;\n  table_theme()\n\n\n\n\nParameter\nSum_Squares\nSum_Squares_Error\ndf\ndf_error\nMean_Square\nF\np\nOmega2_partial\nOmega2_CI_low\nOmega2_CI_high\n\n\n\nMemory_Strategy\n8957.952\n7007.126\n1.00\n88.000\n79.626\n112.500\n0.000\n0.553\n0.441\n1\n\n\nPresentation_Rate\n5953.815\n9445.486\n1.95\n171.619\n55.037\n55.469\n0.000\n0.260\n0.170\n1\n\n\nMemory_Strategy:Presentation_Rate\n272.792\n9445.486\n1.95\n171.619\n55.037\n2.541\n0.083\n0.010\n0.000\n1\n\n\n\n\n\n\nYou can use estimate_contrasts(), from modelbased, to get post-hoc comparisons.\n\nestimate_contrasts(anova_2way, \n                   contrast = \"Presentation_Rate\",\n                   at = \"Memory_Strategy\",\n                   p_adjust = \"bonferroni\") |&gt;\n  table_theme()\n\n\n\n\nLevel1\nLevel2\nMemory_Strategy\nDifference\nCI_low\nCI_high\nSE\ndf\nt\np\n\n\n\nX1\nX2\nRote Repetition\n-8.500\n-11.970\n-5.030\n1.422\n88\n-5.978\n0.000\n\n\nX1\nX2\nVisual Imagery\n-3.687\n-7.157\n-0.217\n1.422\n88\n-2.593\n0.033\n\n\nX1\nX4\nRote Repetition\n-13.149\n-16.983\n-9.315\n1.571\n88\n-8.369\n0.000\n\n\nX1\nX4\nVisual Imagery\n-9.842\n-13.677\n-6.008\n1.571\n88\n-6.265\n0.000\n\n\nX2\nX4\nRote Repetition\n-4.649\n-8.634\n-0.664\n1.633\n88\n-2.847\n0.016\n\n\nX2\nX4\nVisual Imagery\n-6.156\n-10.140\n-2.171\n1.633\n88\n-3.770\n0.001\n\n\n\n\n\n\nUsing modeloutput\n\nMy modeloutput package provides a way to display ANOVA tables in output format similar to other statistical software packages like JASP or SPSS. Add anova_tables(contrast = \"Presentation_Rate\", at = \"Memory_Strategy\") to get a table for post-hoc comparisons.\nanova_tables(anova_2way, \n             contrast = c(\"Presentation_Rate\", \"Memory_Strategy\"), \n             at = c(\"Presentation_Rate\", \"Memory_Strategy\"))\n\n\n\n\n\n\nANOVA Table: Recall_Performance\n\n\nTerm\nSS\nSS Error\ndf\ndf Error\nMS\nMS Error\nF\np\nηp2\n\nωp2\n\n\n\n\n\nMemory_Strategy\n8957.952\n7007.126\n1.000\n 88.000\n79.626\n 0.708\n112.500\n&lt;0.001\n0.561\n0.553\n\n\nPresentation_Rate\n5953.815\n9445.486\n1.950\n171.619\n55.037\n 0.992\n 55.469\n&lt;0.001\n0.387\n0.260\n\n\nMemory_Strategy:Presentation_Rate\n 272.792\n9445.486\n1.950\n171.619\n55.037\n21.655\n  2.541\n    0.083\n0.028\n0.010\n\n\n\n\nModel: aov_car(Recall_Performance ~ (Presentation_Rate) * (Memory_Strategy) + Error(Subject/(Presentation_Rate)))\n\n\ndf correction: Greenhouse-Geisser\n\n\nN = 90\n\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Presentation_Rate\n\n\nLevel 1\nLevel 2\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\n\n1\n2\n −6.093\n −8.091 — −4.095\n1.005\n88 \n −6.061\n&lt;0.001\n−0.562\n\n\n1\n4\n−11.496\n−13.703 — −9.288\n1.111\n88 \n−10.348\n&lt;0.001\n−1.060\n\n\n2\n4\n −5.402\n −7.697 — −3.108\n1.155\n88 \n −4.679\n&lt;0.001\n−0.498\n\n\n\np-values are uncorrected.\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Memory_Strategy\n\n\nLevel 1\nLevel 2\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\nRote Repetition\nVisual Imagery\n−11.520\n−13.678 — −9.362\n1.086\n88 \n−10.607\n&lt;0.001\n−1.062\n\n\np-values are uncorrected.\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Presentation_Rate x Memory_Strategy\n\n\nLevel 1\nLevel 2\nMemory_Strategy\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\n\n1\n2\nRote Repetition\n −8.500\n−11.326 — −5.674 \n1.422\n88 \n−5.978\n&lt;0.001\n−0.784\n\n\n1\n2\nVisual Imagery\n −3.687\n −6.512 — −0.861 \n1.422\n88 \n−2.593\n    0.011\n−0.340\n\n\n1\n4\nRote Repetition\n−13.149\n−16.271 — −10.027\n1.571\n88 \n−8.369\n&lt;0.001\n−1.212\n\n\n1\n4\nVisual Imagery\n −9.842\n−12.964 — −6.720 \n1.571\n88 \n−6.265\n&lt;0.001\n−0.908\n\n\n2\n4\nRote Repetition\n −4.649\n −7.894 — −1.404 \n1.633\n88 \n−2.847\n    0.005\n−0.429\n\n\n2\n4\nVisual Imagery\n −6.156\n −9.400 — −2.911 \n1.633\n88 \n−3.770\n&lt;0.001\n−0.568\n\n\n\np-values are uncorrected.\n\n\n\n\n\n\n\n\nPost-hoc Comparisons: Memory_Strategy x Presentation_Rate\n\n\nLevel 1\nLevel 2\nPresentation_Rate\nDifference\nCI 95%\nSE\ndf\nt\np\nCohen's D\n\n\n\n\nRote Repetition\nVisual Imagery\n1\n−14.227\n−17.464 — −10.989\n1.629\n88 \n−8.732\n&lt;0.001\n−1.312\n\n\nRote Repetition\nVisual Imagery\n2\n −9.413\n−12.687 — −6.139 \n1.647\n88 \n−5.714\n&lt;0.001\n−0.868\n\n\nRote Repetition\nVisual Imagery\n4\n−10.920\n−14.328 — −7.512 \n1.715\n88 \n−6.368\n&lt;0.001\n−1.007\n\n\n\np-values are uncorrected.\n\n\n\n\nFigures - ggplot2\n\nThe most customizable way to plot model results is using the ggplot2 package.\n\nggplot(recall_data, aes(x = Presentation_Rate, y = Recall_Performance,\n                        color = Memory_Strategy, \n                        group = Memory_Strategy)) +\n  geom_point(position = position_jitter(width = .1), alpha = .2) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Presentation Rate\", y = \"Recall Performance\") +\n  scale_color_brewer(palette = \"Set1\", name = \"Memory Strategy\")\n\n\n\n\n\n\n\nFigures - raincloud plot\nMy modeloutput function has a geom_flat_violin() function to create the cloud part of the raincloud plot. There are some other modifications that have to be made to other elements of the ggplot as well.\n\nggplot(recall_data, aes(x = Presentation_Rate, y = Recall_Performance,\n                 color = Memory_Strategy, fill = Memory_Strategy)) +\n  geom_flat_violin(aes(fill = Memory_Strategy),\n                   position = position_nudge(x = .1, y = 0),\n                   adjust = 1.5, trim = FALSE, \n                   alpha = .5, colour = NA) +\n  geom_point(aes(as.numeric(Presentation_Rate) - .15), \n             position = position_jitter(width = .05), alpha = .2) +\n  stat_summary(aes(group = Memory_Strategy),\n               fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", \n               width = .1) +\n  labs(x = \"Presentation_Rate\", y = \"Recall Performance\") +\n  scale_color_brewer(palette = \"Set1\", name = \"Memory Strategy\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  guides(fill = \"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigures - sjPlot\n\nThe main package in R to create and customize plots is ggplot2. However, there is definitely a bit of a learning curve to ggplot2. Instead, the sjPlot package offers convenient ways to plot the results of statistical analyses using plot_model().\n\nplot_model(anova_2way, type = \"int\", show.data = TRUE, jitter = TRUE)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html",
    "href": "data-analysis-regression.html",
    "title": "8  Regression",
    "section": "",
    "text": "ggplot2 Theme\nIt can be nice to set a global ggplot2 theme that is applied to all ggplots. Here is some code on how to 1) create a custom theme e.g., theme_spacious and 2) how to set the global theme based on some combination of custom and template themes.\ntheme_spacious &lt;- function(font.size = 14, bold = TRUE){\n  key.size &lt;- trunc(font.size * .8)\n  if (bold == TRUE) {\n    face.type &lt;- \"bold\"\n  } else {\n    face.type &lt;- \"plain\"\n  }\n  \n  theme(text = element_text(size = font.size),\n        axis.title.x = element_text(margin = margin(t = 15, r = 0,\n                                                    b = 0, l = 0),\n                                    face = face.type),\n        axis.title.y = element_text(margin = margin(t = 0, r = 15,\n                                                    b = 0, l = 0),\n                                    face = face.type),\n        legend.title = element_text(face = face.type),\n        legend.spacing = unit(20, \"pt\"),\n        legend.text = element_text(size = key.size),\n        plot.title = element_text(face = face.type, hjust = .5,\n                                  margin = margin(b = 10)),\n        plot.caption = element_text(hjust = 0, size = key.size,\n                                    margin = margin(t = 20)),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\",\n                                  face = face.type))\n}\n\noutput_theme &lt;- theme_linedraw() + \n  theme_spacious(font.size = 12) + \n  theme(panel.border = element_rect(color = \"gray\"),\n        axis.line.x = element_line(color = \"gray\"),\n        axis.line.y = element_line(color = \"gray\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\ntheme_set(output_theme)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#table-theme",
    "href": "data-analysis-regression.html#table-theme",
    "title": "8  Regression",
    "section": "Table Theme",
    "text": "Table Theme\nIt can also be nice to customize how tables from statistical analyses are displayed. Here is some code on how to define a table theme using the knitr and kableExtra packages. You will see that we will pass tables to table_theme() with some customization for the number of digits to round to, adding a table title, and footnotes.\n\ntable_theme &lt;- function(x, digits = 3, title = NULL, note = NULL) {\n  kable(x, digits = digits, caption = title) |&gt;\n    kable_classic(position = \"left\") |&gt;\n    kable_styling(full_width = FALSE, position = \"left\") |&gt;\n    footnote(general = note)\n}",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#specify-factor-levels",
    "href": "data-analysis-regression.html#specify-factor-levels",
    "title": "8  Regression",
    "section": "Specify factor levels",
    "text": "Specify factor levels\nNotice that the Type of Chocolate variable is categorical. When dealing with categorical variables for statistical analyses in R, it is usually a good idea to define the order of the categories as this will by default determine which category is treated as the reference (comparison group).\n\nhappiness_data &lt;- happiness_data |&gt;\n  mutate(Type_of_Chocolate = factor(Type_of_Chocolate,\n                                    levels = c(\"White\", \"Milk\",\n                                               \"Dark\", \"Alcohol\")))",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#descriptives",
    "href": "data-analysis-regression.html#descriptives",
    "title": "8  Regression",
    "section": "Descriptives",
    "text": "Descriptives\nIt is a good idea to get a look at the data before running your analysis. One way of doing so is to calculate summary statistics (descriptives) and print it out in a table. There are some column we can’t or don’t want descriptives for. For instance there is usually a subject id column in data files or some variables might be categorical rather than continuous variables. We can remove those column using select() from the dplyr package.\n\nlibrary(e1071)\n\nhappiness_data |&gt;\n  select(-Subject, -Type_of_Chocolate) |&gt;\n  pivot_longer(everything(),\n               names_to = \"Variable\", \n               values_to = \"value\") |&gt;\n  group_by(Variable) |&gt;\n  summarise(n = length(which(!is.na(value))),\n            Mean = mean(value, na.rm = TRUE),\n            SD = sd(value, na.rm = TRUE),\n            min = min(value, na.rm = TRUE),\n            max = max(value, na.rm = TRUE),\n            Skewness =\n              skewness(value, na.rm = TRUE, type = 2),\n            Kurtosis =\n              kurtosis(value, na.rm = TRUE, type = 2),\n            '% Missing' =\n              100 * (length(which(is.na(value))) / n())) |&gt;\n  table_theme(title = \"Descriptive Statistics\")\n\n\n\nDescriptive Statistics\n\nVariable\nn\nMean\nSD\nmin\nmax\nSkewness\nKurtosis\n% Missing\n\n\n\nChocolate_Consumption\n152\n10.497\n3.960\n0\n20.545\n-0.080\n-0.125\n0\n\n\nEmotion_Regulation\n152\n57.395\n19.691\n5\n120.213\n-0.018\n0.276\n0\n\n\nFinancial_Wealth\n152\n422.042\n142.156\n10\n760.284\n-0.095\n-0.169\n0\n\n\nHappiness\n152\n86.468\n32.709\n10\n173.036\n0.075\n-0.222\n0\n\n\nSocial_Support\n152\n44.646\n16.315\n7\n84.968\n0.129\n-0.161\n0\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nskewness() and kurtosis() are from the e1071 package\n\n\nUsing modeloutput\n\nI have an R package, modeloutput , for easily creating nice looking tables for statistical analyses. For a descriptives table we can use descriptives_table().\n\nlibrary(modeloutput)\n\nhappiness_data |&gt;\n  select(-Subject, -Type_of_Chocolate) |&gt;\n  descriptives_table()\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nn\nMean\nSD\nmin\nmax\nSkewness\nKurtosis\n% Missing\n\n\n\n\nHappiness\n152.00\n 86.47\n 32.71\n10.00\n173.04\n 0.08\n−0.22\n0.00\n\n\nFinancial_Wealth\n152.00\n422.04\n142.16\n10.00\n760.28\n−0.10\n−0.17\n0.00\n\n\nEmotion_Regulation\n152.00\n 57.40\n 19.69\n 5.00\n120.21\n−0.02\n 0.28\n0.00\n\n\nSocial_Support\n152.00\n 44.65\n 16.32\n 7.00\n 84.97\n 0.13\n−0.16\n0.00\n\n\nChocolate_Consumption\n152.00\n 10.50\n  3.96\n 0.00\n 20.54\n−0.08\n−0.13\n0.00\n\n\n\nTotal N = 152",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#historgrams",
    "href": "data-analysis-regression.html#historgrams",
    "title": "8  Regression",
    "section": "Historgrams",
    "text": "Historgrams\nVisualizing the actual distribution of values on each variable is a good idea too. This is how outliers or other problematic values can be detected. The most simple way to do so is by plotting a histogram for each variable in the data file using hist().\n\nhist(happiness_data$Happiness)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#correlation-tables",
    "href": "data-analysis-regression.html#correlation-tables",
    "title": "8  Regression",
    "section": "Correlation Tables",
    "text": "Correlation Tables\nRegression models are based on the co-variation (or correlation) between variables. Therefore, we might want to first evaluate all of the pairwise correlations in the data.\nUsing correlation\n\nOne option is to use the correlation package to create scatterplots using a combination of plot() and cor_test().\n\nlibrary(correlation)\n\nplot(cor_test(happiness_data, \"Financial_Wealth\", \"Happiness\"))\n\n\n\n\n\n\n\nUsing ggplot2\n\nThe ggplot2 way, more code but better customization. The geom_smooth(method = \"lm\") will plot the regression line on the data, the line of best fit.\n\nlibrary(ggplot2)\n\nggplot(data = happiness_data, aes(x = Financial_Wealth, y = Happiness)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Financial Wealth (annual income)\") +\n  theme_linedraw()\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nUsing base R\n\nThe base R way, easy and simple with plot().\n\nplot(happiness_data$Financial_Wealth, happiness_data$Happiness)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#regression",
    "href": "data-analysis-regression.html#regression",
    "title": "8  Regression",
    "section": "Regression",
    "text": "Regression\nRegression models allow us to test more complicated relationships between variables. It is important to match your research question with the correct regression model. But also, thinking about the different types of regression models can help you clarify your research question. We will take a look at how to conduct the following regression models in R:\n\nSimple regression\nMultiple regression\nHierarchical regression\nMediation analysis\nModeration analysis\n\nTo conduct a regression model is fairly simple in R, all you need is to specify the regression formula in the lm() function.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#simple-regression",
    "href": "data-analysis-regression.html#simple-regression",
    "title": "8  Regression",
    "section": "Simple Regression",
    "text": "Simple Regression\nIn what is called simple regression, there is only one predictor variable. Simple regression is not really necessary because it is directly equivalent to a correlation. However, it is useful to demonstrate what happens once you add multiple predictors into a model.\nThe formula in the lm() function takes on the form of: dv ~ iv.\nModel\n\nreg_simple &lt;- lm(Happiness ~ Financial_Wealth, data = happiness_data)\n\nUsing broom and performance\n\nThe broom and performance packages provide an easy way to get model results into a table format that we can then pass on to table_theme()\n\nlibrary(broom)\n\nglance(reg_simple) |&gt;\n  table_theme(title = \"Model Summary\")\n\n\n\nModel Summary\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n0.047\n0.041\n32.038\n7.4\n0.007\n1\n-741.642\n1489.284\n1498.356\n153960.4\n150\n152\n\n\n\n\n\n\nlibrary(performance)\n\nmodel_parameters(reg_simple) |&gt;\n  table_theme(title = \"Unstandardized Parameters\")\n\n\n\nUnstandardized Parameters\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n(Intercept)\n65.412\n8.165\n0.95\n49.279\n81.545\n8.011\n150\n0.000\n\n\nFinancial_Wealth\n0.050\n0.018\n0.95\n0.014\n0.086\n2.720\n150\n0.007\n\n\n\n\n\n\n\nmodel_parameters(reg_simple, standardize = \"refit\") |&gt;\n  table_theme(title = \"Standardized Parameters\")\n\n\n\nStandardized Parameters\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n(Intercept)\n0.000\n0.079\n0.95\n-0.157\n0.157\n0.00\n150\n1.000\n\n\nFinancial_Wealth\n0.217\n0.080\n0.95\n0.059\n0.374\n2.72\n150\n0.007\n\n\n\n\n\n\nUsing modeloutput\n\nMy modeloutput package provides a way to display regression tables in output format similar to other statistical software packages like JASP or SPSS.\n\nlibrary(modeloutput)\n\nregression_rsquared(reg_simple)\n\n\n\n\n\n\n\nModel Summary: Happiness\n\n\nModel\nR2\n\nR2 adj.\nBIC\n\n\n\nH1\n0.047\n0.041\n1498.356\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\n\n\n\n\n\nregression_modelsig(reg_simple)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n 73472.006\n  2 \n36736.003\n7.400\n0.007\n\n\nH1\nResidual\n153960.370\n150 \n 1026.402\n\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\n\n\n\n\n\nregression_coeff(reg_simple)\n\n\n\n\n\n\n\nRegression Coefficients: Happiness\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n65.412\n49.279 — 81.545\n0.000\n−0.157 — 0.157\n0.079\n8.011\n150 \n&lt;0.001\n\n\nH1\nFinancial_Wealth\n 0.050\n 0.014 — 0.086 \n0.217\n 0.059 — 0.374\n0.080\n2.720\n150 \n    0.007\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe formula on the coefficients table should display horizontally across the bottom in your reports, but in this book format it is not working.\n\n\nYou can also use a single function to display all three tables\n\nregression_tables(reg_simple)\n\n\n\n\n\n\n\nModel Summary: Happiness\n\n\nModel\nR2\n\nR2 adj.\nBIC\n\n\n\nH1\n0.047\n0.041\n1498.356\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n 73472.006\n  2 \n36736.003\n7.400\n0.007\n\n\nH1\nResidual\n153960.370\n150 \n 1026.402\n\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\n\n\n\n\n\n\nRegression Coefficients: Happiness\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n65.412\n49.279 — 81.545\n0.000\n−0.157 — 0.157\n0.079\n8.011\n150 \n&lt;0.001\n\n\nH1\nFinancial_Wealth\n 0.050\n 0.014 — 0.086 \n0.217\n 0.059 — 0.374\n0.080\n2.720\n150 \n    0.007\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#multiple-regression",
    "href": "data-analysis-regression.html#multiple-regression",
    "title": "8  Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nYou can and will most likely want to include multiple predictor variables in a regression model. There will be a different interpretation of the beta coefficients compared to the simple regression model. Now, the beta coefficients will represent a unique contribution (controlling for all the other predictor variables) of that variable to the prediction of the dependent variable. We can compare the relative strengths of the beta coefficients to decide which variable(s) are the strongest predictors of the dependent variable.\nIn this case, we can go ahead and include all the continuous predictor variables in a model predicting Happiness and compare it with the simple regression when we only had Financial Wealth in the model.\nModel\n\nreg_multiple &lt;- lm(Happiness ~ Financial_Wealth + Emotion_Regulation + \n                     Social_Support + Chocolate_Consumption,\n                   data = happiness_data)\n\nUsing broom and performance\n\nThe broom and performance packages provide an easy way to get model results into a table format that we can then pass on to table_theme()\n\nlibrary(broom)\n\nglance(reg_multiple) |&gt;\n  table_theme(title = \"Model Summary\")\n\n\n\nModel Summary\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n0.306\n0.287\n27.616\n16.21\n0\n4\n-717.532\n1447.063\n1465.207\n112107\n147\n152\n\n\n\n\n\n\nlibrary(performance)\n\nmodel_parameters(reg_multiple) |&gt;\n  table_theme(title = \"Unstandardized Parameters\")\n\n\n\nUnstandardized Parameters\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n(Intercept)\n16.976\n9.737\n0.95\n-2.267\n36.220\n1.743\n147\n0.083\n\n\nFinancial_Wealth\n0.018\n0.016\n0.95\n-0.015\n0.050\n1.081\n147\n0.282\n\n\nEmotion_Regulation\n0.461\n0.152\n0.95\n0.160\n0.761\n3.030\n147\n0.003\n\n\nSocial_Support\n0.395\n0.179\n0.95\n0.042\n0.747\n2.211\n147\n0.029\n\n\nChocolate_Consumption\n1.707\n0.596\n0.95\n0.529\n2.885\n2.864\n147\n0.005\n\n\n\n\n\n\n\nmodel_parameters(reg_multiple, standardize = \"refit\") |&gt;\n  table_theme(title = \"Standardized Parameters\")\n\n\n\nStandardized Parameters\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n(Intercept)\n0.000\n0.068\n0.95\n-0.135\n0.135\n0.000\n147\n1.000\n\n\nFinancial_Wealth\n0.077\n0.071\n0.95\n-0.064\n0.219\n1.081\n147\n0.282\n\n\nEmotion_Regulation\n0.277\n0.092\n0.95\n0.096\n0.458\n3.030\n147\n0.003\n\n\nSocial_Support\n0.197\n0.089\n0.95\n0.021\n0.373\n2.211\n147\n0.029\n\n\nChocolate_Consumption\n0.207\n0.072\n0.95\n0.064\n0.349\n2.864\n147\n0.005\n\n\n\n\n\n\nUsing modeloutput\n\nMy modeloutput package provides a way to display regression tables in output format similar to other statistical software packages like JASP or SPSS.\nYou can also add up to three regression models to easily compare in the output\n\nlibrary(modeloutput)\n\nregression_rsquared(reg_simple, reg_multiple)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Summary: Happiness\n\n\nModel\nR2\n\nR2 adj.\nΔR2\n\nΔF\ndf1\ndf2\np\nBIC\nBF\nP(Model|Data)\n\n\n\n\nH1\n0.047\n0.041\n\n\n\n\n\n1498.356\n\n\n\n\nH2\n0.306\n0.287\n0.259\n18.293\n3 \n147 \n&lt;0.001\n1465.207\n1.58 × 107\n\n1.000\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152\n\n\n\n\n\n\n\n\nregression_modelsig(reg_simple, reg_multiple)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n 73472.006\n  2 \n36736.003\n 7.400\n    0.007\n\n\nH1\nResidual\n153960.370\n150 \n 1026.402\n\n\n\n\nH2\nRegression\n 20192.945\n  5 \n 4038.589\n16.210\n&lt;0.001\n\n\nH2\nResidual\n112107.036\n147 \n  762.633\n\n\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152\n\n\n\n\n\n\n\n\nregression_coeff(reg_simple, reg_multiple)\n\n\n\n\n\n\n\nRegression Coefficients: Happiness\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n65.412\n49.279 — 81.545\n0.000\n−0.157 — 0.157\n0.079\n8.011\n150 \n&lt;0.001\n\n\nH1\nFinancial_Wealth\n 0.050\n 0.014 — 0.086 \n0.217\n 0.059 — 0.374\n0.080\n2.720\n150 \n    0.007\n\n\nH2\n(Intercept)\n16.976\n−2.267 — 36.220\n0.000\n−0.135 — 0.135\n0.068\n1.743\n147 \n    0.083\n\n\nH2\nFinancial_Wealth\n 0.018\n−0.015 — 0.050 \n0.077\n−0.064 — 0.219\n0.071\n1.081\n147 \n    0.282\n\n\nH2\nEmotion_Regulation\n 0.461\n 0.160 — 0.761 \n0.277\n 0.096 — 0.458\n0.092\n3.030\n147 \n    0.003\n\n\nH2\nSocial_Support\n 0.395\n 0.042 — 0.747 \n0.197\n 0.021 — 0.373\n0.089\n2.211\n147 \n    0.029\n\n\nH2\nChocolate_Consumption\n 1.707\n 0.529 — 2.885 \n0.207\n 0.064 — 0.349\n0.072\n2.864\n147 \n    0.005\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152\n\n\n\n\n\n\n\nYou can also use a single function to display all three tables\n\nregression_tables(reg_simple, reg_multiple)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Summary: Happiness\n\n\nModel\nR2\n\nR2 adj.\nΔR2\n\nΔF\ndf1\ndf2\np\nBIC\nBF\nP(Model|Data)\n\n\n\n\nH1\n0.047\n0.041\n\n\n\n\n\n1498.356\n\n\n\n\nH2\n0.306\n0.287\n0.259\n18.293\n3 \n147 \n&lt;0.001\n1465.207\n1.58 × 107\n\n1.000\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n 73472.006\n  2 \n36736.003\n 7.400\n    0.007\n\n\nH1\nResidual\n153960.370\n150 \n 1026.402\n\n\n\n\nH2\nRegression\n 20192.945\n  5 \n 4038.589\n16.210\n&lt;0.001\n\n\nH2\nResidual\n112107.036\n147 \n  762.633\n\n\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152\n\n\n\n\n\n\n\n\n\nRegression Coefficients: Happiness\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n65.412\n49.279 — 81.545\n0.000\n−0.157 — 0.157\n0.079\n8.011\n150 \n&lt;0.001\n\n\nH1\nFinancial_Wealth\n 0.050\n 0.014 — 0.086 \n0.217\n 0.059 — 0.374\n0.080\n2.720\n150 \n    0.007\n\n\nH2\n(Intercept)\n16.976\n−2.267 — 36.220\n0.000\n−0.135 — 0.135\n0.068\n1.743\n147 \n    0.083\n\n\nH2\nFinancial_Wealth\n 0.018\n−0.015 — 0.050 \n0.077\n−0.064 — 0.219\n0.071\n1.081\n147 \n    0.282\n\n\nH2\nEmotion_Regulation\n 0.461\n 0.160 — 0.761 \n0.277\n 0.096 — 0.458\n0.092\n3.030\n147 \n    0.003\n\n\nH2\nSocial_Support\n 0.395\n 0.042 — 0.747 \n0.197\n 0.021 — 0.373\n0.089\n2.211\n147 \n    0.029\n\n\nH2\nChocolate_Consumption\n 1.707\n 0.529 — 2.885 \n0.207\n 0.064 — 0.349\n0.072\n2.864\n147 \n    0.005\n\n\n\n\nH1: Happiness ~ Financial_Wealth; N = 152\n\n\nH2: Happiness ~ Financial_Wealth + Emotion_Regulation + Social_Support + Chocolate_Consumption; N = 152",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#mediation",
    "href": "data-analysis-regression.html#mediation",
    "title": "8  Regression",
    "section": "Mediation",
    "text": "Mediation\nMediation analysis is similar to hierarchical regression, however it allows us to test whether the effect of a predictor variable, on the dependent variable, “goes through” a mediator variable. This effect that “goes through” the mediator variable is called the indirect effect; whereas any effect of the predictor variable that does not “go through” the mediator is called the direct effect where:\nTotal Effect = Direct Effect + Indirect Effect\nWe are parsing the variance explained by the predictor variable into direct and indirect effects. Here are some possibilities for the outcome:\n\nFull Mediation: The total effect of the predictor variable can be fully explained by the indirect effect. The mediator fully explains the effect of the predictor variable on the dependent variable. Direct effect = 0, indirect effect != 0.\nPartial Mediation: The total effect of the predictor variable can be partially explained by the indirect effect. The mediator only partially explains the effect of the predictor variable on the dependent variable. Direct effect != 0, indirect effect != 0.\nNo Mediation: The total effect of the predictor variable can not be explained by the indirect effect; there is no indirect effect at all. Direct effect != 0, indirect effect = 0.\n\nThere are two general ways to perform a mediation analysis in R.\n\nThe standard regression method\nPath analysis\n\nRegression\nThe regression method requires multiple regression models because we have two dependent variables in the mode - the mediation and the outcome measure. The mediator acts as both an independent variable and dependent variable in the model. Because regression requires only one dependent variable, we cannot test the full model in one regression model. Instead, we have to test the different pieces of the model one at a time. We also need to perform a test on the indirect effect itself, which will require a separate model.\nIn order to evaluate the standardize coefficients (which is often times most useful), we need to first convert the the variables into z-scores. This is fairly straightforward but does require some extra code. A z-score is standardized score on the scale of standard deviation units and is calculated as:\nz-score = (raw score - mean) / sd\nWe can create new columns representing the z-scores using mutate() from the dplyr package.\n\ndata_mediation &lt;- happiness_data %&gt;%\n  mutate(Happiness_z = \n           (Happiness - mean(Happiness)) / sd(Happiness),\n         Social_Support_z = \n           (Social_Support - mean(Social_Support)) / sd(Social_Support),\n         Emotion_Regulation_z = \n           (Emotion_Regulation - mean(Emotion_Regulation)) / sd(Emotion_Regulation))\n\nThe standard regression model typically involves three regression models and a model testing the indirect effect.\n\nA model with the total effect: dv ~ iv\n\n\n\nreg_total &lt;- lm(Happiness_z ~ Social_Support_z, \n                data = data_mediation)\n\n\nA model with the effect of the predictor on the mediator: m ~ iv\n\n\n\nreg_med &lt;- lm(Emotion_Regulation_z ~ Social_Support_z, \n              data = data_mediation)\n\n\nA model with the direct effect and the effect of the mediator: dv ~ iv + m\n\n\n\nreg_direct &lt;- lm(Happiness_z ~ Emotion_Regulation_z + Social_Support_z, \n                 data = data_mediation)\n\n\nA statistical test of the indirect effect: mediation::mediate()\n\n\n\nlibrary(mediation)\n\nmodel_mediation &lt;- mediate(reg_med, reg_direct,\n                           treat = \"Social_Support_z\",\n                           mediator = \"Emotion_Regulation_z\",\n                           boot = TRUE, sims = 1000, \n                           boot.ci.type = \"bca\")\n## Running nonparametric bootstrap\n\nFor simplicity, we will use the modeloutput package to display the regression output\n\nregression_tables(reg_total, reg_direct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Summary: Happiness_z\n\n\nModel\nR2\n\nR2 adj.\nΔR2\n\nΔF\ndf1\ndf2\np\nBIC\nBF\nP(Model|Data)\n\n\n\n\nH1\n0.187\n0.181\n\n\n\n\n\n413.995\n\n\n\n\nH2\n0.261\n0.251\n0.074\n14.944\n1 \n149 \n&lt;0.001\n404.491\n1.16 × 102\n\n0.991\n\n\n\n\nH1: Happiness_z ~ Social_Support_z; N = 152\n\n\nH2: Happiness_z ~ Emotion_Regulation_z + Social_Support_z; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness_z\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n 28.207\n  2 \n14.103\n34.456\n&lt;0.001\n\n\nH1\nResidual\n122.793\n150 \n 0.819\n\n\n\n\nH2\nRegression\n 15.276\n  3 \n 5.092\n26.302\n&lt;0.001\n\n\nH2\nResidual\n111.600\n149 \n 0.749\n\n\n\n\n\n\nH1: Happiness_z ~ Social_Support_z; N = 152\n\n\nH2: Happiness_z ~ Emotion_Regulation_z + Social_Support_z; N = 152\n\n\n\n\n\n\n\n\n\nRegression Coefficients: Happiness_z\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n0.000\n−0.145 — 0.145\n0.000\n−0.145 — 0.145\n0.073\n0.000\n150 \n    1.000\n\n\nH1\nSocial_Support_z\n0.432\n 0.287 — 0.578\n0.432\n 0.287 — 0.578\n0.074\n5.870\n150 \n&lt;0.001\n\n\nH2\n(Intercept)\n0.000\n−0.139 — 0.139\n0.000\n−0.139 — 0.139\n0.070\n0.000\n149 \n    1.000\n\n\nH2\nEmotion_Regulation_z\n0.350\n 0.171 — 0.530\n0.350\n 0.171 — 0.530\n0.091\n3.866\n149 \n&lt;0.001\n\n\nH2\nSocial_Support_z\n0.212\n 0.033 — 0.391\n0.212\n 0.033 — 0.391\n0.091\n2.335\n149 \n    0.021\n\n\n\n\nH1: Happiness_z ~ Social_Support_z; N = 152\n\n\nH2: Happiness_z ~ Emotion_Regulation_z + Social_Support_z; N = 152\n\n\n\n\n\n\n\n\nregression_tables(reg_med)\n\n\n\n\n\n\n\nModel Summary: Emotion_Regulation_z\n\n\nModel\nR2\n\nR2 adj.\nBIC\n\n\n\nH1\n0.396\n0.392\n368.722\n\n\nH1: Emotion_Regulation_z ~ Social_Support_z; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Emotion_Regulation_z\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n59.837\n  2 \n29.918\n98.456\n&lt;0.001\n\n\nH1\nResidual\n91.163\n150 \n 0.608\n\n\n\n\n\nH1: Emotion_Regulation_z ~ Social_Support_z; N = 152\n\n\n\n\n\n\n\n\nRegression Coefficients: Emotion_Regulation_z\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n0.000\n−0.125 — 0.125\n0.000\n−0.125 — 0.125\n0.063\n0.000\n150 \n    1.000\n\n\nH1\nSocial_Support_z\n0.630\n 0.504 — 0.755\n0.630\n 0.504 — 0.755\n0.063\n9.922\n150 \n&lt;0.001\n\n\n\nH1: Emotion_Regulation_z ~ Social_Support_z; N = 152\n\n\n\n\n\n\nThen we can display the result of the indirect mediation analysis\n\nmodel_parameters(model_mediation) |&gt;\n  table_theme()\n\n\n\n\nParameter\nEstimate\nCI\nCI_low\nCI_high\np\n\n\n\nACME\n0.221\n0.95\n0.110\n0.351\n0.000\n\n\nADE\n0.212\n0.95\n0.053\n0.360\n0.014\n\n\nTotal Effect\n0.432\n0.95\n0.317\n0.557\n0.000\n\n\nProp. Mediated\n0.510\n0.95\n0.259\n0.855\n0.000\n\n\n\n\n\n\nPath Analysis\nThere is a statistical modelling approach called path analysis that does allow for multiple dependent variables in a single model. Here is how to specify a path analysis model using the lavaan package.\nSee the lavaan website for how to conduct mediation in lavaan\nModel\n\nmodel &lt;- \"\n# a path\nEmotion_Regulation ~ a*Social_Support\n\n# b path\nHappiness ~ b*Emotion_Regulation\n\n# c prime path \nHappiness ~ c*Social_Support\n\n# indirect and total effects\nindirect := a*b\ntotal := c + a*b\n\"\n\nfit &lt;- sem(model, data = happiness_data)\n\nI also have a package, semoutput for displaying results from lavaan models:\nlibrary(semoutput)\n\nsem_tables(fit)\n## Warning in max(nchar(x_lhs)): no non-missing arguments to max; returning\n## -Inf\n## Warning in max(nchar(x_rhs)): no non-missing arguments to max; returning\n## -Inf\n## Warning in max(nchar(x_lhs)): no non-missing arguments to max; returning\n## -Inf\n## Warning in max(nchar(x_rhs)): no non-missing arguments to max; returning\n## -Inf\n\n\n\n\n\nModel Significance\n    \n\nN\n      χ2\n\n      df\n      p\n    \n\n\n152 \n0.000\n0 \n\n\n\n\n\n\n\n\n\nModel Fit\n    \n\nCFI\n      RMSEA\n      90% CI\n      TLI\n      SRMR\n      AIC\n      BIC\n    \n\n\n1.000\n0.000\n0.000 — 0.000\n1.000\n0.000\n2714.271\n2729.391\n\n\n\n\n\n\n\n\nRegression Paths\n    \n\nPredictor\n      DV\n      \n        Standardized\n      \n    \n\nβ\n      95% CI\n      sig\n      SE\n      z\n      p\n    \n\n\n\nSocial_Support\nEmotion_Regulation\n0.629\n0.544 — 0.715\n***\n0.044\n14.356\n    0.000\n\n\nEmotion_Regulation\nHappiness\n0.350\n0.181 — 0.520\n***\n0.086\n 4.057\n&lt;0.001\n\n\nSocial_Support\nHappiness\n0.212\n0.039 — 0.384\n*\n0.088\n 2.402\n    0.016\n\n\na*b\nindirect\n0.221\n0.109 — 0.333\n***\n0.057\n 3.859\n&lt;0.001\n\n\nc+a*b\ntotal\n0.432\n0.309 — 0.555\n***\n0.063\n 6.882\n&lt;0.001\n\n\n\n * p &lt; .05; ** p &lt; .01; *** p &lt; .001\n    \n\n\nBut we need to calculate bootstrapped and bias-corrected confidence intervals around the indirect effect:\n\nmonteCarloCI(fit, nRep = 1000, standardized = TRUE) |&gt;\n  table_theme()\n\n\n\n\n\nest.std\nci.lower\nci.upper\n\n\n\nindirect\n0.221\n0.106\n0.335\n\n\ntotal\n0.432\n0.307\n0.556",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data-analysis-regression.html#moderation",
    "href": "data-analysis-regression.html#moderation",
    "title": "8  Regression",
    "section": "Moderation",
    "text": "Moderation\nAnother type of multiple regression model is the moderation analysis. The concept of moderation in regression is analogous to the concept of an interaction in ANOVA. The idea here is that the relationship (correlation or slope) between two variables changes as a function of another variable, called the moderator. As the values on the moderator variable increases, the correlation or slope might increase or decrease. The moderator can be either continuous or categorical.\nIn our happiness data, we can test whether the relationship between chocolate consumption and happiness is moderated by the type of chocolate consumed. In this case, type of chocolate is a categorical variable: White, milk, dark, or alcohol.\nIt is highly advisable to first “center” the predictor and moderator variables. Centering refers to centering the scores around the mean (making the mean = 0). This can simply be done by subtracting out the mean from each score. We can do this with mutate() from the dplyr package.\nSince type of chocolate is categorical we do not need to center it.\n\ndata_mod &lt;- happiness_data %&gt;%\n  mutate(Chocolate_Consumed_c = \n           Chocolate_Consumption - mean(Chocolate_Consumption))\n\nModel\n\nmodel_null &lt;- lm(Happiness ~ Chocolate_Consumed_c + Type_of_Chocolate, \n                 data = data_mod)\n\nmodel_moderation &lt;- lm(Happiness ~ Chocolate_Consumed_c * Type_of_Chocolate, \n                       data = data_mod)\n\n\nregression_tables(model_null, model_moderation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Summary: Happiness\n\n\nModel\nR2\n\nR2 adj.\nΔR2\n\nΔF\ndf1\ndf2\np\nBIC\nBF\nP(Model|Data)\n\n\n\n\nH1\n0.174\n0.151\n\n\n\n\n\n1491.733\n\n\n\n\nH2\n0.223\n0.186\n0.050\n3.061\n3 \n144 \n0.030\n1497.409\n5.85 × 10−2\n\n0.055\n\n\n\n\nH1: Happiness ~ Chocolate_Consumed_c + Type_of_Chocolate; N = 152\n\n\nH2: Happiness ~ Chocolate_Consumed_c * Type_of_Chocolate; N = 152\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table: Happiness\n\n\nModel\nTerm\nSum of Squares\ndf\nMean Square\nF\np\n\n\n\n\nH1\nRegression\n250273.269\n  5 \n50054.654\n7.729\n&lt;0.001\n\n\nH1\nResidual\n133482.237\n147 \n  908.042\n\n\n\n\nH2\nRegression\n239803.383\n  8 \n29975.423\n5.914\n&lt;0.001\n\n\nH2\nResidual\n125481.246\n144 \n  871.398\n\n\n\n\n\n\nH1: Happiness ~ Chocolate_Consumed_c + Type_of_Chocolate; N = 152\n\n\nH2: Happiness ~ Chocolate_Consumed_c * Type_of_Chocolate; N = 152\n\n\n\n\n\n\n\n\n\nRegression Coefficients: Happiness\n\n\nModel\nTerm\nUnstandardized\nStandardized\nt\ndf\np\n\n\nb\n95% CI\nβ\n95% CI\nSE\n\n\n\n\nH1\n(Intercept)\n76.468\n66.808 — 86.129\n−0.306\n−0.601 — −0.010\n0.149\n15.643\n147 \n&lt;0.001\n\n\nH1\nChocolate_Consumed_c\n 2.801\n 1.577 — 4.024 \n 0.339\n 0.191 — 0.487 \n0.075\n 4.523\n147 \n&lt;0.001\n\n\nH1\nType_of_ChocolateMilk\n 5.000\n−8.662 — 18.662\n 0.153\n−0.265 — 0.571 \n0.211\n 0.723\n147 \n    0.471\n\n\nH1\nType_of_ChocolateDark\n15.000\n 1.338 — 28.662\n 0.459\n 0.041 — 0.876 \n0.211\n 2.170\n147 \n    0.032\n\n\nH1\nType_of_ChocolateAlcohol\n20.000\n 6.338 — 33.662\n 0.611\n 0.194 — 1.029 \n0.211\n 2.893\n147 \n    0.004\n\n\nH2\n(Intercept)\n76.468\n67.003 — 85.933\n−0.306\n−0.595 — −0.016\n0.146\n15.968\n144 \n&lt;0.001\n\n\nH2\nChocolate_Consumed_c\n 0.417\n−1.981 — 2.815 \n 0.050\n−0.240 — 0.341 \n0.147\n 0.343\n144 \n    0.732\n\n\nH2\nType_of_ChocolateMilk\n 5.000\n−8.386 — 18.386\n 0.153\n−0.256 — 0.562 \n0.207\n 0.738\n144 \n    0.462\n\n\nH2\nType_of_ChocolateDark\n15.000\n 1.614 — 28.386\n 0.459\n 0.049 — 0.868 \n0.207\n 2.215\n144 \n    0.028\n\n\nH2\nType_of_ChocolateAlcohol\n20.000\n 6.614 — 33.386\n 0.611\n 0.202 — 1.021 \n0.207\n 2.953\n144 \n    0.004\n\n\nH2\nChocolate_Consumed_c:Type_of_ChocolateMilk\n 1.282\n−2.109 — 4.673 \n 0.155\n−0.255 — 0.566 \n0.208\n 0.747\n144 \n    0.456\n\n\nH2\nChocolate_Consumed_c:Type_of_ChocolateDark\n 3.606\n 0.215 — 6.997 \n 0.437\n 0.026 — 0.847 \n0.208\n 2.102\n144 \n    0.037\n\n\nH2\nChocolate_Consumed_c:Type_of_ChocolateAlcohol\n 4.648\n 1.256 — 8.039 \n 0.563\n 0.152 — 0.973 \n0.208\n 2.709\n144 \n    0.008\n\n\n\n\nH1: Happiness ~ Chocolate_Consumed_c + Type_of_Chocolate; N = 152\n\n\nH2: Happiness ~ Chocolate_Consumed_c * Type_of_Chocolate; N = 152\n\n\n\n\n\n\n\nTo better understand the nature of a moderation, it helps to plot the results.\nFigures - ggeffects and ggplot2\n\nThe most customizable way to plot model results is using the ggeffects and ggplot2 packages. We can use ggeffect() to get a data frame with estimated model values for specified terms in the model and the dependent variable.\nFor continuous variables, we can customize at what values we want to plot. For “all” possible values from the original data we can specify [all]. If your variable was standardized before entering it into the model you can use [2, 0, -2] to get values at +2 standard deviations above the mean, the mean, and -2 standard deviations below the mean.\n\nlibrary(ggeffects)\n\ndata_plot &lt;- ggeffect(model_moderation,\n                       terms = c(\"Chocolate_Consumed_c [all]\", \n                                 \"Type_of_Chocolate\")) |&gt;\n  rename(Chocolate_Consumed_c = x, Type_of_Chocolate = group,\n         Happiness = predicted)\n\nThis results in a data frame of estimated values from the model that we can now plot using ggplot2\n\n\nlibrary(ggplot2)\n\nggplot(data_plot, aes(Chocolate_Consumed_c, Happiness, \n                      color = Type_of_Chocolate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, \n                  fill = Type_of_Chocolate), \n              show.legend = FALSE, alpha = .1, color = NA) +\n  geom_smooth(method = \"lm\", alpha = 0) +\n  scale_color_brewer(palette = \"Set1\",\n                     guide_legend(title = \"Type of Chocolate\")) +\n  labs(x = \"Amount of Chocolate Consumed\")\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nFigures - sjPlot\n\nThe easiest way to plot the results of a moderation analysis is using the plot_model() function from the sjPlot package. Note that it is a bit more complicated to plot a moderation analysis when the moderator is a continuous variable instead of categorical but the sjPlot package actually makes it a bit easier.\n\nplot_model(model_moderation, typ = \"int\")",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "update-packages.html",
    "href": "update-packages.html",
    "title": "Update Packages",
    "section": "",
    "text": "I will sporadically update my GitHub R Packages. It is recommended to use the latest versions. Copy and paste this code into a script to update the packages on your computer.\n\nlibrary(devtools)\n\ninstall_github(\"EngleLab/englelab\", upgrade = FALSE)\ninstall_github(\"dr-JT/psyworkflow\", upgrade = FALSE)\ninstall_github(\"dr-JT/semoutput\", upgrade = FALSE)\ninstall_github(\"dr-JT/pupillometry\", upgrade = FALSE)\ninstall_github(\"dr-JT/modeloutput\", upgrade = FALSE)",
    "crumbs": [
      "Appendices",
      "Update Packages"
    ]
  }
]