[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engle Lab useRguide",
    "section": "",
    "text": "Welcome\nThis useRguide for the Attention & Working Memory Lab at Georgia Tech provides training in the basics of how to use R, the tidyverse, and tools for how we process and analyze data in our lab. The workflow and data processing steps presented in this guide are specific to the type of data we tend to work with in our lab. We primarily collect behavioral data on a large set of cognitive tasks to test individual differences in cognitive ability. However, you will find that most of the principles presented here apply to working with other types of data. This is especially true of the sections on data manipulation and data visualization."
  },
  {
    "objectID": "introduction.html#navigation",
    "href": "introduction.html#navigation",
    "title": "Introduction",
    "section": "Navigation",
    "text": "Navigation\nYou can navigate this useRguide using the left side bar. Select the section headers to see an introduction to each section.\nWithin each chapter there will be a table of contents on the right side bar that you can use to navigate.\nThe top menu navigation bar has convenient links to the lab website, the lab manual, and documentation on R packages that were developed in this lab."
  },
  {
    "objectID": "introduction.html#section-contents",
    "href": "introduction.html#section-contents",
    "title": "Introduction",
    "section": "Section Contents",
    "text": "Section Contents\nThe Getting Started section covers how to install and update software, introduce the basics of how R works, and more.\nThe Tidyverse section is a genearl introduction to working with data in R using the Tidyverse set of packages.\nThe Data Processing section will cover data processing steps in a way that are specific to our lab.\nThe Data Visualization section will introduce different ways of plotting data using the ggplot2 package.\nWe use R at every stage of processing data from tidying up messy raw data files to generating statistical and graphical reports.\n\n\n\n\nFigure 1: Data processing steps"
  },
  {
    "objectID": "installation.html#sec-install-r",
    "href": "installation.html#sec-install-r",
    "title": "1  Installs and Updates",
    "section": "Install R",
    "text": "Install R\nirst you need to download the latest version of R from their website https://www.r-project.org\n\nSelect CRAN on the left, just under Download\nSelect the first option under 0-Cloud\nSelect the download option depending on your computer\nSelect the base installation (for Windows) or the Latest Release (for Mac)\nOpen and Run the installation file"
  },
  {
    "objectID": "installation.html#sec-install-r-studio",
    "href": "installation.html#sec-install-r-studio",
    "title": "1  Installs and Updates",
    "section": "Install R Studio",
    "text": "Install R Studio\nThe easiest way to interact with R is through the R Studio environment. To do this you need to download R Studio\n\nSelect the Free version of R Studio Desktop\n\nSelect the download option depending on your computer"
  },
  {
    "objectID": "installation.html#sec-the-rstudio-environemnt",
    "href": "installation.html#sec-the-rstudio-environemnt",
    "title": "1  Installs and Updates",
    "section": "The RStudio Environemnt",
    "text": "The RStudio Environemnt\nGo ahead an open the RStudio application on your computer.\nWhen you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below\n\n\n\n\nFigure 1.1: RStudio window panes\n\n\n\nThere are 4 window panes and each one has it’s own set of tabs associated with it:\n\nThe Console window (the bottom left window pane) is where code is executed and output is displayed.\n\nThe Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window.\nThe Source window is also where you can view data frames you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one data frame called ‘data’.\n\nThe Environment window (top right window pane) is where you can see any data frames, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter.\n\n\nwelcome_message &lt;- \"hello\"\n\nYou should now see the object welcome_message in the Environment window pane\n\nThe Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation."
  },
  {
    "objectID": "installation.html#sec-rstudio-settings",
    "href": "installation.html#sec-rstudio-settings",
    "title": "1  Installs and Updates",
    "section": "RStudio Settings",
    "text": "RStudio Settings\nI highly suggest changing the default RStudio General settings by going to Tools -&gt; Global Options\n\n\n\n\nFigure 1.2: RStudio settings window\n\n\n\nYou can also change the theme, font type, and font size, if you navigate to the Appearance tab in Settings. You may also like to change the organization of the RStudio window panes in Pane Layout."
  },
  {
    "objectID": "installation.html#sec-update-r",
    "href": "installation.html#sec-update-r",
    "title": "1  Installs and Updates",
    "section": "Update R",
    "text": "Update R\nIf you already have R installed, but want to update it to the most current version follow these steps.\nWarning: When updating R (not RStudio), it may remove all packages you have installed\nFirst check what version of R you have installed.\n\nOpen RStudio\nIn the console window you will see the R version you are running (e.g., R version 4.1.0)\nIf you have an R version older than 4.0.0 than you should update R.\nRun the following lines of code in your console window. This is an easy way to re-install all your currently installed packages. This step will save a list of packages to re-install later.\n\n\n# Save current packages and their versions to object called ip\n\nip &lt;- installed.packages()\nip\n\n# Save the object as an .rds file\n\nsaveRDS(ip, \"CurrentPackages.rds\")\n\n\nExit out of all R or RStudio windows\nDownload and install the latest version of R (see the section on installing R above)\nOpen RStudio\nCheck if your previously installed packages are installed using the Packages tab in the bottom right window\nIf you need to re-install your previous packages, then run the following lines of code\n\n\n# After updating R, load the file and reinstall packages\n\nip &lt;- readRDS(\"CurrentPackages.rds\")\n\ninstall.packages(ip[,1])"
  },
  {
    "objectID": "installation.html#sec-update-r-studio",
    "href": "installation.html#sec-update-r-studio",
    "title": "1  Installs and Updates",
    "section": "Update R Studio",
    "text": "Update R Studio\nGo to Help -&gt; Check for Updates"
  },
  {
    "objectID": "pkgs-and-funcs.html#sec-using-functions",
    "href": "pkgs-and-funcs.html#sec-using-functions",
    "title": "2  Functions and Packages",
    "section": "Using Functions",
    "text": "Using Functions\nBasically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them.\nFunctions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separated by commas , . Some arguments are optional and some are required for the function to work.\nFor example, there is a function to create a sequence of numbers, seq().\n\nseq(1, 100, by = 10)\n\n [1]  1 11 21 31 41 51 61 71 81 91\n\n\nIn the seq() function above we specified three arguments, separated by commas. The first two arguments were set without specifying the argument name, however the third argument we used the argument name by to define seq(by = 10). If you don’t explicitly use the argument name it will implicitly assume an argument based on the order it is entered, depending on how the author created the function.\n\nThe Helper Function\nA handy tip is to frequently make use of the helper function, ?. Type ?seq into the R console. Helper documentation will be provided for that function and as you can see, the first argument defined is from and the second argument is to.\n\nOrder only matters if you do not specify argument names\nSpecifying the actual argument names, the above code is identical to the three following examples:\n\nseq(from = 1, to = 100, by = 10)\nseq(to = 100, by = 10, from = 1)\nseq(1, 100, 10)\n\nThere are also default values that arguments take, which means if you don’t define an argument it will take on the default value. The helper documentation shows that the from argument has a default of from = 1, therefore we could even leave out the from = argument because we were using the default value:\n\nseq(to = 100, by = 10)\n\nWhat this means is that it can be important to know what the default values are for functions you are using and you can figure that out with the helper function ?"
  },
  {
    "objectID": "pkgs-and-funcs.html#sec-r-packages",
    "href": "pkgs-and-funcs.html#sec-r-packages",
    "title": "2  Functions and Packages",
    "section": "R Packages",
    "text": "R Packages\nThe community of R users have developed a vast number of functions that expand on the base R functions. Many of the functions developed by R users allow you to do more complicated things with your data without having to be an advanced R programmer. And the great thing is that as more psychology researchers use R, the more functions there are specifically for psychological research.\nFunctions that R users develop are collected in what are called packages. Most R packages are hosted on The Comprehensive R Archive Network - CRAN. Some other packages, ones that are in more developmental stages and may not be as stable, are hosted on GitHub.\nInstall and Load Packages\nTo install packages from CRAN is easy. Simply type into the console window: install.packages(\"packagename\")\nFor example:\n\ninstall.packages(\"dplyr\")\n\nOnce you have a package installed, you can load that package into your current environment. Your R Environment consists of things such as objects (variables) you have created, data you have imported, and functions you have loaded. Your R Environment are like the tools and objects you have available to you and are working with.\nWhen you load a package you are bringing the functions from that package into your environment so you can start using them. To load a package is easy: library(package_name)\nFor example:\n\nlibrary(dplyr)"
  },
  {
    "objectID": "R-basics.html#sec-creating-r-objects",
    "href": "R-basics.html#sec-creating-r-objects",
    "title": "3  R Basics",
    "section": "Creating R objects",
    "text": "Creating R objects\nIn R, everything that exists is an object and everything you do to objects are functions. You can define an object using the assignment operator &lt;-.\nEverything on the left hand side of the &lt;- assignment operator is an object. Everything on the right hand side of &lt;- are functions or values. Go ahead and type the following two lines of code in your script\n\nstring &lt;- \"hello\"\nstring\n## [1] \"hello\"\n\nYou can execute/run a line of code by placing the cursor anywhere on the line and press Ctrl + Enter. Go ahead an run the two lines of code.\nIn this example, the first line creates a new object called string with a value of “hello”. The second line simply prints the output of string to the Console window. In the second line there is no assignment operator. When there is no &lt;- this means you are essentially just printing to the console. You can’t do anything with stuff that is just printed to the console, it is just for viewing purposes.\nFor instance, if I wanted to calculate 1 + 2 I could do this by printing it to the console\n\n1 + 2\n## [1] 3\n\nHowever, if I wanted to do something else with the result of that calculation then I would not be able to unless I assigned the result to an object using &lt;-\n\nresult &lt;- 1 + 2\nresult &lt;- result * 5\n\nresult\n## [1] 15\n\nThe point is, you are almost always going to assign the result of some function or value to an object. Printing to the console is not very useful. Almost every line of code, then, will have an object name on the left hand side of &lt;- and a function or value on the right hand side of &lt;-\nIn the first example above, notice how I included \" \" around hello. This tells R that hello is a string, not an object. If I were to not include \" \", then R would think I am calling an object. And since there is no object with the name hello it will print an error\n\nstring &lt;- hello\n## Error in eval(expr, envir, enclos): object 'hello' not found\n\nDo not use \" \" for Numerical values\n\na &lt;- \"5\" + \"1\"\n## Error in \"5\" + \"1\": non-numeric argument to binary operator\n\nYou can execute lines of code by:\n\nTyping them directly into the Console window\nTyping them into the Script window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time.\nClicking on Source at the top right of the Script window. This will run ALL the lines of code contained in the script file.\n\nIt is important to know that EVERYTHING in R is case sensitive.\n\na &lt;- 5\n\na + 5\n## [1] 10\nA + 5\n## Error in eval(expr, envir, enclos): object 'A' not found"
  },
  {
    "objectID": "R-basics.html#sec-classes",
    "href": "R-basics.html#sec-classes",
    "title": "3  R Basics",
    "section": "Classes",
    "text": "Classes\nClasses are types of values that exist in R:\n\ncharacter \"hello\", \"19\"\nnumeric (or double) 2, 32.55\ninteger 5, 99\nlogical TRUE, FALSE\n\nTo evaluate the class of an object you can use the typeof()\n\ntypeof(a)\n## [1] \"double\"\n\nTo change the class of values in an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions.\n\nas.integer(a)\n## [1] 5\n\n\nas.character(a)\n## [1] \"5\"\n\n\nas.numeric(\"hello\")\n## Warning: NAs introduced by coercion\n## [1] NA"
  },
  {
    "objectID": "R-basics.html#sec-vectors",
    "href": "R-basics.html#sec-vectors",
    "title": "3  R Basics",
    "section": "Vectors",
    "text": "Vectors\nOkay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures see this nice Introduction to R\nVectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element.\n\nb &lt;- c(1,3,5)\n\nc() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() function concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like?\n\nb\n## [1] 1 3 5\n\nYou can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance,\n\nb[2]\n## [1] 3\n\nThe value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two different ways.\n\nb[2:3]\n## [1] 3 5\nb[-1]\n## [1] 3 5\n\nNow, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements.\n\nb\n## [1] 1 3 5\n\nTo change vector b we need to define b as vector b with the first element removed\n\nb &lt;- b[-1]\nb\n## [1] 3 5\n\nVector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b.\n\nc(5,b)\n## [1] 5 3 5\n\nHere the c() function created a vector with the value 5 as the first element followed by the values in vector b\nOr we can use the variable a that has a value of 5. Let’s add this to vector b\n\nb &lt;- c(a,b)\nb\n## [1] 5 3 5\n\nWhat if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq()\n\nseq(0, 1000, by = 4)\n##   [1]    0    4    8   12   16   20   24   28   32   36   40   44   48   52\n##  [15]   56   60   64   68   72   76   80   84   88   92   96  100  104  108\n##  [29]  112  116  120  124  128  132  136  140  144  148  152  156  160  164\n##  [43]  168  172  176  180  184  188  192  196  200  204  208  212  216  220\n##  [57]  224  228  232  236  240  244  248  252  256  260  264  268  272  276\n##  [71]  280  284  288  292  296  300  304  308  312  316  320  324  328  332\n##  [85]  336  340  344  348  352  356  360  364  368  372  376  380  384  388\n##  [99]  392  396  400  404  408  412  416  420  424  428  432  436  440  444\n## [113]  448  452  456  460  464  468  472  476  480  484  488  492  496  500\n## [127]  504  508  512  516  520  524  528  532  536  540  544  548  552  556\n## [141]  560  564  568  572  576  580  584  588  592  596  600  604  608  612\n## [155]  616  620  624  628  632  636  640  644  648  652  656  660  664  668\n## [169]  672  676  680  684  688  692  696  700  704  708  712  716  720  724\n## [183]  728  732  736  740  744  748  752  756  760  764  768  772  776  780\n## [197]  784  788  792  796  800  804  808  812  816  820  824  828  832  836\n## [211]  840  844  848  852  856  860  864  868  872  876  880  884  888  892\n## [225]  896  900  904  908  912  916  920  924  928  932  936  940  944  948\n## [239]  952  956  960  964  968  972  976  980  984  988  992  996 1000\n\nVectors can only contain elements of the same “class”.\n\nd &lt;- c(1, \"2\", 5, 9)\nd\n## [1] \"1\" \"2\" \"5\" \"9\"\n\n\nas.numeric(d)\n## [1] 1 2 5 9"
  },
  {
    "objectID": "R-basics.html#sec-factors",
    "href": "R-basics.html#sec-factors",
    "title": "3  R Basics",
    "section": "Factors",
    "text": "Factors\nFactors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor()\n\nfactor(c(\"male\", \"female\", \"male\", \"male\", \"female\"))\n## [1] male   female male   male   female\n## Levels: female male\n\n\nfactor(c(\"high\", \"low\", \"medium\", \"high\", \"low\"))\n## [1] high   low    medium high   low   \n## Levels: high low medium\n\n\nf &lt;- factor(c(\"high\", \"low\", \"medium\", \"high\", \"low\"), \n            levels = c(\"high\", \"medium\", \"low\"))\nf\n## [1] high   low    medium high   low   \n## Levels: high medium low"
  },
  {
    "objectID": "R-basics.html#sec-lists",
    "href": "R-basics.html#sec-lists",
    "title": "3  R Basics",
    "section": "Lists",
    "text": "Lists\nLists are containers of objects. Unlike Vectors, Lists can hold different classes of objects.\n\nlist(1, \"2\", 2, 4, 9, \"hello\")\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] \"2\"\n## \n## [[3]]\n## [1] 2\n## \n## [[4]]\n## [1] 4\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] \"hello\"\n\nYou might have noticed that there are not only single brackets, but double brackets [[ ]]\nThis is because Lists can hold not only single elements but can hold vectors, factors, lists, data frames, and pretty much any kind of object.\n\nl &lt;- list(c(1,2,3,4), \"2\", \"hello\", c(\"a\", \"b\", \"c\"))\nl\n## [[1]]\n## [1] 1 2 3 4\n## \n## [[2]]\n## [1] \"2\"\n## \n## [[3]]\n## [1] \"hello\"\n## \n## [[4]]\n## [1] \"a\" \"b\" \"c\"\n\nYou can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]].\n\nl[[1]]\n## [1] 1 2 3 4\n\nTo reference elements within list elements you use double brackets followed by a single bracket\n\nl[[4]][2]\n## [1] \"b\"\n\nYou can even give names to the list elements\n\nperson &lt;- list(name = \"Jason\", \n               phone = \"123-456-7890\", \n               age = 23, \n               favorite_colors = c(\"blue\", \"red\", \"brown\"))\nperson\n## $name\n## [1] \"Jason\"\n## \n## $phone\n## [1] \"123-456-7890\"\n## \n## $age\n## [1] 23\n## \n## $favorite_colors\n## [1] \"blue\"  \"red\"   \"brown\"\n\nAnd you can use the names to reference elements in a list\n\nperson[[\"name\"]]\n## [1] \"Jason\"\nperson[[\"favorite_colors\"]][3]\n## [1] \"brown\""
  },
  {
    "objectID": "R-basics.html#sec-data-frames",
    "href": "R-basics.html#sec-data-frames",
    "title": "3  R Basics",
    "section": "Data Frames",
    "text": "Data Frames\nYou are probably already familiar with data frames. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statistical analyses on.\nSo let’s create a data frame so you can see what one looks like in RStudio\n\ndata &lt;- data.frame(id = 1:10, \n                   x = c(\"a\", \"b\"), \n                   y = seq(10,100, by = 10))\ndata\n##    id x   y\n## 1   1 a  10\n## 2   2 b  20\n## 3   3 a  30\n## 4   4 b  40\n## 5   5 a  50\n## 6   6 b  60\n## 7   7 a  70\n## 8   8 b  80\n## 9   9 a  90\n## 10 10 b 100\n\nYou can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data)\nNotice that it created three columns labeled id, x, and y. Also notice that since we only specified a vector of length 2 for x this column is coerced into 10 rows of repeating “a” and “b”. All columns in a data frame must have the same number of rows.\nYou can use the $ notation to reference just one of the columns in the data frame\n\ndata$y\n##  [1]  10  20  30  40  50  60  70  80  90 100\n\nAlternatively you can use\n\ndata[\"y\"]\n##      y\n## 1   10\n## 2   20\n## 3   30\n## 4   40\n## 5   50\n## 6   60\n## 7   70\n## 8   80\n## 9   90\n## 10 100\n\nTo reference only certain rows within a column\n\ndata$y[1:5]\n## [1] 10 20 30 40 50\ndata[1:5,\"y\"]\n## [1] 10 20 30 40 50"
  },
  {
    "objectID": "R-basics.html#sec-if...then-statements",
    "href": "R-basics.html#sec-if...then-statements",
    "title": "3  R Basics",
    "section": "If…then Statements",
    "text": "If…then Statements\nIf…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,…\nFirst we need to know how to perform logical operations in R\n\n\n\n\nFigure 3.1: List of logical operators in R\n\n\n\nOkay, we have this variable a\n\na &lt;- 5\n\nNow let’s say we want to determine if the value of a is greater than 3\n\na &gt; 3\n## [1] TRUE\n\nYou can see that the output of this statement a &gt; 3 is TRUE\nNow let’s write an if…then statement. If a is greater than 3, then multiply a by 2.\n\nif (a &gt; 3) {\n  a &lt;- a*2\n}\na\n## [1] 10\n\nThe expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code.\nThe { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as.\n\na &lt;- 5\nif (a &gt; 3) a &lt;- a*2\na\n## [1] 10\n\nWhat if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing\n\na &lt;- 5\nif (a &gt; 3) {\n  a &lt;- a*2\n} else {\n  a &lt;- NA\n}\na\n## [1] 10\n\nYou can keep on chaining if…then… else… if… then statements together.\n\na &lt;- 5\nif (is.na(a)) {\n  print(\"Missing Value\")\n} else if (a &lt; 0) {\n  print(\"A is less than 0\")\n} else if (a &gt; 3) {\n  print(\"A is greater than 3\")\n}\n## [1] \"A is greater than 3\""
  },
  {
    "objectID": "R-basics.html#more-resources",
    "href": "R-basics.html#more-resources",
    "title": "3  R Basics",
    "section": "More Resources",
    "text": "More Resources\nFor additional tips in the basics of coding R see:\nhttps://r4ds.hadley.nz\nhttps://ramnathv.github.io/pycon2014-r/visualize/README.html\nhttps://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&tap_s=10907-287229\nhttp://compcogscisydney.org/psyr/"
  },
  {
    "objectID": "R-intermediate.html#sec-for-loops",
    "href": "R-intermediate.html#sec-for-loops",
    "title": "4  R Intermediate",
    "section": "For Loops",
    "text": "For Loops\nFor loops allow you iterate the same line of code over multiple instances.\nLet’s say we have a vector of numerical values\n\nc &lt;- c(1,6,3,8,2,9)\nc\n## [1] 1 6 3 8 2 9\n\nand want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d\nWhat we need to do is loop this if…then statement for each element in c\nWe can start out by writing the for loop statement\n\nfor (i in seq_along(c)) {\n  \n}\n\nThis is how it works. The statement inside of parentheses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c).\nseq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6.\nThe for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration.\n\nfor (i in seq_along(c)) {\n print(i) \n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n\nYou can see how on each iteration it prints the values of seq_along(c) from the first element to the last element.\nWhat we will want to do is, on each iteration of the for loop, access the ith element of the vector c.\nRecall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c.\n\nfor (i in seq_along(c)) {\n print(c[i]) \n}\n## [1] 1\n## [1] 6\n## [1] 3\n## [1] 8\n## [1] 2\n## [1] 9\n\nNow instead of printing i the for loop is printing each element of vector c.\nLet’s use the same if…then statement as above\n\na &lt;- 5\nif (a &gt; 3) {\n  a &lt;- a*2\n} else {\n  a &lt;- NA\n}\na\n## [1] 10\n\nBut instead we need to replace a with c[i]\nFor now let’s just print() the output of the if… then statement.\n\nfor (i in seq_along(c)) {\n  if (c[i] &gt; 3) {\n    print(c[i]*2)\n  } else {\n    print(NA)\n  }\n}\n## [1] NA\n## [1] 12\n## [1] NA\n## [1] 16\n## [1] NA\n## [1] 18\n\nNow for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA.\nBut just printing things to the console is useless! Let’s overwright the old values in c with the new values.\n\nfor (i in seq_along(c)) {\n  if (c[i] &gt; 3) {\n    c[i] &lt;- c[i]*2\n  } else {\n    c[i] &lt;- NA\n  }\n}\n\nBut what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works.\nBut if you are goind to do this to a “new” vector that is not yet created you will run into an error.\n\nc &lt;- c(1,6,3,8,2,9)\nfor (i in seq_along(c)) {\n  if (c[i] &gt; 3) {\n    d[i] &lt;- c[i]*2\n  } else {\n    d[i] &lt;- NA\n  }\n}\n## Error: object 'd' not found\n\nYou first need to create vector d - in this case we can create an empty vector.\n\nd &lt;- c()\n\nSo the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA.\n\nc &lt;- c(1,6,3,8,2,9)\nd &lt;- c()\nfor (i in seq_along(c)) {\n  if (c[i] &gt; 3) {\n    d[i] &lt;- c[i]*2\n  } else {\n    d[i] &lt;- NA\n  }\n}\n\nc\n## [1] 1 6 3 8 2 9\nd\n## [1] NA 12 NA 16 NA 18\n\nYay! Good job."
  },
  {
    "objectID": "R-intermediate.html#sec-creating-your-own-functions",
    "href": "R-intermediate.html#sec-creating-your-own-functions",
    "title": "4  R Intermediate",
    "section": "Creating Your Own Functions",
    "text": "Creating Your Own Functions\nThis section is optional. It will go over how to create your own functions.\nEven if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actually work.\nWe are going to create a function that calculates an average of values.\nTo define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance,\n\nfunction_name &lt;- function() {\n  \n}\n\nThis is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average.\nLet’s say we have an array a that has 10 elements\n\na &lt;- c(1,7,4,3,8,8,7,9,2,4)\na\n##  [1] 1 7 4 3 8 8 7 9 2 4\n\nTo calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions.\n\nsum(a)\n## [1] 53\nlength(a)\n## [1] 10\n\nsum(a)/length(a)\n## [1] 5.3\n\nEasy! So now we can just put this into a function.\n\naverage &lt;- function(x) {\n  avg &lt;- sum(x)/length(x)\n  return(avg)\n}\n\nWhen creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function\nYou can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function.\nLet’s see the function in action\n\naverage(a)\n## [1] 5.3\n\nCool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate.\nBut what if our vector contains missing values?\n\nb &lt;- c(1,NA,4,2,7,NA,8,4,9,3)\naverage(b)\n## [1] NA\n\nUh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is because in our function we use the function sum() without specifying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE.\nIt is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present.\nLet’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however.\nBefore we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change:\n\navg &lt;- sum(x)/length(x)\n\nto\n\navg &lt;- sum(x, na.rm = na.ignore)/length(x)\n\nLet’s try this out on our vector b\n\nna.ignore &lt;- TRUE\nsum(b, na.rm = na.ignore)/length(b)\n## [1] 3.8\n\nWe can test if our average function is calculating this correctly by using the actual base R function mean().\n\nmean(b, na.rm = TRUE)\n## [1] 4.75\n\nUh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8.\nUnfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by\n\nlength(b[!is.na(b)])\n## [1] 8\n\nThis is saying, evaluate the length of elements in b that are not missing.\nNow we can modify our function with\n\nna.ignore &lt;- TRUE\nsum(b, na.rm = na.ignore)/length(b[!is.na(b)])\n## [1] 4.75\n\nto get\n\naverage &lt;- function(x, na.ignore = FALSE) {\n  avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)])\n  return(avg)\n}\n\n\naverage(b, na.ignore = TRUE)\n## [1] 4.75\n\n\nmean(b, na.rm = TRUE)\n## [1] 4.75\n\nWalla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default.\nWhen using functions it is important to know what the default values are\nBoth for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function."
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#sec-r-scripts-vs.-quarto",
    "href": "quarto-rmarkdown-basics.html#sec-r-scripts-vs.-quarto",
    "title": "5  Quarto Basics",
    "section": "R Scripts vs. Quarto",
    "text": "R Scripts vs. Quarto\nUsing R scripts makes sense when the purpose is to output a data file. Once you have your script all setup, you don’t necessarily care about what objects are created in the R environment or what is being printed to the console, etc. What you care about is the outputted data file. The R script was just a means to get there.\nWith data visualization and statistical analysis we care about generating figures, tables, and statistical reports - not data files. The process of data visualization and statistical analysis is also typically more explorative, iterative, and cyclic. Ultimately, we will want to generate a formatted, easy to read, final report of our analyses. All these things are not possible with R scripts.\nAn alternative to R scripts (.R) are Quarto (.qmd) and R Markdown (.Rmd) documents. The primary use of Quarto documents in the lab is to generate statistical reports in .html format.\n\n\n\n\n\n\nNote\n\n\n\nQuarto documents are the next generation of R Markdown documents, therefore, this guide will simply refer to Quarto documents from here on out.\nThe basic structure of R Markdown and Quarto documents are the same. And most, though not all, features of R Markdown documents have been ported over to Quarto documents. And for the most part you can convert R Markdown documents to Quarto without any problems.\n\n\nFollow this link for a brief Intro to Quarto Documents.\nTo open an Quartodocument go to\nFile -&gt; New File -&gt; Quarto Document…\nSelect HTML and click Create\nAn example Quarto document will open. We can use this to get familiar with the structure of Quarto documents.\nBy default, Quarto documents will open using the Visual Markdown Editor. This editor mode creates an easy to read formatted text that feels more similar to writing in a Microsoft Word document. This type of visual markdown text is called WYSIWG. It hides the markdown formatting and displays the text as it would be formatted in a document.\n\n\n\n\n\n\nNote\n\n\n\nYou can switch to Source Editor mode if you want to see the actual raw non-formatted content of the document. Go ahead and try switching modes to see the difference.\nYou will likely find some advantages to editing Quarto documents in source mode rather than Visual Editor mode.\n\n\nThere are three types of content that form the structure of a Quarto document.\n\nA YAML header\nR code chunks\nMarkdown formatted text"
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#sec-yaml-header",
    "href": "quarto-rmarkdown-basics.html#sec-yaml-header",
    "title": "5  Quarto Basics",
    "section": "YAML header",
    "text": "YAML header\nThe YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashes.\n---\ntitle: \"Title of document\"\noutput: html_document\n---\nThere are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://quarto.org/docs/output-formats/html-basics.html"
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#sec-r-code-chunks",
    "href": "quarto-rmarkdown-basics.html#sec-r-code-chunks",
    "title": "5  Quarto Basics",
    "section": "R code chunks",
    "text": "R code chunks\nUnlike a typical R script file (.R), a Quarto document (.qmd) is a mixture of markdown text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks.\nCreate an R code chunk\nR code chunks are enclosed with\n#&gt; ```{r}\n#&gt; ```\nYou can create R code chunks with the shortcut:\nMac: ⌘ ⌥ i (command + alt/opt + i)\nWindows: ⌃ ⌥ i (ctrl + alt + i)\nOr by going to Insert -&gt; Code Cell -&gt; R in the toolboar of the Quarto document (same section as Source / Visual).\nInsert the followin code in the R code chunk\n\nsummary(cars)\n\nCreate another R code chunk with the following code\n\nplot(pressure)\n\nExecute R code chunk\nTo run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do so.\nYou can see that the results of the R code are now displayed in the document."
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#sec-markdown-text",
    "href": "quarto-rmarkdown-basics.html#sec-markdown-text",
    "title": "5  Quarto Basics",
    "section": "Markdown text",
    "text": "Markdown text\nThe markdown text sections are not the same as adding comments to lines of code. You can write up descriptive reports, create bulletted or numbered lists, embed images or web links, create tables, and more.\nThe text is formatted using a language known as Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by Markdown syntax.\nThere are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. Here is one I reference often: Markdown Cheatsheet"
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#sec-rendering-a-quarto-document",
    "href": "quarto-rmarkdown-basics.html#sec-rendering-a-quarto-document",
    "title": "5  Quarto Basics",
    "section": "Rendering a Quarto document",
    "text": "Rendering a Quarto document\nWhen you have finalized the content of a Quarto document you will then want to generate the document into the specified output format (.html in this case).\nTo render a Quarto document click on Render at the top. This will\n\nGenerate a live preview of the document in the Viewer pane\nCreate a .html file\n\nYou can now use the .html file to view your visualizations and statistical analyses and share or present them with the lab."
  },
  {
    "objectID": "quarto-rmarkdown-basics.html#visual-editor",
    "href": "quarto-rmarkdown-basics.html#visual-editor",
    "title": "5  Quarto Basics",
    "section": "Visual Editor",
    "text": "Visual Editor\nSee Visual Editing in RStudio for more details on using the visual editor including shortcuts and tips.\nThere are two convenient ways to insert formatted content in the visual editor:\n\nUse the Insert tool in the toolbar at the top\nSimply type / to quickly display the Insert Anything shortcut options. You can even start typing/searching what you want to insert."
  },
  {
    "objectID": "tips.html#sec-helper-function",
    "href": "tips.html#sec-helper-function",
    "title": "6  Useful Tips",
    "section": "Helper Function",
    "text": "Helper Function\nWhen working with functions, it can be difficult to remember the argument names and values you need to specify. However, there is a helper function that can make this process much easier: ?. By typing ?function_name() in the console, you can access the function’s documentation and quickly figure out what arguments you need to provide. This can save you a lot of time and frustration, especially when working with complex functions.\n\n?seq()"
  },
  {
    "objectID": "tips.html#sec-generative-ai",
    "href": "tips.html#sec-generative-ai",
    "title": "6  Useful Tips",
    "section": "Generative AI",
    "text": "Generative AI\nGenerative AI can be a useful assistant to both learning and writing R code. It will make mistakes but that is what actually makes it a useful learning tool, it can also help you discover ways of doing things you wouldn’t have thought of before.\nStart out small, if you are not immediately sure how to proceed with writing R code for something you want to do then prompt an AI model to write some code and provide an explanation for you. Continuing prompting it and/or edit the code to suit your specific need.\nYou should also start using AI models to assist you in other areas of your work as well. Again, just start out small. Get in the habit and setup a workflow where an AI model is right at your fingertips, just a few clicks of the mouse or keyboard away."
  },
  {
    "objectID": "tips.html#sec-store-frequently-used-code-for-reuse",
    "href": "tips.html#sec-store-frequently-used-code-for-reuse",
    "title": "6  Useful Tips",
    "section": "Store Frequently Used Code For Reuse",
    "text": "Store Frequently Used Code For Reuse\nIf you find yourself using the same or similar sequences of code repeatedly, it can be incredibly helpful to have a central location where you can store your frequently used code and easily retrieve it at any time. While GitHub is a popular option for this, it requires learning a new system. Notion is a program I personally use and recommend, but a simple folder on your desktop with R scripts is also a viable option.\nWithout a central location for frequently used code, you may find yourself spending a significant amount of time and effort searching through previous projects to locate the code you need. This can be a daunting task, requiring a good memory and a lot of time. Having a singular place to go to for all your frequently used code can make this process much easier and save you time and energy in the long run."
  },
  {
    "objectID": "tips.html#sec-use-templates",
    "href": "tips.html#sec-use-templates",
    "title": "6  Useful Tips",
    "section": "Use Templates!",
    "text": "Use Templates!\nCreating your own templates and/or templates for your lab is highly recommended. This will save you a significant amount of time and effort, enabling you to start working with your data more quickly and set up new data analysis projects with ease. Additionally, consider creating an R package that include Quarto documents for analyses and reports. This will help streamline your workflow even further.\nI have developed several R packages for the lab that contain useful templates and documents. Please make use of them."
  },
  {
    "objectID": "tips.html#sec-limit-number-of-packages",
    "href": "tips.html#sec-limit-number-of-packages",
    "title": "6  Useful Tips",
    "section": "Limit Number of Packages",
    "text": "Limit Number of Packages\nWhen using R, it’s recommended to limit the number of packages you use. You may be surprised at how much you can accomplish with just a few packages. Limiting your package usage makes it easier to manage your installed packages, and also helps with the learning curve, as you don’t have to memorize functions from a large number of packages."
  },
  {
    "objectID": "tips.html#sec-ask-a-friend",
    "href": "tips.html#sec-ask-a-friend",
    "title": "6  Useful Tips",
    "section": "Ask a Friend",
    "text": "Ask a Friend\nWhile there are many functions available for most tasks, finding the right one can be a challenge. Instead of spending time on long and convoluted solutions, consider asking friends or colleagues if they know of a package or function that could help you accomplish what you need. Collaborating with others is a great way to discover new functions and tools that you may not have known existed."
  },
  {
    "objectID": "tips.html#sec-google-search",
    "href": "tips.html#sec-google-search",
    "title": "6  Useful Tips",
    "section": "Google Search",
    "text": "Google Search\nThis section may be less relevant now, given the rise of generative AI models. However, Google can still be a valuable tool for finding R solutions quickly. To get more targeted results, try including the name of the package or function you think might help you in your search phrase. If you use dplyr frequently, for example, use dplyr in your search phrase to find solutions that are consistent with your preferred way of working.\nIf you’re unsure of which function to use, try including the function name in your search phrase. You can also use Google to find more detailed documentation for specific packages or functions. However, it’s best to avoid links that start with https://cran.r-project.org or https://www.rdocumentation.org, as these are usually just copies of the ? help documentation.\nInstead, look for links that include https://github.com. GitHub repos often include links to more extensive documentation, such as the GitHub repo for the popular dplyr package (https://github.com/tidyverse/dplyr), which has a link on the right side of the repo page to detailed documentation on all the functions in the package."
  },
  {
    "objectID": "tips.html#sec-explore-functions-in-a-package",
    "href": "tips.html#sec-explore-functions-in-a-package",
    "title": "6  Useful Tips",
    "section": "Explore Functions in a Package",
    "text": "Explore Functions in a Package\nAdditionally, there may be functions in the packages you already use that you have yet to discover. Take some time to explore all the different functions within a package, particularly those that you use frequently. The GitHub repository for a package is a great resource for exploring all of its functions. To access it, simply type the name of the package followed by the term “GitHub” into a search engine."
  },
  {
    "objectID": "this-is-the-way.html#sec-the-pipe-operator",
    "href": "this-is-the-way.html#sec-the-pipe-operator",
    "title": "7  This is the Way",
    "section": "The pipe operator",
    "text": "The pipe operator\nThere are now two pipe operators. The original dplyr pipe operator is %&gt;% and the newer base R pipe operator is |&gt; . You can use either one but as a lab we will try to switch over to using the base R pipe.\nThe pipe operator is extremely useful and we use it extensively but needs some explanation.\nThere are different methods for writing code that performs multiple functions on the same object. For instance, all these examples use the tidyverse but only one of them (the last one) uses the pipe operator.\n\nlibrary(dplyr)\n# create a sample dataframe\ndata &lt;- data.frame(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"))\n\n# three functions, filter, calculate a mean, \n# then select only some columns to keep\ndata &lt;- filter(data, y != \"c\")\ndata &lt;- mutate(data, x_mean = mean(x))\ndata &lt;- select(data, y, x_mean)\n\nAnother way of writing this code:\n\nlibrary(dplyr)\n# create a sample dataframe\ndata &lt;- data.frame(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"))\n\n# three functions, filter, calculate a mean, \n# then select only some columns to keep\ndata &lt;- select(mutate(filter(data, y != \"c\"), \n                      x_mean = mean(x)), y, x_mean)\n\nOf these two methods, the first is preferable as it is easier to read. Another alternative is to use the pipe operator |&gt;.\n\nlibrary(dplyr)\n# create a sample dataframe\ndata &lt;- data.frame(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"))\n\n# three functions, filter, calculate a mean, \n# then select only some columns to keep\ndata &lt;- data |&gt;\n  filter(y != \"c\") |&gt;\n  mutate(x_mean = mean(x)) |&gt;\n  select(y, x_mean)\n\nWith the pipe operator, the result of the previous line gets passed (or piped) onto the next function. The first line in this example is simply specifying the data frame that is being passed from one line to the next. Notice how I did not have to specify data inside the filter(), mutate(), and select() functions. This makes the code more concise and easier to read. The end result of the last function, then gets assigned to the data &lt;-.\nIf an error occurs somewhere in the pipe, an easy way to troubleshoot it is to remove the pipe operator at one of the lines one at a time to figure out where the error is located."
  },
  {
    "objectID": "import-write.html#sec-csv-files",
    "href": "import-write.html#sec-csv-files",
    "title": "8  Import and Export Data",
    "section": "CSV Files",
    "text": "CSV Files\ncsv files are by far the easiest files to import into R and most software programs. For this reason, I suggest any time you want to save/output a data file to your computer, do it in csv format.\nImport .csv\nWe can import csv files using read_csv() from the readr package.\n\nlibrary(readr)\nread_csv(\"filepath/datafile.csv\")\n\nYou can see this is very simple. We just need to specify a file path to the data.\n\n\n\n\n\n\nImportant\n\n\n\nDO NOT USE ABSOLUTE FILE PATHS!\n\n\nI will talk more about file paths later but for now we will use absolute file paths, although it is highly suggested not to use them. This chapter is more about the different functions to import various types of data files.\nFirst, figure out the absolute file path to your downloads folder (or wherever the unzipped data folder is located). On Windows the absolute file path will usually start from the C:/ drive. On Macs, it starts from ~/\nImport the Flanker_Scores.csv file. You might have something that looks like\n\nread_csv(\"~/Downloads/Flanker_Scores.csv\")\n\nHowever, this just printed the output of read_csv() to the console. To actually import this file into R, we need to assign it to an object in our Environment.\n\nimport_csv &lt;- read_csv(\"~/Downloads/Flanker_Scores.csv\")\n\nYou can name the object whatever you like. I named it import_csv.\nTo view the data frame\n\nView(import_csv)\n\nOutput .csv\nWe can output a csv file using write_csv() from the readr package.\n\nwrite_csv(object, \"filepath/filename.csv\")\n\nLet’s output the object import_csv to a csv file named: new_Flanker_Scores.csv to the downloads folder\n\nwrite_csv(import_csv, \"~/Downloads/new_Flanker_Scores.csv\")\n\nNote that whenever writing (outputting) a file to our computer there is no need to assign the output to an object."
  },
  {
    "objectID": "import-write.html#sec-tab-delimited",
    "href": "import-write.html#sec-tab-delimited",
    "title": "8  Import and Export Data",
    "section": "Tab-Delimited",
    "text": "Tab-Delimited\ntab-delimited files are a little more tedious to import just because they require specifying more arguments. Which means you have to memorize more to import tab-delimited files.\nImport .txt\nTo import a tab-delimited file we can use read_delim() from the readr package.\n\nread_delim(\"filepath/filename.txt\", delim = \"\\t\", \n           escape_double = FALSE, trim_ws = TRUE)\n\nThere are three additional arguments we have to specify: delim, escape_double, and trim_ws. The notation for tab-delimted files is \"\\t\".\nLet’s import the Flanker_raw.txt file\n\nimport_tab &lt;- read_delim(\"~/Downloads/Flanker_raw.txt\", \"\\t\", \n                         escape_double = FALSE, trim_ws = TRUE)\n\nView the import_tab object\nOutput .txt\nWe can output a tab-delimited file using write_delim() from the readr package.\n\nwrite_delim(object, \n            path = \"filepath/filename.txt\", delim = \"\\t\")\n\nOutput the import_tab object to a file named: new_Flanker_raw.txt\n\nwrite_delim(import_tab, \n            path = \"~/Downloads/Flanker_raw.txt\", delim = \"\\t\")"
  },
  {
    "objectID": "import-write.html#sec-spss",
    "href": "import-write.html#sec-spss",
    "title": "8  Import and Export Data",
    "section": "SPSS",
    "text": "SPSS\nAs horrible as it might sound, there might be occasions where we need to import an SPSS data file. And worse, we might need to output an SPSS data file!\nI will suggest to use different packages for importing and outputing spss files.\nImport .sav\nTo import an SPSS data file we can use read.spss() from the foreign package.\n\nlibrary(foreign)\nread.spss(\"filepath/filename.sav\", \n          to.data.frame = TRUE, use.value.labels = TRUE)\n\nThe use.value.labels argument allows us to import the value labels from an SPSS file.\nImport and View the sav file CH9 Salary Ex04.sav\n\nimport_sav &lt;- read.spss(\"~/Downloads/CH9 Salary Ex04.sav\")\n\nOutput .sav\nTo output an SPSS data file we can use write_sav() from the haven packge.\n\nlibrary(haven)\nwrite_sav(object, \"filepath/filename.sav\")\n\nGo ahead and output the import_sav object to a file: new_CH9 Salary Ex04.sav\n\nwrite_sav(import_sav, \"~/Downloads/new_CH9 Salary Ex04.sav\")"
  },
  {
    "objectID": "import-write.html#sec-rstudio-import-gui",
    "href": "import-write.html#sec-rstudio-import-gui",
    "title": "8  Import and Export Data",
    "section": "RStudio Import GUI",
    "text": "RStudio Import GUI\nThe nice thing about R Studio is that there is also a GUI for importing data files.\nWhen you are having difficulty importing a file correctly or unsure of the file format you can use the RStudio Import GUI.\nIn the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. Select whatever data type you want to import\nYou will see a data import window open up that looks like this\n\n\n\n\nFigure 8.2: RStudio Import GUI\n\n\n\nSelect “Browse” on the top right and select the data file you want to import.\nThe “Data Preview” window will let you see if it is importing it in the right format. You can change the import options below this.\nYou might want to change the “Name” but you can always do this later in the R Script.\nMake sure all the settings are correct by assessing the “Data Preview” window. Does the data frame look as you would expect it to?\nFinally, copy and paste the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines.\nRather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script.\ncsv files have a nice feature in that RStudio knows that these are file types we might want to import. So instead of navigating through the Import Dataset GUI we can just click on the file in the Files window pane."
  },
  {
    "objectID": "import-write.html#sec-e-prime--export",
    "href": "import-write.html#sec-e-prime--export",
    "title": "8  Import and Export Data",
    "section": "E-Prime -Export",
    "text": "E-Prime -Export\nThe E-Prime program we use to administer tasks has the option to output a .txt file. These file types are appended with -Export in the filename. These files are tab delimited but also have a unique encoding which makes them difficult to figure out how to import into R. After some trial and error I figured out they are encoded as UCS-2LE, thus they can be imported with the following settings in read_delim()\n\nread_delim(\"data/folder\", delim = \"\\t\", \n           escape_double = FALSE, trim_ws = TRUE, na = \"NULL\",\n           locale = locale(encoding = \"UCS-2LE\"))"
  },
  {
    "objectID": "import-write.html#sec-multiple-files",
    "href": "import-write.html#sec-multiple-files",
    "title": "8  Import and Export Data",
    "section": "Multiple Files",
    "text": "Multiple Files\nYou might find yourself in a situation where you need to import multiple data files and merge them into a single dataframe. For instance, with a batch of E-Prime -Export data files.\nBind\nIn R, a “bind” is combining data frames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes data frames that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns.\nTo bind multiple files together requires only a couple of steps\n\nGet a list of all the files\n\nImport and bind the list of files using purrr::map_df()\nmap_df() is a function from the purrr R package that allows you to apply a function to each element of a list or vector and then combine the results into a data frame. It is especially useful when you have a list of data frames that you want to combine into a single data frame. The function applies a function to each data frame in the list, and then combines the results into a single data frame. This paragraph was written by Notion AI\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr)\n\n# 1. Get a list of all files\nfiles &lt;- list.files(\"data/folder\", pattern = \"-Export\", \n                    full.names = TRUE)\n\n# 2. Import and bind the list of files using purrr::map_df()\ndata_import &lt;- files |&gt;\n  map_df(read_delim, delim = \"\\t\", \n         escape_double = FALSE, trim_ws = TRUE, na = \"NULL\",\n         locale = locale(encoding = \"UCS-2LE\"))\n\nThis example shows how to import multiple E-Prime -Export files into a single dataframe. But the same procedure, with different arguments inside of purrr::map_df(), can be used for any type of data file.\nJoin\nIn R, a “join” is merging data frames together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the data frames together.\nTo join multiple files together requires a few steps:\n\nGet a list of all the files\n\nImport the list of files using purrr::map_df()\nmap_df() is a function from the purrr R package that allows you to apply a function to each element of a list or vector and then combine the results into a data frame. It is especially useful when you have a list of data frames that you want to combine into a single data frame. The function applies a function to each data frame in the list, and then combines the results into a single data frame. This paragraph was written by Notion AI\n\n\nJoin each imported data file into a single data frame by a unique id using purrr::reduce() and dplyr::full_join().\nThe function reduce() from the purrr R package is used to iteratively apply a function to a list or vector and accumulate the results. The function takes two arguments: the list or vector to be processed, and the function to be applied to each element. The function is applied to the first element and the result is stored. The function is then applied to the stored result and the second element, and the result is stored. This process is repeated until all elements have been processed, and the final result is returned. This can be useful for operations like calculating the sum or product of a list of numbers, or concatenating a list of strings. This paragraph was written by Notion AI\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr)\n\n# 1. Get a list of all files\nfiles &lt;- list.files(\"data/folder\", pattern = \"_Scores\", \n                    full.names = TRUE)\n\n# 2 - 3. Import with lapply() and then join with plyr::join_all()\ndata_import &lt;- files |&gt;\n  map(read_csv) |&gt;\n  reduce(full_join, by = \"Subject\")\n\nThis example shows how to import multiple scored data files (csv file type) and join all of them by ids in the “Subject” column using purrr::reduce() and dplyr::full_join()."
  },
  {
    "objectID": "working-with-data.html#sec-dplyr",
    "href": "working-with-data.html#sec-dplyr",
    "title": "9  Working with Data",
    "section": "dplyr",
    "text": "dplyr\nThe language of dplyr will be the underlying framework for how you will think about manipulating a dataframe.\n\n\n\n\nFigure 9.1: dplyr logo\n\n\n\ndplyr uses intuitive language that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs that refer to performing a particular action on a data frame. The core dplyr functions are:\n\nrename() renames columns\nfilter() filters rows based on their values in specified columns\nselect() selects (or removes) columns\nmutate() creates new columns based on transformation from other columns, or edits values within existing columns\ngroup_by() splits data frame into separate groups based on specified columns\nsummarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.)\n\nFor more information on these functions Visit the dplyr webpage\nFor more detailed instructions on how to use the dplyr functions see the Data Transformation chapter in the popular R for Data Science book."
  },
  {
    "objectID": "working-with-data.html#sec-stay-within-the-data-frame",
    "href": "working-with-data.html#sec-stay-within-the-data-frame",
    "title": "9  Working with Data",
    "section": "Stay within the Data Frame",
    "text": "Stay within the Data Frame\nNot only is the language of dplyr intuitive but it allows you to perform data manipulations all within the dataframe itself, without having to create external variables, lists, for loops, etc.\nIt can be tempting to hold information outside of a data frame but in general I suggest avoiding this strategy. Instead, hold the information in a new column within the data frame itself.\nFor example: A common strategy I see any many R scripts is to hold the mean or count of a column of values outside the dataframe and in a new variable in the Environment.\n\ndata &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3))\n\ny_mean &lt;- mean(data$y)\n\nThis variable then could be used to subtract out the mean from the values in column y\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\ndata &lt;- mutate(data, \n               y_new = y - y_mean)\n\nhead(data)\n##   x y  y_new\n## 1 1 2 -1.125\n## 2 6 3 -0.125\n## 3 4 2 -1.125\n## 4 3 1 -2.125\n## 5 7 4  0.875\n## 6 5 6  2.875\n\nAlthough there is nothing wrong with this approach, in general, I would advise against this strategy. A better strategy is to do all this without leaving the data frame data.\n\nlibrary(dplyr)\n\ndata &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3))\ndata &lt;- mutate(data,\n               y_mean = mean(y),\n               y_new = y - y_mean)\n\nhead(data)\n##   x y y_mean  y_new\n## 1 1 2  3.125 -1.125\n## 2 6 3  3.125 -0.125\n## 3 4 2  3.125 -1.125\n## 4 3 1  3.125 -2.125\n## 5 7 4  3.125  0.875\n## 6 5 6  3.125  2.875\n\nIt can be tempting to also think about writing for loops in your R script, but honestly for the most part for loops are avoidable thanks to a dplyr function called group_by().\nThe only time I end up needing a for loop is when creating code to put into a function."
  },
  {
    "objectID": "working-with-data.html#setup-r-script",
    "href": "working-with-data.html#setup-r-script",
    "title": "9  Working with Data",
    "section": "Setup R Script",
    "text": "Setup R Script\nSetup\nAt the top of your script load the three packages you will need for this Chapter\n\n# Setup\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\nNotice how I added a commented line at the top. Adding comments to your scripts is highly advisable, as it will help you understand your scripts when you come back to them after not working on them for a while.\nImport\nImport the data file you downloaded. Refer to Chapter 8 for how to do this.\n\nimport &lt;- read_csv(\"data/tidyverse_example.csv\")\n\nIt is always a good idea to get to know your dataframe before you start messing with it. What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc.\nWhat are the column names? use colnames() for a quick glance at the column names\n\ncolnames(import)\n##  [1] \"Subject\"              \"TrialProc\"            \"Trial\"               \n##  [4] \"Condition\"            \"RT\"                   \"ACC\"                 \n##  [7] \"Response\"             \"TargetArrowDirection\" \"SessionDate\"         \n## [10] \"SessionTime\"\n\nTo take a quick look at the first few rows of a dataframe use head().\n\nhead(import)\n## # A tibble: 6 × 10\n##   Subject TrialProc Trial Condition      RT   ACC Response\n##     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n## 1   14000 practice      1 incongruent  1086     1 left    \n## 2   14000 practice      2 incongruent   863     1 left    \n## 3   14000 practice      3 congruent     488     1 right   \n## 4   14000 practice      4 incongruent   588     1 right   \n## 5   14000 practice      5 congruent     581     1 right   \n## 6   14000 practice      6 incongruent   544     1 right   \n## # ℹ 3 more variables: TargetArrowDirection &lt;chr&gt;, SessionDate &lt;chr&gt;,\n## #   SessionTime &lt;time&gt;\n\nThis gives you a good idea of what column names you will be working with and what kind of values they contain.\nTo evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column.\n\nunique(import$Condition)\n\n[1] \"incongruent\" \"congruent\"  \n\nunique(import$Trial)\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192\n\nmax(import$Trial)\n\n[1] 192\n\nlength(unique(import$Subject))\n\n[1] 410\n\nunique(import$TrialProc)\n\n[1] \"practice\" \"real\"    \n\nunique(import$ACC)\n\n[1] 1 0\n\n\nAll these functions we just used from colnames() to unique() were to temporarily evaluate our data. They are not required to perform the actual data analysis. Therefore, I usually just type these in the console. A general rule of thumb is that if it is not required to be saved in your Script file then just type it in the console.\n\nOkay let’s take a look at how to use the dplyr functions to score this data."
  },
  {
    "objectID": "working-with-data.html#sec-rename",
    "href": "working-with-data.html#sec-rename",
    "title": "9  Working with Data",
    "section": "rename()",
    "text": "rename()\nWe do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple\n\ndata &lt;- rename(import, Accuracy = ACC)\n\nrename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. This will be illustrated this later\nNotice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new data frame, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_csv() import function again to be able to see the original data file, just a little more tedious."
  },
  {
    "objectID": "working-with-data.html#sec-filter",
    "href": "working-with-data.html#sec-filter",
    "title": "9  Working with Data",
    "section": "filter()",
    "text": "filter()\nfilter() is an inclusive filter and requires the use of logical statements. In Chapter 2: Basic R I talked a little bit about logical statements. Figure 3.1 shows a list of logical operators in R.\nIn addition to the logical operators, other functions can be used in filter(), such as:\n\nis.na() - include if missing\n!is.na() - include if not missing\nbetween() - values that are between a certain range of numbers\nnear() - values that are near a certain value\n\nWe do not want to include practice trials when calculating the mean on RTs. We will use filter() to remove these rows. First let’s evaluate the values in these columns\n\nunique(import$TrialProc)\n## [1] \"practice\" \"real\"\nunique(import$Condition)\n## [1] \"incongruent\" \"congruent\"\n\nWe can specify our filter() in a couple of different ways\n\ndata &lt;- filter(data, \n               TrialProc != \"practice\", \n               Condition != \"neutral\")\n\nor\n\ndata &lt;- filter(import, \n               TrialProc == \"real\", \n               Condition == \"congruent\" | Condition == \"incongruent\")\n\nSpecifying multiple arguments separated by a comma , in filter() is equivalent to an & (and) statement.\nIn the second option, since there are two types of rows on Condition that we want to keep we need to specify Condition == twice, separated by | (or). We want to keep rows where Condition == \"congruent\" or Condition == \"incongruent\"\nNotice that the arguments have been separated on different lines. This is okay to do and makes it easier to read the code. Just make sure the end of the line still has a comma.\nGo ahead and view data. Did it properly remove practice trials? How about neutral trials?\n\nunique(data$TrialProc)\n## [1] \"real\"\nunique(data$Condition)\n## [1] \"incongruent\" \"congruent\"\n\nAgain you should type these in the console NOT in the R Script!\nThere is a lot of consistency of how you specify arguments in the dplyr package.\n\nYou always first specify the data frame that the function is being performed on, followed by the arguments for that function.\nColumn names can be called just like regular R objects, that is without putting the column name in \" \" like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put \" \" around column names."
  },
  {
    "objectID": "working-with-data.html#sec-select",
    "href": "working-with-data.html#sec-select",
    "title": "9  Working with Data",
    "section": "select()",
    "text": "select()\nselect() allows you to select which columns to keep and/or remove.\nLet’s keep Subject, Condition, RT, Trial, and Accuracy and remove TrialProc, TargetArrowDirection, SessionDate, and SessionTime.\nselect() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage\nWe could just simply select all the columns we want to keep\n\ndata &lt;- select(data, Subject, Condition, RT, Trial, Accuracy)\n\nalternatively we can specify which columns we want to remove by placing a - in front of the columns\n\ndata &lt;- select(data, -TrialProc, -TargetArrowDirection, \n               -SessionDate, -SessionTime)\n\nor we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session\n\ndata &lt;- select(data, -TrialProc, -TargetArrowDirection, \n               -starts_with(\"Session\"))\n\nYou might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightforward method of simply selecting which columns to keep.\nYou can also rename variables as you select() them… let’s change Accuracy back to ACC… just because we are crazy!\n\ndata &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy)\n\nWe are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy."
  },
  {
    "objectID": "working-with-data.html#sec-mutate",
    "href": "working-with-data.html#sec-mutate",
    "title": "9  Working with Data",
    "section": "mutate()",
    "text": "mutate()\nmutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the data frame. You can\n\nchange the values in already existing columns\ncreate new columns based on transformation of other columns\n\nChanging values in an existing column\nReaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating means.\nWhat we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the data frame and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column:\n\nmin(data$RT)\n## [1] 0\n\nNow lets mutate()\n\ndata &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT))\n\nSince we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statement. To do so within the mutate() function we use the function called ifelse().\nifelse() evaluates a logical statement specified in the first argument, RT &lt; 200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next argument, in this case sets RT = NA. If the logical statement is FALSE then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged).\nCreating a new column\nLet’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, across all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.)\nSo first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean.\n\nmean(data$RT, na.rm = TRUE)\n## [1] 529.1414\n\nSince we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE.\nWe can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT.\nFirst take note of how many columns there are in data\n\nncol(data)\n## [1] 5\n\nSo after calculating the grandRT we should expect there to be one additional column for a total of 6 columns\n\ndata &lt;- mutate(data, grandRT = mean(RT, na.rm = TRUE))\n\nCool!\nNow let’s calculate another column that is the difference between RT and grandRT.\n\ndata &lt;- mutate(data, RTdiff = RT - grandRT)\n\nWe can put all these mutate()s into one mutate()\n\ndata &lt;- mutate(data, \n               RT = ifelse(RT &lt; 200, NA, RT),\n               grandRT = mean(RT, na.rm = TRUE),\n               RTdiff = RT - grandRT)\n\nNotice how I put each one on a separate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line."
  },
  {
    "objectID": "working-with-data.html#sec-case_when",
    "href": "working-with-data.html#sec-case_when",
    "title": "9  Working with Data",
    "section": "case_when()",
    "text": "case_when()\nOften times you will want to mutate() values conditionally based on values in other columns. There are two functions that will help you do this, ifelse() and case_when(). ifelse() is a base R function and case_when() is a dplyr function.\nifelse() takes the format: ifelse(conditional argument, value if TRUE, value if FALSE)\nAs an example, lets say we want to code a new variable that indicates whether the reaction time on a trial met a certain response deadline or not. Let’s call this column Met_ResponseDeadline and give a value of 1 to trials that met the deadline and 0 to trials that did not meet the deadline. Let’s set the response deadline at a reaction time of 500 milliseconds.\nThe conditional argument will take the form: RT is less than or equal to 500. If this statement is TRUE, then we will assign a value of 1 to the column Met_ResponseDeadline. If this statement is FALSE, then we will assign a value of 0 to the column Met_ResponseDeadline.\nThe code looks like:\n\ndata &lt;- import |&gt;\n  mutate(Met_ResponseDeadline = ifelse(RT &lt;= 500, 1, 0))\n\nCheck out data to make sure it worked.\nYou can even combine multiple ifelse() statements into one. Let’s say we actually want to recode the column ACC to reflect not just correct and incorrect response but also whether they met the response deadline or not. That is, a value of 1 will represent responses that were correct AND met the response deadline and values of 0 represent responses that were either incorrect, did not meet the response deadline, or both.\n\ndata &lt;- import |&gt;\n  mutate(ACC = ifelse(ACC == 1, ifelse(RT &lt;= 500, 1, 0), 0))\n\nThe arguments for the first ifelse() are as follows: Accuracy is equal to 1. If TRUE, then second ifelse() statement. If FALSE, then 0.\nThis makes sense because if the accuracy is 0 (incorrect), then the value needs to remain 0. However, if the accuracy is 1, the value will depend on whether the reaction time is less than 500 (thus the second ifelse()).\nIf accuracy is equal to 1, then if reaction time is less than or equal to 500, then set accuracy to 1. If FALSE, then set accuracy to 0.\nKnow that you can place the additional ifelse() statement in either the TRUE or FALSE argument and can keep iterating on ifelse() statements for as long as you need (however that can get pretty complicated).\ncase_when() is an alternative to an ifelse(). Anytime you need multiple ifelse() statements case_when() tends to simplify the code and logic involved.\nLet’s see examples of the two examples provided for ifelse() as a comparison.\n\ndata &lt;- import |&gt;\n  mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1,\n                                          RT &gt; 500 ~ 0))\n\nNotice that the notation is quite different here. Each argument contains the format: conditional statement followed by the symbol ~ (this should be read as “then set as”) and then a value to be assigned when the conditional statement is TRUE. There is no value to specify when it is FALSE.\nTherefore, it is important when using the case_when() function to either 1) include enough TRUE statement arguments to cover ALL possible values or 2) use the uncharacteristically non-intuitive notation - TRUE ~ \"some value\". In the example above, all possible RT values are included in the two arguments RT &lt;= 500 and RT &gt; 500.\nTo provide an example of the second option:\n\ndata &lt;- import |&gt;\n  mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1,\n                                          TRUE ~ 0))\n\nThe case_when() function will evaluate each argument in sequential order. So when it gets to the last argument (and this should always be the last argument), this is basically saying, when it is TRUE that none of the above arguments were TRUE (hence why this argument is being evaluated) then (~) set the value to “some value” (whatever value you want to specify).\nNow this function gets a little more complicated if you want to set values to NA. NA values are technically logical values like TRUE or FALSE. The values in a column can only be of one type; numerical, character, logical, etc. Therefore, if you have numerical values in a column but want to set some to NA, then this becomes an issue when using case_when() (hopefully this will be fixed in future updates to dplyr). For now, how to get around this is changing the type of value that NA is. For instance; as.numeric(NA), as.character(NA).\n\ndata &lt;- import |&gt;\n  mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1,\n                                          RT &gt; 500 ~ 0,\n                                          TRUE ~ as.numeric(NA)))\n\nNow on to the example in which we used two ifelse() statements.\n\ndata &lt;- import |&gt;\n  mutate(ACC = case_when(ACC == 1 & RT &lt;= 500 ~ 1,\n                         ACC == 1 & RT &gt; 500 ~ 0,\n                         ACC == 0 ~ 0))\n\nWhen you have multiple ifelse() statements case_when() becomes easier to read. Compare this use of case_when() with the equivalent ifelse() above.\nThe case_when() function makes it very explicit what is happening. There are three conditional statements, therefore three categories of responses.\n\nA correct response and reaction time that meets the deadline.\nA correct response and reaction time that DOES NOT meet the deadline.\nAn incorrect response\n\nThese three options cover all possible combinations between the the two columns ACC and RT.\nAccuracy should only be set to 1 (correct) with the first option and that is made quite clearly because it is the only one with ~ 1.\nThis is not as obvious in the ifelse() example.\nLet’s move on to the next dplyr function."
  },
  {
    "objectID": "working-with-data.html#group_by",
    "href": "working-with-data.html#group_by",
    "title": "9  Working with Data",
    "section": "group_by()",
    "text": "group_by()\nThis function is very handy if we want to perform functions separately on different groups or splits of the data frame. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject separately. Instead of manually breaking the data frame up by Subject, the group_by() function does this automatically in the background. Like this…\n\ndata &lt;- data |&gt;\n  group_by(Subject) |&gt;\n  mutate(RT = ifelse(RT &lt; 200, NA, RT),\n         grandRT = mean(RT, na.rm = TRUE),\n         RTdiff = RT - grandRT) |&gt;\n  ungroup()\n\n\n\n\n\n\n\nCaution\n\n\n\nI suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it.\n\n\nYou will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition.\n\ndata &lt;- data |&gt;\n  group_by(Subject, Condition) |&gt;\n  mutate(RT = ifelse(RT &lt; 200, NA, RT),\n         grandRT = mean(RT, na.rm = TRUE),\n         RTdiff = RT - grandRT) |&gt;\n  ungroup()\n\ngroup_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). Therefore, it can essentially replace most uses of for loops."
  },
  {
    "objectID": "working-with-data.html#by-vs.-group_by",
    "href": "working-with-data.html#by-vs.-group_by",
    "title": "9  Working with Data",
    "section": ".by vs. group_by()",
    "text": ".by vs. group_by()\nA new feature was recently introduced to replace some uses of group_by(). You can use the argument .by = inside of mutate() and summarise() (and other functions) to avoid having to use ungroup().\n.by = will only group the data frame for that one function and the returned output will be an ungrouped data frame. In general, I would suggest using .by = unless there is good reason to use group_by().\nThe above grouped mutate can be performed with by = like this:\n\ndata &lt;- data |&gt;\n  mutate(.by = c(Subject, Condition),\n         RT = ifelse(RT &lt; 200, NA, RT),\n         grandRT = mean(RT, na.rm = TRUE),\n         RTdiff = RT - grandRT)\n\nYou can see that .by = is also more concise (fewer lines of code) than group_by()"
  },
  {
    "objectID": "working-with-data.html#sec-summarise",
    "href": "working-with-data.html#sec-summarise",
    "title": "9  Working with Data",
    "section": "summarise()",
    "text": "summarise()\nThe summarise() function will reduce a data frame by summarizing values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation.\nRemember that in order to calculate the FlankerEffect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials\nWe’ve done our filtering, selecting, mutating, now let’s aggregate RTs across Condition to calculate mean RT. We will use a combo of .by = and summarise(). summarise() is almost always used in conjunction with .by = or group_by().\nLet’s also summarise the mean accuracy across conditions.\n\ndata &lt;- data |&gt;\n  summarise(.by = c(Subject, Condition),\n            RT.mean = mean(RT, na.rm = TRUE),\n            ACC.mean = mean(ACC, na.rm = TRUE))\n\nTo summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me.\nWhat does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject.\nAlso, notice how non-grouped columns got removed: Trial, and ACC."
  },
  {
    "objectID": "working-with-data.html#sec-pivot_wider",
    "href": "working-with-data.html#sec-pivot_wider",
    "title": "9  Working with Data",
    "section": "pivot_wider()",
    "text": "pivot_wider()\nOur data frame now looks like\n\nhead(data)\n## # A tibble: 6 × 4\n##   Subject Condition   RT.mean ACC.mean\n##     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n## 1   14000 incongruent    510.    0.574\n## 2   14000 congruent      401.    0.931\n## 3   14001 incongruent    423.    0.852\n## 4   14001 congruent      392.    0.980\n## 5   14002 congruent      462.    0.765\n## 6   14002 incongruent    536.    0.463\n\nUltimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the data frame. To do so we will use the pivot_wider() function from the tidyr package.\nThe tidyr package, like readr and dplyr, is from the tidyverse set of packages. The pivot_wider() function will convert a long data frame to a wide data frame. In other words, it will spread values on different rows across different columns.\nIn our example, what we want to do is pivot_wider() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, and congruent trials being represented down rows we are spreading them across columns (widening the data frame).\nThe three main arguments to specify in pivot_wider() are\n\nid_cols: The column names that uniquely identifies (e.g. “Subject”) each observation and that you want to be retained when reshaping the data frame.\nnames_from: The column name that contains the variables to create new columns by (e.g. “Condition”). The values in this column will become Column names in the wider data format\nvalues_from: The column name that contains the values (e.g. “RT”).\n\n\ndata_wide &lt;- data |&gt;\n  pivot_wider(id_cols = \"Subject\",\n              names_from = \"Condition\", \n              values_from = \"RT.mean\")\n\nNow our data frame looks like\n\nhead(data_wide)\n## # A tibble: 6 × 3\n##   Subject incongruent congruent\n##     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n## 1   14000        510.      401.\n## 2   14001        423.      392.\n## 3   14002        536.      462.\n## 4   14003        679.      567.\n## 5   14004        655.      548.\n## 6   14005        559.      472.\n\nNotice that the ACC.mean column and values were dropped. To add more transparency to our data frame it would be a good idea to label what values the “congruent” and “incongruent” columns contain. You can do this with the optional names_prefix argument. For instance:\n\ndata_wide &lt;- data |&gt;\n  pivot_wider(id_cols = \"Subject\",\n              names_from = \"Condition\", \n              values_from = \"RT.mean\",\n              names_prefix = \"RT_\")\n\n\nhead(data_wide)\n## # A tibble: 6 × 3\n##   Subject RT_incongruent RT_congruent\n##     &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n## 1   14000           510.         401.\n## 2   14001           423.         392.\n## 3   14002           536.         462.\n## 4   14003           679.         567.\n## 5   14004           655.         548.\n## 6   14005           559.         472.\n\nNow a stranger (or a future YOU) will be able to look at this data frame and immediately know that reaction time values are contained in these columns.\nFrom here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this\n\ndata_wide &lt;- data_wide |&gt;\n  mutate(FlankerEffect_RT = RT_incongruent - RT_congruent)\n\n\nhead(data_wide)\n## # A tibble: 6 × 4\n##   Subject RT_incongruent RT_congruent FlankerEffect_RT\n##     &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n## 1   14000           510.         401.            109. \n## 2   14001           423.         392.             31.3\n## 3   14002           536.         462.             74.0\n## 4   14003           679.         567.            113. \n## 5   14004           655.         548.            107. \n## 6   14005           559.         472.             87.1\n\nPerfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file to a data frame with one row per subject and a column of FlankerEffect scores.\nWhat if we have multiple columns we want to get id_cols, names_from, or values_from? pivot_wider() allows for this very easily. For instance:\n\ndata_wide &lt;- data |&gt;\n  pivot_wider(id_cols = \"Subject\",\n              names_from = \"Condition\", \n              values_from = c(\"RT.mean\", \"ACC.mean\"))\n\n\nhead(data_wide)\n## # A tibble: 6 × 5\n##   Subject RT.mean_incongruent RT.mean_congruent ACC.mean_incongruent\n##     &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;                &lt;dbl&gt;\n## 1   14000                510.              401.               0.574 \n## 2   14001                423.              392.               0.852 \n## 3   14002                536.              462.               0.463 \n## 4   14003                679.              567.               0.0926\n## 5   14004                655.              548.               0.0370\n## 6   14005                559.              472.               0.204 \n## # ℹ 1 more variable: ACC.mean_congruent &lt;dbl&gt;\n\nNow you can see that we have four columns corresponding to reaction times and accuracy values across the two conditions. You can use the same notation c() if you want to use multiple column for id_cols, names_from, values_from.\nNow we can calculate a FlankerEffect for both RT and Accuracy values\n\ndata_wide &lt;- data_wide |&gt;\n  mutate(FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent,\n         FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent)"
  },
  {
    "objectID": "working-with-data.html#sec-pivot_longer",
    "href": "working-with-data.html#sec-pivot_longer",
    "title": "9  Working with Data",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nFor our goal with this data set, we do not need to switch back to a longer data format, however reshaping your data to a longer format may be something you want to do one day.\nLet’s try to reshape the data_wide back to a long format that we originally started with.\nWhen you have multiple value columns this is not as intuitive as pivot_wider(). To see more documentation and examples use ?tidyr::pivot_longer().\n\ndata_long &lt;- data_wide |&gt;\n  pivot_longer(contains(\"mean\"),\n               names_to = c(\".value\", \"Condition\"),\n               names_sep = \"_\")"
  },
  {
    "objectID": "working-with-data.html#all-together-now",
    "href": "working-with-data.html#all-together-now",
    "title": "9  Working with Data",
    "section": "All Together Now",
    "text": "All Together Now\nWe can pipe the relevant functions in the chapter together as such\n\n## Setup\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n## Import\nimport &lt;- read_csv(\"data/tidyverse_example.csv\")\n\n## Score\ndata &lt;- import |&gt;\n  rename(Accuracy = ACC) |&gt;\n  filter(TrialProc == \"real\") |&gt;\n  select(Subject, Condition, RT, Trial, ACC = Accuracy) |&gt;\n  group_by(Subject, Condition) |&gt;\n  mutate(RT = ifelse(RT &lt; 200, NA, RT),\n         grandRT = mean(RT, na.rm = TRUE),\n         RTdiff = RT - grandRT) |&gt;\n  summarise(RT.mean = mean(RT, na.rm = TRUE),\n            ACC.mean = mean(ACC, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  pivot_wider(id_cols = \"Subject\",\n              names_from = \"Condition\", \n              values_from = c(\"RT.mean\", \"ACC.mean\")) |&gt;\n  mutate(FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent,\n         FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent)\n\n\nVirtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr.\nhttps://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html"
  },
  {
    "objectID": "data-processing-steps.html#data-preparation",
    "href": "data-processing-steps.html#data-preparation",
    "title": "10  Data Processing Steps",
    "section": "Data Preparation",
    "text": "Data Preparation\nThere are two scenarios in which you may need to start processing and analyzing data:\n\nBefore data collection has finished\nAfter data collection has finished\n\nFor both of these scenarios, you will start with messy raw data files in some file format. Messy raw data files are hard to understand, have poor column and value labels, contain way too many columns and rows, and are just hard to work with. Data preparation is all about getting raw data files that are easy to work with.\n\n\n\n\nFigure 10.1: Data preparation workflow\n\n\n\nThe end product of the data preparation stage is tidy raw data files. Tidy raw data files are easy to understand, have sensible column and value labels, contain only relevant columns and rows, and are very easy to work with.\nThe Data Preparation stage is required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a messy raw data file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of the data preparation stage is simply to create tidy raw data files from the messy raw data files.\nTidy raw data files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a tidy raw data file and understand what each column represents, and what the values in the column refer to."
  },
  {
    "objectID": "data-processing-steps.html#scoring-and-cleaning-data",
    "href": "data-processing-steps.html#scoring-and-cleaning-data",
    "title": "10  Data Processing Steps",
    "section": "Scoring and Cleaning Data",
    "text": "Scoring and Cleaning Data\nOnce you have tidy raw data files, you can start scoring the tasks and implement any data cleaning procedures. In general, there are three steps that are typically done at this stage:\n\n\nScoring the data\nThis involves aggregating performance across trials on a task for each subject.\n\n\nCleaning the data\nThis involves removing subjects that are identified to be problematic and / or outliers.\n\n\nCalculating reliability\nFor individual differences, it is important to report the reliability of each task."
  },
  {
    "objectID": "data-processing-steps.html#preparing-a-single-data-file",
    "href": "data-processing-steps.html#preparing-a-single-data-file",
    "title": "10  Data Processing Steps",
    "section": "Preparing a Single Data File",
    "text": "Preparing a Single Data File\nThe scoring and cleaning data stage will produce scored data files for each task, but ultimately we want a single data file to use for statistical analysis.\nTherefore, the next stage is to join all the scored data files into a single dataframe and select only relevant variables (columns) that we need for analysis."
  },
  {
    "objectID": "data-processing-steps.html#setup-project-folder",
    "href": "data-processing-steps.html#setup-project-folder",
    "title": "10  Data Processing Steps",
    "section": "Setup Project Folder",
    "text": "Setup Project Folder\nBefore moving on to the other chapters in this section, create a research study project folder. See Appendix C on how to do this."
  },
  {
    "objectID": "tidy-raw-data.html#overview-of-template",
    "href": "tidy-raw-data.html#overview-of-template",
    "title": "11  Tidy Raw Data",
    "section": "Overview of Template",
    "text": "Overview of Template\nSetup\n\n# ---- Setup ----\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr) # delete if importing a single file, not a batch of files\n\n# directories\nimport_dir &lt;- \"data/raw/messy\"\noutput_dir &lt;- \"data/raw\"\n\n# file names\ntask &lt;- \"taskname\"\nimport_file &lt;- paste(task, \".txt\", sep = \"\")\noutput_file &lt;- paste(task, \"raw.csv\", sep = \"_\")\n# ---------------\n\n\n\nLoad packages\nAny packages required for this script are loaded at the top. The here, readr, and dplyr packages will be used. The purrr package will be used if you are importing a batch of files. If you are only importing a single file you will not need this and therefore should be deleted.\n\n\nSet Import/Output Directories\nTo make this example easier you will not have to actually import/output any files.\n\n\nSet Import/Output Filenames\nThe only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\".\n\nImport\nI have provided two different import options. The first one is if you are importing a single file. The second is if you are importing a batch of files. For more details on importing a batch of files see Section 8.6.1\n\n# ---- Import Data ----\n# to import a single file\ndata_import &lt;- read_delim(here(import_dir, import_file), delim = \"\\t\",\n                          escape_double = FALSE, trim_ws = TRUE)\n\n# alternatively to import a batch of files...\n# change the arguments in purrr::map_df() depending on type of data files\n# this example is for files created from eprime and needs encoding = \"UCS-2LE\"\nfiles &lt;- list.files(here(import_dir, task), pattern = \".txt\", full.names = TRUE)\ndata_import &lt;- files |&gt;\n  map_df(read_delim, locale = locale(encoding = \"UCS-2LE\"),\n         delim = \"\\t\", escape_double = FALSE, trim_ws = TRUE, na = \"NULL\")\n# ---------------------\n\nTidy raw data\nThis is the meat of the script, where the action happens. It will also be different for every task - obviously. I will cover these steps in more detail below.\n\n# ---- Tidy Data ----\ndata_raw &lt;- data_import |&gt;\n  filter() |&gt;\n  rename() |&gt;\n  mutate() |&gt;\n  select()\n# -------------------\n\nOutput data\nNo need to change anything here. Isn’t that nice?\n\n# ---- Save Data ----\nwrite_csv(data_raw, here(output_dir, output_file))\n# -------------------\n\nrm(list = ls())"
  },
  {
    "objectID": "tidy-raw-data.html#filter-rows",
    "href": "tidy-raw-data.html#filter-rows",
    "title": "11  Tidy Raw Data",
    "section": "Filter Rows",
    "text": "Filter Rows\nOne of the first things that is useful to do is get rid of rows in the messy data file that you don’t need.\nFor E-Prime data, Procedure[Trial] is usually the column name you need to only keep rows for practice and real trials procedures.\n\n\n\n\n\n\nTip\n\n\n\nType colnames(data_import) in the console window to get a read out of all the column names in your data. It is much faster and easier to see column names in the console than navigating the data frame itself.\n\n\nYou need to figure out the value names that correspond to the rows you want to keep. Use unique(``Procedure[Trial]``)\nLet’s say we only want to keep rows that have a value in the Procedure[Trial] column as either TrialProc or PracProc.\n\nrename(TrialProc = `Procedure[Trial]`)\nfilter(TrialProc == \"TrialProc\" | TrialProc == \"PracProc\")"
  },
  {
    "objectID": "tidy-raw-data.html#change-values-in-columns",
    "href": "tidy-raw-data.html#change-values-in-columns",
    "title": "11  Tidy Raw Data",
    "section": "Change Values in Columns",
    "text": "Change Values in Columns\nYou will likely want to change some of the value labels in columns to make more sense and standardize it across tasks. In general, you should avoid numeric labels for categorical data. Instead, you should just use word strings that describe the category intuitively (e.g., “red”, “blue”, “green” instead of 1, 2, 3).\nLet’s change the TrialProc values so they are more simple and easy to read.\n\nmutate(TrialProc = case_when(TrialProc == \"TrialProc\" ~ \"real\", \n                             TrialProc == \"PracProc\" ~ \"practice\",\n                             TRUE ~ as.character(NA))) \n\nYou may want to do more complex changing of values or creating entirely new columns. See the **Working with Data* section for more details."
  },
  {
    "objectID": "tidy-raw-data.html#keep-only-a-few-columns",
    "href": "tidy-raw-data.html#keep-only-a-few-columns",
    "title": "11  Tidy Raw Data",
    "section": "Keep only a few Columns",
    "text": "Keep only a few Columns\nYou will also likely want to select only a subset of columns to keep in the tidy raw data file.\n\nselect(Subject, TrialProc, Trial, Condition, Accuracy, RT, Response,\n       CorrectResponse, AdminTime, SessionDate, SessionTime)"
  },
  {
    "objectID": "score-clean-data.html#overview",
    "href": "score-clean-data.html#overview",
    "title": "12  Score and Clean Data",
    "section": "Overview",
    "text": "Overview\nAt this stage of data processing you have tidy raw data files for each task. The next step is to process the raw data into aggregate summary variables. This involves several steps:\n\nCalculate task scores\nEvaluate and remove problematic subjects and outliers\nCalculate reliability"
  },
  {
    "objectID": "score-clean-data.html#setup",
    "href": "score-clean-data.html#setup",
    "title": "12  Score and Clean Data",
    "section": "Setup",
    "text": "Setup\n\n# ---- Setup ----\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(englelab)    # for data cleaning functions\nlibrary(tidyr)       # for pivot_wider. delete if not using\nlibrary(psych)       # for cronbach's alpha. delete if not using\n\n# directories\nimport_dir &lt;- \"data/raw\"\noutput_dir &lt;- \"data/scored\"\n\n# file names\ntask &lt;- \"taskname\"\nimport_file &lt;- paste(task, \"raw.csv\", sep = \"_\")\noutput_file &lt;- paste(task, \"Scores.csv\", sep = \"_\")\n\n## data cleaning parameters\noutlier_cutoff &lt;- 3.5\n# --------------\n\n\n\npackages\nAny packages required for this script are loaded at the top. For this task we will need the here, readr, dplyr, tidyr, englelab, and psych packages.\n\n\ndirectories\nIf you are using my default project organization, you do not need to change anything here.\n\n\nfile names\nThe only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\".\n\n\ndata cleaning parameters\nIn this section of the script we can set certain data cleaning criterion to variables. This makes it easy to see what data cleaning criteria were used right at the top of the script rather than having to read through and try to interpret the script.\nFor the visual arrays task we should remove subjects who had low accuracy scores - lets say those with accuracy lower 50% (chance level performance on this task)\nAdd acc_criterion &lt;- .5 to this section of the script.\nThere is already an outlier criterion added to this section by default. This criterion will remove outliers that have scores on the task greater or less than 3.5 standard deviations from the mean."
  },
  {
    "objectID": "score-clean-data.html#import",
    "href": "score-clean-data.html#import",
    "title": "12  Score and Clean Data",
    "section": "Import",
    "text": "Import\n\n# ---- Import Data ----\ndata_import &lt;- read_csv(here(import_dir, import_file))\n# ---------------------\n\nBecause of this template, you can now quickly import the data file by running the line of code in the import section. Take a look at the dataframe.\nNotice how there are rows that correspond to practice trials. We do not want to use these rows when calculating scores on the task. So let’s filter for only real trials right when we import the data, so that we won’t have to remember to do it later.\nAdd a row to keep only real trials\n\n# ---- Import Data ----\ndata_import &lt;- read_csv(here(import_dir, import_file)) |&gt;\n  filter(TrialProc == \"real\")\n# ---------------------"
  },
  {
    "objectID": "score-clean-data.html#calculate-task-scores",
    "href": "score-clean-data.html#calculate-task-scores",
    "title": "12  Score and Clean Data",
    "section": "Calculate Task Scores",
    "text": "Calculate Task Scores\nThis is where the action happens. It will also be different for every task - obviously. However, there are a few steps that are pretty common.\n\n# ---- Score Data ----\ndata_scores &lt;- data_import |&gt;\n  muatate(.by = Subject) |&gt;\n  summarise(.by = Subject)\n# -------------------\n\nTrim Reaction Time\nTo trim reaction times less than 200ms:\n\nmutate(RT = ifelse(RT &lt; 200, NA, RT))\n\nTo trim reaction times less than 200ms or greater than 10000ms\n\nmutate(RT = ifelse(RT &lt; 200 | RT &gt; 10000, NA, RT))\n\nAlternatively, using dplyr::case_when()\n\nmutate(RT = case_when(RT &lt; 200 ~ as.numeric(NA),\n                      RT &gt; 10000 ~ as.numeric(NA),\n                      TRUE ~ RT))\n\nSummary Statistic\nOnce you group and clean the data, you can calculate a summary statistic such as a mean, median, or standard deviation.\nTo calculate the mean accuracy and reaction time for each subject and condition:\n\nsummarise(.by = c(Subject, Condition),\n          Accuracy.mean = mean(Accuracy, na.rm = TRUE),\n          RT.mean = mean(RT, na.rm = TRUE))\n\nTransform Data to Wide\nIf you are grouping by Subject and Condition, then you will likely want to transform the aggregated data into a wide format. This is because summarise() will produce a row for each Condition per Subject. What you might want is a single row per subject, with the conditions spread out across columns.\n\n\n\n\n\n\nTip\n\n\n\nIf you forget how to use a function or what the argument names are then type ?functionName() in the console (e.g. ?pivot_wider()).\n\n\n\npivot_wider(id_cols = Subject,\n            names_from = Condition,\n            values_from = Accuracy.mean)\n\nMore Complex Scoring\nThis is an example of how to calculate k scores for the visual arrays task. You can see this is a little more involved.\n\ndata_scores &lt;- data_import |&gt;\n  summarise(.by = c(Subject, SetSize),\n            CR.n = sum(CorrectRejection, na.rm = TRUE),\n            FA.n = sum(FalseAlarm, na.rm = TRUE),\n            M.n = sum(Miss, na.rm = TRUE),\n            H.n = sum(Hit, na.rm = TRUE)) |&gt;\n  mutate(CR = CR.n / (CR.n + FA.n),\n         H = H.n / (H.n + M.n),\n         k = SetSize * (H + CR - 1)) |&gt;\n  pivot_wider(id_cols = Subject,\n              names_from = SetSize,\n              names_prefix = \"VA.k_\",\n              values_from = k) |&gt;\n  mutate(VA.k = (VA.k_5 + VA.k_7) / 2)"
  },
  {
    "objectID": "score-clean-data.html#data-cleaning",
    "href": "score-clean-data.html#data-cleaning",
    "title": "12  Score and Clean Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe next section of the script template is for cleaning the data by removing problematic subjects and/or removing outliers.\nRemove Problematic Subjects\nDepending on the task, problematic subjects can be detected in different ways. For this task we will simply remove subjects that had less than chance performance (were just guessing or did not understand the task).\nTo do so, we will use a custom function created for this purpose, remove_problematic() from our englelab package. ?englelab::remove_problematic\nThe main argument is filter =. You need to put in here, what you would put inside of dplyr::filter() to remove anyone with less than chance performance: dplyr::filter(Accuracy.mean &lt; acc_criterion). Remember at the top of the script we added the accuracy criterion by setting acc_criterion &lt;- .5 (chance performance).\nThe other argument, that is more optional, is log_file =. This allows us to save a data file containing only the subjects that were removed. This is good if we later on want to inspect how many subjects were removed, and why.\n\ndata_cleaned &lt;- data_scores |&gt;\n  remove_problematic(\n    filter = \"Accuracy.mean &lt; acc_criterion\",\n    log_file = here(\"data/logs\", paste(task, \"_problematic.csv\", sep = \"\")))\n\nRemove Outliers\nRemove outliers based on their final task scores. A typical way we remove outliers is by setting their score to missing NA if it is 3.5 standard deviations above or below the mean.\nTo do so, we will use a custom function created for this purpose, replace_outliers from our englelab package.\nThere are several arguments that need to be defined. See ?englelab::replace_outliers for a description on this.\nThe fully piped |&gt; code for this entire section looks like:\n\n# ---- Clean Data ----\ndata_cleaned &lt;- data_scores |&gt;\n  remove_problematic(\n    filter = \"Accuracy.mean &lt; acc_criterion\",\n    log_file = here(\"data/logs\", paste(task, \"_problematic.csv\", sep = \"\"))) |&gt;\n  replace_outliers(\n    variables = \"VA.k\",\n    cutoff = outlier_cutoff,\n    with = \"NA\",\n    log_file = here(\"data/logs\", paste(task, \"_outliers.csv\", sep = \"\"))) |&gt;\n  filter(!is.na())\n# -------------------\n\nNotice that filter(!is.na()) is specified after replace_outliers. This removes any of the outliers from the dataframe."
  },
  {
    "objectID": "score-clean-data.html#calculate-reliability",
    "href": "score-clean-data.html#calculate-reliability",
    "title": "12  Score and Clean Data",
    "section": "Calculate Reliability",
    "text": "Calculate Reliability\nThere are two standard ways of calculating reliability: split-half and cronbach’s alpha.\nIt is best to calculate reliability only on the subjects that made it passed data cleaning.\n\nreliability &lt;- data_import |&gt;\n  filter(Subject %in% data_cleaned$Subject)\n\nreliability will be the raw data frame that we will use to calculate reliabilty estimates on.\nSplit-half reliability\nHere is an example if the task score was an aggregate of accuracy.\n\nsplithalf &lt;- reliability |&gt;\n  mutate(.by = Subject,\n         Split = ifelse(Trial %% 2, \"odd\", \"even\")) |&gt;\n  summarise(.by = c(Subject, Split),\n            Accuracy.mean = mean(Accuracy, na.rm = TRUE)) |&gt;\n  pivot_wider(id_cols = Subject,\n              names_from = Split,\n              values_from = Accuracy.mean) |&gt;\n  summarise(r = cor(even, odd)) |&gt;\n  mutate(r = (2 * r) / (1 + r))\n\ndata_cleaned$Score_splithalf &lt;- splithalf$r\n\nFor tasks, like the visual arrays, where the score is not a simple aggregate but a more complicated calculation this is more involved and we would basically want to score the data set for even and odd trials separately.\nTo calculate the split-half reliability of k scores on the visual arrays task would look something like:\n\nsplithalf &lt;- reliability |&gt;\n  mutate(.by = c(Subject, SetSize),\n         Split = ifelse(Trial %% 2, \"odd\", \"even\")) |&gt;\n  summarise(.by = c(Subject, SetSize, Split),\n            CR.n = sum(CorrectRejection, na.rm = TRUE),\n            FA.n = sum(FalseAlarm, na.rm = TRUE),\n            M.n = sum(Miss, na.rm = TRUE),\n            H.n = sum(Hit, na.rm = TRUE)) |&gt;\n  mutate(CR = CR.n / (CR.n + FA.n),\n         H = H.n / (H.n + M.n),\n         k = SetSize * (H + CR - 1)) |&gt;\n  pivot_wider(id_cols = Subject,\n              names_from = c(SetSize, Split),\n              names_prefix = \"VA.k_\",\n              values_from = k) |&gt;\n  mutate(VA.k_even = (VA.k_5_even + VA.k_7_even) / 2,\n         VA.k_odd = (VA.k_5_odd + VA.k_7_odd) / 2)\n  summarise(r = cor(VA.k_even, VA.k_odd)) |&gt;\n  mutate(r = (2 * r) / (1 + r))\n\ndata_cleaned$Score_splithalf &lt;- splithalf$r\n\nCronbach’s alpha\nHere is an example if the task score was an aggregate of accuracy.\n\ncronbachalpha &lt;- reliability |&gt;\n  select(Subject, Trial, Accuracy) |&gt;\n  pivot_wider(id_cols = Subject,\n              names_from = Trial,\n              values_from = Accuracy) |&gt;\n  select(-Subject) |&gt;\n  alpha()  # from the psych package\n\ndata_cleaned$Score_cronbachalpha &lt;- cronbachalpha$total$std.alpha"
  },
  {
    "objectID": "single-merged-file.html#overview",
    "href": "single-merged-file.html#overview",
    "title": "13  Single Merged File",
    "section": "Overview",
    "text": "Overview\nAt this stage of data processing you have created multiple data files containing the task scores, reliabilities, and other variables for each task. The next step is to create a single merged data file containing the primary task scores for each task that you will be performing data analysis on. We can also create data files with the reliabilities, administration times, and a log of the data cleaning steps."
  },
  {
    "objectID": "single-merged-file.html#setup",
    "href": "single-merged-file.html#setup",
    "title": "13  Single Merged File",
    "section": "Setup",
    "text": "Setup\n\n# ---- Setup ----\n# packages\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\n\n# directories\nimport_dir &lt;- \"data/scored\"\noutput_dir &lt;- \"data\"\n\n# file names\noutput_scores &lt;- \"TaskScores.csv\"\noutput_reliabilities &lt;- \"Reliabilities.csv\"\noutput_admintimes &lt;- \"AdminTimes.csv\"\noutput_datacleaning &lt;- \"DataCleaning_log.csv\"\n# --------------\n\n\n\npackages\nAny packages required for this script are loaded at the top. For this task we will need the here, readr, dplyr, purrr, and tidyr packages.\n\n\ndirectories\nIf you are using my default project organization, you do not need to change anything here.\n\n\nfile names\nFeel free to change these as you like. E.g., You may want to add the study name in front of these file names, “StudyName_TaskScores.csv”."
  },
  {
    "objectID": "single-merged-file.html#import",
    "href": "single-merged-file.html#import",
    "title": "13  Single Merged File",
    "section": "Import",
    "text": "Import\nFor more details on importing a batch of files see Section 8.6.2\n\n# ---- Import Data ----\nfiles &lt;- list.files(here(import_dir), pattern = \"Scores\", full.names = TRUE)\ndata_import &lt;- files %&gt;%\n  map(read_csv) %&gt;%\n  reduce(full_join, by = \"Subject\")\n# ---------------------"
  },
  {
    "objectID": "single-merged-file.html#task-scores",
    "href": "single-merged-file.html#task-scores",
    "title": "13  Single Merged File",
    "section": "Task Scores",
    "text": "Task Scores\n\n# ---- Select Task Scores ----\ndata_scores &lt;- data_import %&gt;%\n  select() %&gt;%\n  filter()\n\n# list of final subjects\nsubjlist &lt;- select(data_scores, Subject)\n# ----------------------------\n\nAt this step, you may also want to filter out subjects that have missing data on specific tasks, or too much missing data across all the tasks.\nI advise creating a final subject list of all subjects that made it to this state of data processing."
  },
  {
    "objectID": "single-merged-file.html#reliabilities",
    "href": "single-merged-file.html#reliabilities",
    "title": "13  Single Merged File",
    "section": "Reliabilities",
    "text": "Reliabilities\n\n# ---- Reliabilities ----\ndata_reliabilities &lt;- data_import %&gt;%\n  select(contains(\"splithalf\"), contains(\"cronbachalpha\")) %&gt;%\n  drop_na() %&gt;%\n  distinct() %&gt;%\n  pivot_longer(everything(),\n               names_to = c(\"Task\", \"metric\"),\n               names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\",\n               values_to = \"value\") %&gt;%\n  pivot_wider(id_col = Task,\n              names_from = metric,\n              values_from = value)\n# -----------------------\n\nThe code in pivot_longer(names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\") follows a specific naming scheme used for column names. Underscores (optional) can be used for task names and column descriptions, but a period (required) is ONLY used to seperate the task name / description from the description of the task score type / what the value represents (e.g., RT = reaction time, splithalf = split-half reliability)\nTask_Name_MoreStuff.ScoreType\ne.g., StroopDL_Last4Rev.ResponseDeadline, VAorient_S.k, Antisaccade.ACC\nThis should also be used for reliabilities and admin times:\ne.g., StroopDL_Last4Rev.splithalf, StroopDL.AdminTime"
  },
  {
    "objectID": "single-merged-file.html#admin-times",
    "href": "single-merged-file.html#admin-times",
    "title": "13  Single Merged File",
    "section": "Admin Times",
    "text": "Admin Times\n\n# ---- Admin Times ----\ndata_merge &lt;- data_import %&gt;%\n  select(contains(\"AdminTime\")) %&gt;%\n  summarise_all(list(mean = mean, sd = sd), na.rm = TRUE) %&gt;%\n  pivot_longer(everything(),\n               names_to = c(\"Task\", \"metric\"),\n               names_pattern = \"(\\\\w+.\\\\w+).(\\\\w+)\",\n               values_to = \"value\") %&gt;%\n  mutate(value = round(value, 3)) %&gt;%\n  pivot_wider(id_col = Task,\n              names_from = metric,\n              names_prefix = \"AdminTime.\",\n              values_from = \"value\")\n# ---------------------"
  },
  {
    "objectID": "intro-ggplot2.html#fundamentals-of-data-visualization",
    "href": "intro-ggplot2.html#fundamentals-of-data-visualization",
    "title": "14  Intro to ggplot2",
    "section": "Fundamentals of Data Visualization",
    "text": "Fundamentals of Data Visualization\nData visualization is an essential skill for anyone working with data. It is a combination of statistical understanding and design principles and is really about graphical data analysis and communication and perception.\nData visualization is often times glossed over in our stats courses. This is unfortunate because it is so important for better understanding our data, for communicating our results to others, and frankly it is too easy to create poorly designed visualizations.\nAs a scientist, there are two purposes for visualizing our data.\n\nData exploration: it is difficult to fully understand our data just by looking at numbers on a screen arranged in rows and columns. Being skilled in data visualization will help you better understand your data.\nExplain and Communicate: You will also need to explain and communicate your results to colleagues or in scientific publications.\n\nThe same data visualization principles apply to both purposes, however for communicating your results you may want to place more emphasis on aesthetics and readability. For data exploration your visualizations do not have to be pretty.\nLeland Wilkinson (Grammar of Graphics, 1999) formalized two main principles in his plotting framework:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mappings\n\nThe essential grammatical elements to create any visualization are:\n\n\n\n\nFigure 14.1: Essential grammatical elements for data visualization\n\n\n\nPlotting Functions in R\nIt is possible to create plots in R using the base R function plot(). The neat thing about plot() is that it is really good at knowing what kind of plot you want without you having to specify. However, these are not easy to customize and the output is a static image not an R object that can be modified.\nTo allow for data visualization that is more in line with the principles for a grammar of graphics, Hadley Wickham created the ggplot2 package. This by far the most popular package for data visualization in R.\n\n\n\n\nFigure 14.2: ggplot2 logo"
  },
  {
    "objectID": "intro-ggplot2.html#grammar-of-graphics",
    "href": "intro-ggplot2.html#grammar-of-graphics",
    "title": "14  Intro to ggplot2",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe saw from the last chapter that the two main components in a grammar of graphics are:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mappings\n\nWe also saw that the three essential elements are the data layer, aesthetics layer, and geometrics layer. In ggplot2 there are a total of 7 layers we can add to a plot\n\n\n\n\nFigure 14.3: ggplot2 elements"
  },
  {
    "objectID": "intro-ggplot2.html#data-layer",
    "href": "intro-ggplot2.html#data-layer",
    "title": "14  Intro to ggplot2",
    "section": "Data layer",
    "text": "Data layer\nThe Data Layer specifies the data being plotted.\n\n\n\n\nFigure 14.4: ggplot2 data layer\n\n\n\nLet’s see what this means more concretely with an example data set. A very popular data set used for teaching data science is the iris data set. In this data set various species of iris were measured on their sepal and petal length and width.\nThis data set actually comes pre-loaded with R, so you can simply view it by typing in your console\n\nView(iris)\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\nWe can see that this data is in wide format. What type of graph we can visualize will depend on the format of the data set. On occasion, in order to visualize a certain pattern of the data will require you to change the formatting of the data.\nLet’s go ahead and start building our graphical elements in ggplot2. Load the ggplot2 library. Then:\n\nlibrary(ggplot2)\n\nggplot(data = iris)\n\n\n\n\nYou can see that we only have a blank square. This is because we have not added any other layers yet, we have only specified the data layer."
  },
  {
    "objectID": "intro-ggplot2.html#aesthetics-layer",
    "href": "intro-ggplot2.html#aesthetics-layer",
    "title": "14  Intro to ggplot2",
    "section": "Aesthetics Layer",
    "text": "Aesthetics Layer\nThe next grammatical element is the aesthetic layer, or aes for short. This layer specifies how we want to map our data onto the scales of the plot.\n\n\n\n\n\n\nNote\n\n\n\nNote that aesthetics refers to a mapping function. That is, how certain elements (e.g., color, shape, size, etc.) map onto variables in our data.\nYou can also set specific values for these elements (not mapped onto any variables in the data). These are not typically referred to as aesthetics in the grammar of graphics.\nE.g., to make all points in a plot red: geom_point(color = “red”)\n\n\n\n\n\n\nFigure 14.5: ggplot2 aesthetic (aes) layer\n\n\n\nThe aesthetic layer maps variables in our data onto scales in our graphical visualization, such as the x and y coordinates. In ggplot2 the aesthetic layer is specified using the aes() function. Let’s create a plot of the relationship between Sepal.Length and Sepal.Width, putting them on the x and y axis respectively.\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width))\n\n\n\n\nYou can see we went from a blank box to a graph with the variable and scales of Sepal.Length mapped onto the x-axis and Sepal.Width on the y-axis.\nThe aeshetic layer also maps variables in our data to other elements in our graphical visualization, such as color, size, fill, etc.\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species))"
  },
  {
    "objectID": "intro-ggplot2.html#geometries-layer",
    "href": "intro-ggplot2.html#geometries-layer",
    "title": "14  Intro to ggplot2",
    "section": "Geometries Layer",
    "text": "Geometries Layer\nThe next essential element for data visualization is the geometries layer or geom layer for short.\n\n\n\n\nFigure 14.6: ggplot2 geometrics (geom) layer\n\n\n\nJust to demonstrate to you that ggplot2 is creating R graphic objects that you can modify and not just static images, let’s assign the previous graph with data and aesthetics layers only onto an R object called p, for plot.\n\np &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width))\n\nNow let’s say we want to add the individual raw data points to create a scatterplot. To do this we can use the function geom_point(). This is a geom layer and the type of geom we want to add are points.\nIn ggplot2 there is a special notation that is similar to the pipe operator %&gt;% seen before. Except it is plus sign +\n\np + geom_point()\n\n\n\n\nAnd walla! Now we have a scatterplot of the relationship between Sepal.Length and Sepal.Width. Cool.\nIf we look at the scatterplot it appears that there are at least two groups or clusters of points. These clusters might represent the different species of flowers, represented in the Species column. There are different ways we can visualize or separate this grouping structure.\nFirst, let’s create an aesthetic to map the color of the points to the variable Species in our data.\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n\n\n\nNext, we will consider how to plot Species in separate plots within the same visualization."
  },
  {
    "objectID": "intro-ggplot2.html#facets-layer",
    "href": "intro-ggplot2.html#facets-layer",
    "title": "14  Intro to ggplot2",
    "section": "Facets Layer",
    "text": "Facets Layer\nThe facet layer allows you to create subplots within the same graphic object\n\n\n\n\nFigure 14.7: ggplot2 facet layer\n\n\n\nThe previous three layers are the essential layers. The facet layer is not essential, however given your data you may find it helps you to explore or communicate your data.\nLet’s create facets of our scatterplot by Species\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_grid(~ Species)"
  },
  {
    "objectID": "intro-ggplot2.html#statistics-layer",
    "href": "intro-ggplot2.html#statistics-layer",
    "title": "14  Intro to ggplot2",
    "section": "Statistics Layer",
    "text": "Statistics Layer\nThe statistics layer allows you plot statistical values calculated from the data\n\n\n\n\nFigure 14.8: ggplot2 statistics (stat) layer\n\n\n\nSo far we have only plotted the raw data values. However, we may be interested in plotting some statistics or calculated values, such as a regression line, means, standard error bars, etc.\nLet’s add a regression line to the scatterplot. First without the facet layer then with the facet layer\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_grid(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE)\n## `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "intro-ggplot2.html#coordinates-layer",
    "href": "intro-ggplot2.html#coordinates-layer",
    "title": "14  Intro to ggplot2",
    "section": "Coordinates Layer",
    "text": "Coordinates Layer\nThe coordinate layer allows you to adjust the x and y coordinates\n\n\n\n\nFigure 14.9: ggplot2 coordinates (coord) layer\n\n\n\nThere are two main groups of functions that are useful for adjusting the x and y coordinates.\naxis limits\nYou can adjust limits (min and max) of the x and y axes using the coord_cartesian(xlim = \"\", ylim = \"\") function.\nIf you want to compare two separate graphs, then they need to be on the same scale! This is actually a very important design principle in data visualization.\nCompare these two sets of plots:\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(patchwork)\n\np1 &lt;- ggplot(filter(iris, Species == \"setosa\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\np2 &lt;- ggplot(filter(iris, Species == \"versicolor\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\np3 &lt;- ggplot(filter(iris, Species == \"virginica\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\np1 + p2 + p3\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nlibrary(dplyr)\nlibrary(patchwork)\n\np1 &lt;- ggplot(filter(iris, Species == \"setosa\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  coord_cartesian(xlim = c(4, 8), ylim = c(2, 5))\n\np2 &lt;- ggplot(filter(iris, Species == \"versicolor\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  coord_cartesian(xlim = c(4, 8), ylim = c(2, 5))\n\np3 &lt;- ggplot(filter(iris, Species == \"virginica\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  coord_cartesian(xlim = c(4, 8), ylim = c(2, 5))\n\np1 + p2 + p3\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe patchwork package was used to easily create plot of two separate plots side-by-side. The patchwork package is excellent for arranging and combining multiple plots into one figure in all sorts of configurations.\n\n\naxis ticks and labels\nYou can adjust the scale (major and minor ticks) of the x and y axes using the scale_x_ and scale_y_ sets of functions. The two main sets of functions to know are for continuous and discrete scales:\n\ncontinuous: scale_x_continuous(breaks = seq()) and scale_y_continuous(breaks = seq())\ndiscrete: scale_x_discrete(breaks = c()) and scale_y_continuous(breaks = c())\n\nFor example:\n\nggplot(filter(iris, Species == \"setosa\"), \n             aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) +\n  scale_x_continuous(breaks = seq(4, 8, by = .5)) +\n  scale_y_continuous(breaks = seq(2, 5, by = .5))\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is advisable to set the limits of breaks = to be the same as the xlim and ylim specified in coord_cartesian()"
  },
  {
    "objectID": "intro-ggplot2.html#themes-layer",
    "href": "intro-ggplot2.html#themes-layer",
    "title": "14  Intro to ggplot2",
    "section": "Themes Layer",
    "text": "Themes Layer\nThe Themes Layer refers to any visual elements not mapped to data variables\n\n\n\n\nFigure 14.10: ggplot2 themes layer\n\n\n\nYou can change the labels of x or y axis, add a plot title, modify a legend title, add text anywhere on the plot, change the background color, axis lines, plot lines, etc.\nThere are three types of elements within the Themes Layer; text, line, and rectangle. Together these three elements can control all the non-data ink in the graph. Underneath these three elements are sub-elements and this can be represented in a hierarchy such as:\n\n\n\n\nFigure 14.11: ggplot2 theme elements\n\n\n\nFor instance, you can see that you can control the design of the text for the plot title and legend title theme(title = element_text()) or individually with theme(plot.title = element_text(), legend.title = element_text()).\n\nAny text element can be modified with element_text()\nAny line element can be modified with element_line()\nAny rect element can be modified with element_rect()\n\nYou can then control different features such as the color, linetype, size, font family, etc.\n\n\n\n\nFigure 14.12: ggplot2 theme functions\n\n\n\nAs an example let’s change some theme elements to our facet plot. Let’s change the axis value labels to red font and increase the size\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme(axis.text = element_text(color = \"red\", size = 14))\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nNow let’s only change the x-axis text and not the y-axis text.\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme(axis.text.x = element_text(color = \"red\", size = 14))\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nIt is a good idea to have a consistent theme across all your graphs. And so you might want to just create a theme object that you can add to all your graphs.\n\na_theme &lt;- theme(axis.text.x = element_text(color = \"red\", size = 14),\n                 panel.grid = element_blank(),\n                 panel.background = element_rect(fill = \"pink\"))\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme(axis.text.x = element_text(color = \"red\", size = 14)) +\n  a_theme\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nBuilt-in Themes\nFor the most part you can probably avoid the theme() function by using built-in themes, unless there is a specific element you want to modify.\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme(axis.text.x = element_text(color = \"red\", size = 14)) +\n  theme_linedraw()\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nYou can also set a default theme for the rest of your ggplots at the top of your script. That way you do not have to keep on specifying the theme for evey ggplot.\n\ntheme_set(theme_linedraw())\n\nNow you can create a ggplot with theme_linedraw() without specifying theme_linedraw() every single time.\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species) +\n  stat_smooth(method = \"lm\", se = FALSE)\n\nYou can do a google search to easily find different types of theme templates."
  },
  {
    "objectID": "reproducible-workflow.html#sec-what-does-reproducibility-mean",
    "href": "reproducible-workflow.html#sec-what-does-reproducibility-mean",
    "title": "Appendix A — Reproducible Workflow",
    "section": "What does reproducibility mean?",
    "text": "What does reproducibility mean?\nReproducibility in data processing and analysis means that all processing and analysis steps can be fully replicated using only the original raw data files and the execution of the R (or other program) scripts. There are different levels of reproducibility:\n\nPartially reproducible - only some data processing and analysis steps can be reproduced, which may be due to a lack of original raw data files or the use of non-reproducible software.\nMinimal level of reproducibility (acceptable) - full reproduction of data processing and analysis steps on your computer (or a lab computer) without any modifications needed.\nModerately reproducible (desired) - meets the minimal level plus other people not involved in the research project can reproduce the steps with minimal modifications.\nHighly reproducible (good luck!) - full reproduction of steps without modification needed by people not involved in the research project 5 - 10+ years from now.\n\nNote that these levels are arbitrarily defined by myself and what I came up with on the spot. A minimal level of reproducibility is still acceptable, as achieving more requires significant time and effort. Though, we should strive for a moderate amount of reproducibility, and achieving it requires more than just writing code. Your code must be organized, easy to understand, and include notes and documentation. Even if you or someone else attempts to reproduce your steps in the future, they can modify the code to make it work. The highest level of reproducibility is difficult to achieve due to software and code updates. Your code may only work with the current version of R or other packages. There are solutions to this problem of software and code updates, but who knows if those will work in the future!\n\nSimply using R for data analysis does not guarantee that your workflow is reproducible. In fact, there are many non-reproducible ways to use R. To ensure at least a moderate level of reproducibility, consider the following criteria (this is not an exhaustive list):\n\nYour statistical analysis can be fully reproduced using only the raw data files and R scripts\nYour code can be reproduced on other computers without any modifications\nYour data and R scripts are organized and documented in a way that makes them easily understandable to unfamiliar parties\n\nThis last criterion is extremely important, but is often overlooked. Simply posting your data and scripts to an open access repository like OSF is not enough to guarantee reproducibility. If others cannot understand your workflow, then it is not reproducible. Therefore, it is crucial to take the time to think about the organization of your project, files, data, and scripts."
  },
  {
    "objectID": "file-organization.html#sec-rprojects-and-here",
    "href": "file-organization.html#sec-rprojects-and-here",
    "title": "Appendix B — File Organization",
    "section": "RProjects and here()",
    "text": "RProjects and here()\nYou need to be using RStudio Projects for anything you do in R.\nIn fact, you should be opening RStudio by opening an RProject file.\nRStudio Projects allow you to open isolated instances of R and RStudio for each of your projects. In combination with the here package it will also provide a simple and fool proof way of specifying file paths.\nEvery time you use here() you know that the file path will start at where you have your .Rproj file saved. Instead of messing around with working directories using setwd() or getwd(), just use here() and RStudio Projects. This becomes especially helpful when working with Quarto documents.\nFor instance, I have an .Rproj file saved in my “useRguide” folder. When I use here(), the function will start at a file path to that location.\n\nlibrary(here)\n## here() starts at /Users/jtsukahara3/GitHub Repos/useRguide\nhere()\n## [1] \"/Users/jtsukahara3/GitHub Repos/useRguide\"\n\nYou can then use a relative file path inside of here().\n\nhere(\"data/raw/flanker_raw.csv\")\n## [1] \"/Users/jtsukahara3/GitHub Repos/useRguide/data/raw/flanker_raw.csv\"\n\nThis is equivalent to\n\nhere(\"data\", \"raw\", \"flanker_raw.csv\")\n## [1] \"/Users/jtsukahara3/GitHub Repos/useRguide/data/raw/flanker_raw.csv\"\n\nI typically like to set the first argument as the relative file path and the second argument as the file name. This visually separates the file path and the file name, making your script easier to read.\n\nhere(\"data/raw\", \"flanker_raw.csv\")\n## [1] \"/Users/jtsukahara3/GitHub Repos/useRguide/data/raw/flanker_raw.csv\"\n\nYou can then use here() directly in import and output functions:\n\nimport &lt;- read_csv(here(\"data/raw\", \"flanker_raw.csv\"))\n\n\nwrite_csv(data, here(\"data/scored\", \"flanker_scored.csv\"))"
  },
  {
    "objectID": "file-organization.html#mainscript-file",
    "href": "file-organization.html#mainscript-file",
    "title": "Appendix B — File Organization",
    "section": "mainscript file",
    "text": "mainscript file\nWhen you break up your data processing stages into separate R scripts, this can lead to having a lot of R scripts to source. It can be tedious to open each script, source it, then repeat for every R script while making sure you are doing everything in the right order.\nInstead, I prefer to create a mainscript.Rmd file that uses source() to source each R script file in a particular order, and render_quarto() to render Quarto documents.\nUsing a Quarto (.qmd) document for the mainscript file is useful if you want to add in documentation about the study and data analysis workflow. For instance, you can add brief descriptions of how outliers were detected and dealt with, how reliability was calculated, etc.\nIt also makes it easier for yourself, your future self, or someone else to look at your mainscript.qmd file and easily understand your data processing workflow without having to go through each of your R scripts line-by-line."
  },
  {
    "objectID": "file-organization.html#use-templates",
    "href": "file-organization.html#use-templates",
    "title": "Appendix B — File Organization",
    "section": "Use Templates!",
    "text": "Use Templates!\nDon’t start from scratch, like ever.\nIt can be a ton of work, and mental effort, to create an efficient and reproducible project organization from scratch. Generally, I would try to avoid starting a project from blank, empty, R scripts. More than likely what will happen is that any organization you had thought of just breaks down because you are struggling enough with writing the R code itself, and you need to get a report on the data to your advisor so you decide to use shortcuts “for now”. But we all know that “for now” always turns into forever.\nUnfortunately, not starting from scratch usually means that you have already setup an organization system with R script templates that you can use. This is likely not the case. So what to do?\nWell. in the next chapter, I will introduce you to my psyworkflow package that allows you to automatically setup the organizational structure I have outlined in this chapter, along with R script templates for each data processing stage."
  },
  {
    "objectID": "psyworkflow.html#install",
    "href": "psyworkflow.html#install",
    "title": "Appendix C — psyworkflow",
    "section": "Install",
    "text": "Install\nFirst, if you do not have the devtools package installed:\n\ninstall.packages(\"devtools\")\n\nInstall the psyworkflow package from my GitHub repository using the devtools package:\n\ndevtools::install_github(\"dr-JT/psyworkflow\")\n\nRestart R: Session -&gt; Resart R"
  },
  {
    "objectID": "psyworkflow.html#sec-download-r-script-templates",
    "href": "psyworkflow.html#sec-download-r-script-templates",
    "title": "Appendix C — psyworkflow",
    "section": "Download R Script Templates",
    "text": "Download R Script Templates\nIf you already have an RProject setup and just want to download some of the R script templates you can do so with the get_template() function.\n\nworkflow::get_template()\n\nTo see what the options are type in the console window\n\n?workflow::get_template"
  },
  {
    "objectID": "psyworkflow.html#create-a-new-project",
    "href": "psyworkflow.html#create-a-new-project",
    "title": "Appendix C — psyworkflow",
    "section": "Create a New Project",
    "text": "Create a New Project\nClose RStudio and reopen a new instance of RStudio (not from an RProject file).\nOnce you have the psyworkflow package installed you will be able to create a new RProject directory and file from my Research Study template. This will automatically create the directory structure outlined in the previous chapter. It will also add template R scripts in R / templates and a masterscript.Rmd file.\nUsing this template will allow you to get right to working with your data in R, without having to spend too much time thinking about organization (I have already done that for you).\nTo create an RProject from this template:\nFile -&gt; New Project… -&gt; New Directory -&gt; Research Study (you might need to scroll down to see it)\nThis will bring up a window to customize the template:\n\n\n\n\nFigure C.1: Create new project window\n\n\n\nType in whatever you want for the Directory Name - this will end up being the name of the project folder and RProject file.\nClick on Browse… and create the project on your desktop, for now.\nKeep all the defaults and select Create Project.\nGive it some time, and it will reopen RStudio from the newly created RProject. Take a look at the file pane and you can see that the folders have been created, and R Script templates downloaded."
  }
]